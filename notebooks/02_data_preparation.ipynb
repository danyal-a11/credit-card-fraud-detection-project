{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3413accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "✓ Data loaded: (284807, 31)\n",
      "\n",
      "======================================================================\n",
      "DATA QUALITY CHECK\n",
      "======================================================================\n",
      "Total transactions: 284,807\n",
      "Total features: 31\n",
      "Missing values: 0\n",
      "Duplicate rows: 1081\n",
      "\n",
      "======================================================================\n",
      "REMOVING DUPLICATES\n",
      "======================================================================\n",
      "Duplicates found: 1081\n",
      "✓ Removed 1,081 duplicate rows\n",
      "✓ Clean dataset size: (283726, 31)\n",
      "✓ Duplicates remaining: 0\n",
      "\n",
      "======================================================================\n",
      "CLASS DISTRIBUTION\n",
      "======================================================================\n",
      "Class\n",
      "0    283253\n",
      "1       473\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fraud rate: 0.167%\n",
      "\n",
      "======================================================================\n",
      "CHECK 1: CLASS IMBALANCE SEVERITY\n",
      "======================================================================\n",
      "Legitimate transactions: 283,253\n",
      "Fraud transactions: 473\n",
      "Imbalance ratio: 598.8:1\n",
      "  SEVERE imbalance - will need special handling in modeling\n",
      "\n",
      "======================================================================\n",
      "CHECK 2: FEATURE DISTRIBUTIONS\n",
      "======================================================================\n",
      "Amount column statistics:\n",
      "  Min: $0.00\n",
      "  Max: $25691.16\n",
      "  Mean: $88.47\n",
      "  Median: $22.00\n",
      "  Std Dev: $250.40\n",
      "\n",
      "Transactions above 99th percentile: 2,838\n",
      "✓ We'll handle these with robust scaling\n",
      "\n",
      "======================================================================\n",
      "CHECK 3: ZERO VARIANCE IN INPUT FEATURES\n",
      "======================================================================\n",
      "✓ All input features have sufficient variance\n",
      "\n",
      "Note: Class (target variable) has low variance by design\n",
      "      This is normal for imbalanced classification problems\n",
      "\n",
      "======================================================================\n",
      "CHECK 4: FEATURE TYPES\n",
      "======================================================================\n",
      "INPUT FEATURES (30 total):\n",
      "  • V1-V28: Already PCA-transformed (DON'T scale again)\n",
      "  • Time: Needs scaling (seconds since first transaction)\n",
      "  • Amount: Needs scaling (transaction dollar amounts)\n",
      "\n",
      "TARGET VARIABLE (1 total):\n",
      "  • Class: What we're predicting (0=Legit, 1=Fraud)\n",
      "\n",
      "✓ We'll only scale Time and Amount in next step\n",
      "\n",
      "======================================================================\n",
      "CHECK 5: FINAL DATA SHAPE\n",
      "======================================================================\n",
      "Total columns: 31\n",
      "  ├── Input features: 30 (V1-V28, Time, Amount)\n",
      "  └── Target variable: 1 (Class)\n",
      "\n",
      "Total transactions: 283,726\n",
      "Memory usage: 69.27 MB\n",
      "\n",
      "======================================================================\n",
      "✓ DATA IS CLEAN AND READY FOR FEATURE SCALING!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# PHASE 3 STEP 11: LOAD AND INSPECT DATA\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/raw/creditcard.csv')\n",
    "print(f\"✓ Data loaded: {df.shape}\")\n",
    "\n",
    "# Quick inspection\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total transactions: {len(df):,}\")\n",
    "print(f\"Total features: {df.shape[1]}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Handle duplicates\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REMOVING DUPLICATES\")\n",
    "print(\"=\"*70)\n",
    "duplicates_count = df.duplicated().sum()\n",
    "print(f\"Duplicates found: {duplicates_count}\")\n",
    "\n",
    "if duplicates_count > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"✓ Removed {duplicates_count:,} duplicate rows\")\n",
    "    print(f\"✓ Clean dataset size: {df.shape}\")\n",
    "    print(f\"✓ Duplicates remaining: {df.duplicated().sum()}\")\n",
    "else:\n",
    "    print(\"✓ No duplicates found\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print(df['Class'].value_counts())\n",
    "print(f\"\\nFraud rate: {df['Class'].mean()*100:.3f}%\")\n",
    "\n",
    "\n",
    "# PRE-MODELING VALIDATION CHECKS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECK 1: CLASS IMBALANCE SEVERITY\")\n",
    "print(\"=\"*70)\n",
    "fraud_count = df['Class'].sum()\n",
    "legit_count = len(df) - fraud_count\n",
    "imbalance_ratio = legit_count / fraud_count\n",
    "\n",
    "print(f\"Legitimate transactions: {legit_count:,}\")\n",
    "print(f\"Fraud transactions: {fraud_count:,}\")\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "if imbalance_ratio > 100:\n",
    "    print(\"  SEVERE imbalance - will need special handling in modeling\")\n",
    "else:\n",
    "    print(\"✓ Moderate imbalance - standard techniques will work\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECK 2: FEATURE DISTRIBUTIONS\")\n",
    "print(\"=\"*70)\n",
    "# Check Amount column specifically (our only non-PCA feature besides Time)\n",
    "print(f\"Amount column statistics:\")\n",
    "print(f\"  Min: ${df['Amount'].min():.2f}\")\n",
    "print(f\"  Max: ${df['Amount'].max():.2f}\")\n",
    "print(f\"  Mean: ${df['Amount'].mean():.2f}\")\n",
    "print(f\"  Median: ${df['Amount'].median():.2f}\")\n",
    "print(f\"  Std Dev: ${df['Amount'].std():.2f}\")\n",
    "\n",
    "# Check for extreme outliers\n",
    "q99 = df['Amount'].quantile(0.99)\n",
    "extreme_outliers = (df['Amount'] > q99).sum()\n",
    "print(f\"\\nTransactions above 99th percentile: {extreme_outliers:,}\")\n",
    "print(\"✓ We'll handle these with robust scaling\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECK 3: ZERO VARIANCE IN INPUT FEATURES\")\n",
    "print(\"=\"*70)\n",
    "# Check if any INPUT FEATURES have zero or near-zero variance\n",
    "# (We exclude Class because it's our TARGET, not a feature)\n",
    "feature_cols = [col for col in df.columns if col != 'Class']\n",
    "variances = df[feature_cols].var()\n",
    "low_variance = variances[variances < 0.01]\n",
    "\n",
    "if len(low_variance) == 0:\n",
    "    print(\"✓ All input features have sufficient variance\")\n",
    "else:\n",
    "    print(f\"  Input features with low variance: {len(low_variance)}\")\n",
    "    print(low_variance)\n",
    "\n",
    "print(f\"\\nNote: Class (target variable) has low variance by design\")\n",
    "print(f\"      This is normal for imbalanced classification problems\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECK 4: FEATURE TYPES\")\n",
    "print(\"=\"*70)\n",
    "print(\"INPUT FEATURES (30 total):\")\n",
    "print(\"  • V1-V28: Already PCA-transformed (DON'T scale again)\")\n",
    "print(\"  • Time: Needs scaling (seconds since first transaction)\")\n",
    "print(\"  • Amount: Needs scaling (transaction dollar amounts)\")\n",
    "print(\"\\nTARGET VARIABLE (1 total):\")\n",
    "print(\"  • Class: What we're predicting (0=Legit, 1=Fraud)\")\n",
    "print(\"\\n✓ We'll only scale Time and Amount in next step\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECK 5: FINAL DATA SHAPE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total columns: {df.shape[1]}\")\n",
    "print(f\"  ├── Input features: 30 (V1-V28, Time, Amount)\")\n",
    "print(f\"  └── Target variable: 1 (Class)\")\n",
    "print(f\"\\nTotal transactions: {df.shape[0]:,}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ DATA IS CLEAN AND READY FOR FEATURE SCALING!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "723260ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE SCALING\n",
      "======================================================================\n",
      "\n",
      "Before scaling:\n",
      "Time range: 0.00 to 172792.00\n",
      "Amount range: €0.00 to €25691.16\n",
      "V1 range (for comparison): -56.41 to 2.45\n",
      "\n",
      "After scaling:\n",
      "Time_scaled range: -2.00 to 1.64\n",
      "Amount_scaled range: -0.35 to 102.25\n",
      "\n",
      "✓ Features scaled successfully!\n",
      "   All features now on similar scale\n",
      "   Final feature count: 30\n",
      "\n",
      "First few rows of scaled data:\n",
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9       V10  ...       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  0.090794  ...  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425 -0.166974  ... -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  0.207643  ...  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024 -0.054952  ...  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  0.753074  ...  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Time_scaled  Amount_scaled  Class  \n",
      "0 -0.189115  0.133558 -0.021053    -1.996823       0.244200      0  \n",
      "1  0.125895 -0.008983  0.014724    -1.996823      -0.342584      0  \n",
      "2 -0.139097 -0.055353 -0.059752    -1.996802       1.158900      0  \n",
      "3 -0.221929  0.062723  0.061458    -1.996802       0.139886      0  \n",
      "4  0.502292  0.219422  0.215153    -1.996781      -0.073813      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# PHASE 3 PART 12: FEATURE SCALING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check current scales\n",
    "print(\"\\nBefore scaling:\")\n",
    "print(f\"Time range: {df['Time'].min():.2f} to {df['Time'].max():.2f}\")\n",
    "print(f\"Amount range: €{df['Amount'].min():.2f} to €{df['Amount'].max():.2f}\")\n",
    "print(f\"V1 range (for comparison): {df['V1'].min():.2f} to {df['V1'].max():.2f}\")\n",
    "\n",
    "# Create a copy for scaling\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# Scale Time and Amount\n",
    "scaler = StandardScaler()\n",
    "df_scaled['Time_scaled'] = scaler.fit_transform(df[['Time']])\n",
    "df_scaled['Amount_scaled'] = scaler.fit_transform(df[['Amount']])\n",
    "\n",
    "# Drop original Time and Amount\n",
    "df_scaled = df_scaled.drop(['Time', 'Amount'], axis=1)\n",
    "\n",
    "# Reorder columns (put scaled features after V28, before Class)\n",
    "feature_cols = [f'V{i}' for i in range(1, 29)] + ['Time_scaled', 'Amount_scaled']\n",
    "df_scaled = df_scaled[feature_cols + ['Class']]\n",
    "\n",
    "print(\"\\nAfter scaling:\")\n",
    "print(f\"Time_scaled range: {df_scaled['Time_scaled'].min():.2f} to {df_scaled['Time_scaled'].max():.2f}\")\n",
    "print(f\"Amount_scaled range: {df_scaled['Amount_scaled'].min():.2f} to {df_scaled['Amount_scaled'].max():.2f}\")\n",
    "\n",
    "print(\"\\n✓ Features scaled successfully!\")\n",
    "print(f\"   All features now on similar scale\")\n",
    "print(f\"   Final feature count: {df_scaled.shape[1] - 1}\")  # -1 for Class column\n",
    "\n",
    "print(\"\\nFirst few rows of scaled data:\")\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1101115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPLITTING FEATURES AND TARGET\n",
      "======================================================================\n",
      "\n",
      "Features (X) shape: (283726, 30)\n",
      "  Total samples: 283,726\n",
      "  Total features: 30\n",
      "\n",
      "Target (y) shape: (283726,)\n",
      "  Total samples: 283,726\n",
      "  Target name: Class\n",
      "\n",
      "Target distribution:\n",
      "  Legitimate (0): 283,253 (99.83%)\n",
      "  Fraud (1): 473 (0.167%)\n",
      "\n",
      "Feature names:\n",
      "['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Time_scaled', 'Amount_scaled']\n",
      "\n",
      "✓ Data split complete!\n"
     ]
    }
   ],
   "source": [
    "# PHASE 3 PAER 13: FEATURE TARGET SPLIT\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPLITTING FEATURES AND TARGET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# X = features (everything except Class)\n",
    "# y = target (what we want to predict - Class)\n",
    "X = df_scaled.drop('Class', axis=1)\n",
    "y = df_scaled['Class']\n",
    "\n",
    "print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
    "print(f\"  Total samples: {X.shape[0]:,}\")\n",
    "print(f\"  Total features: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\nTarget (y) shape: {y.shape}\")\n",
    "print(f\"  Total samples: {y.shape[0]:,}\")\n",
    "print(f\"  Target name: {y.name}\")\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Legitimate (0): {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"  Fraud (1): {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.3f}%)\")\n",
    "\n",
    "print(f\"\\nFeature names:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "print(\"\\n✓ Data split complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab715079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAIN-TEST SPLIT\n",
      "======================================================================\n",
      "\n",
      "Training set:\n",
      "  Total samples: 226,980\n",
      "  Features: 30\n",
      "  Fraud rate: 0.167%\n",
      "  Frauds: 378\n",
      "  Legitimate: 226,602\n",
      "  Ratio (Legitimate:Fraud): 599.5:1\n",
      "\n",
      "Testing set:\n",
      "  Total samples: 56,746\n",
      "  Features: 30\n",
      "  Fraud rate: 0.167%\n",
      "  Frauds: 95\n",
      "  Legitimate: 56,651\n",
      "  Ratio (Legitimate:Fraud): 596.3:1\n",
      "\n",
      "✓ Split maintains fraud ratio in both sets!\n",
      "   Training: 226,980 samples (80%)\n",
      "   Testing: 56,746 samples (20%)\n",
      "\n",
      "======================================================================\n",
      "IMBALANCE SUMMARY\n",
      "======================================================================\n",
      "Training set ratio: 599.5:1 (226,602 legit : 378 fraud)\n",
      "Testing set ratio:  596.3:1 (56,651 legit : 95 fraud)\n",
      "\n",
      "Both sets maintain ~599:1 class imbalance\n"
     ]
    }
   ],
   "source": [
    "# PHASE 3 STEP 14 TRAIN-TEST SPLIT\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split: 80% for training, 20% for testing\n",
    "# stratify=y ensures same fraud ratio in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,    # Makes results repeatable\n",
    "    stratify=y          # Keeps same fraud proportion in both sets\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Total samples: {X_train.shape[0]:,}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Fraud rate: {y_train.mean()*100:.3f}%\")\n",
    "print(f\"  Frauds: {y_train.sum():,}\")\n",
    "print(f\"  Legitimate: {(y_train == 0).sum():,}\")\n",
    "\n",
    "# NEW: Calculate and display training set ratio\n",
    "train_legit = (y_train == 0).sum()\n",
    "train_fraud = y_train.sum()\n",
    "train_ratio = train_legit / train_fraud\n",
    "print(f\"  Ratio (Legitimate:Fraud): {train_ratio:.1f}:1\")\n",
    "\n",
    "print(f\"\\nTesting set:\")\n",
    "print(f\"  Total samples: {X_test.shape[0]:,}\")\n",
    "print(f\"  Features: {X_test.shape[1]}\")\n",
    "print(f\"  Fraud rate: {y_test.mean()*100:.3f}%\")\n",
    "print(f\"  Frauds: {y_test.sum():,}\")\n",
    "print(f\"  Legitimate: {(y_test == 0).sum():,}\")\n",
    "\n",
    "# NEW: Calculate and display testing set ratio\n",
    "test_legit = (y_test == 0).sum()\n",
    "test_fraud = y_test.sum()\n",
    "test_ratio = test_legit / test_fraud\n",
    "print(f\"  Ratio (Legitimate:Fraud): {test_ratio:.1f}:1\")\n",
    "\n",
    "print(f\"\\n✓ Split maintains fraud ratio in both sets!\")\n",
    "print(f\"   Training: {len(X_train):,} samples ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"   Testing: {len(X_test):,} samples ({len(X_test)/len(X)*100:.0f}%)\")\n",
    "\n",
    "# NEW: Summary comparison\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"IMBALANCE SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Training set ratio: {train_ratio:.1f}:1 ({train_legit:,} legit : {train_fraud:,} fraud)\")\n",
    "print(f\"Testing set ratio:  {test_ratio:.1f}:1 ({test_legit:,} legit : {test_fraud:,} fraud)\")\n",
    "print(f\"\\nBoth sets maintain ~{train_ratio:.0f}:1 class imbalance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77bbc1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 15: COMPARING SMOTE VS ADASYN\n",
      "======================================================================\n",
      "\n",
      " ORIGINAL IMBALANCE:\n",
      "   Training frauds: 378\n",
      "   Training legitimate: 226,602\n",
      "   Imbalance ratio: 599:1\n",
      "\n",
      "======================================================================\n",
      "METHOD 1: SMOTE (Synthetic Minority Over-sampling Technique)\n",
      "======================================================================\n",
      "\n",
      " SMOTE APPROACH:\n",
      "   • Creates equal synthetic samples near ALL fraud examples\n",
      "   • Uniform distribution across fraud space\n",
      "   • Industry-standard for extreme imbalance\n",
      "\n",
      "Before SMOTE:\n",
      "  Legitimate (0): 226,602\n",
      "  Fraud (1): 378\n",
      "  Ratio: 599:1\n",
      "\n",
      "After SMOTE:\n",
      "  Legitimate (0): 226,602\n",
      "  Fraud (1): 226,602\n",
      "  Ratio: 1:1 (perfectly balanced!)\n",
      "\n",
      "✓ SMOTE complete!\n",
      "   Original training size: 226,980\n",
      "   SMOTE training size: 453,204\n",
      "   Synthetic frauds created: 226,224\n",
      "\n",
      "======================================================================\n",
      "METHOD 2: ADASYN (Adaptive Synthetic Sampling)\n",
      "======================================================================\n",
      "\n",
      " ADASYN APPROACH:\n",
      "   • Creates MORE synthetic samples near hard-to-classify frauds\n",
      "   • Adaptive distribution based on classification difficulty\n",
      "   • Focuses on challenging boundary cases\n",
      "\n",
      "Before ADASYN:\n",
      "  Legitimate (0): 226,602\n",
      "  Fraud (1): 378\n",
      "  Ratio: 599:1\n",
      "\n",
      "After ADASYN:\n",
      "  Legitimate (0): 226,602\n",
      "  Fraud (1): 226,638\n",
      "  Ratio: ~1:1 (adaptively balanced!)\n",
      "\n",
      "✓ ADASYN complete!\n",
      "   Original training size: 226,980\n",
      "   ADASYN training size: 453,240\n",
      "   Synthetic frauds created: 226,260\n",
      "\n",
      "======================================================================\n",
      "COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Dataset Sizes:\n",
      "  Original training:   226,980 samples\n",
      "  SMOTE balanced:      453,204 samples\n",
      "  ADASYN balanced:     453,240 samples\n",
      "\n",
      "Synthetic Frauds Created:\n",
      "  SMOTE:   226,224 synthetic frauds\n",
      "  ADASYN:  226,260 synthetic frauds\n",
      "\n",
      "Fraud Distribution:\n",
      "  SMOTE fraud samples:   226,602\n",
      "  ADASYN fraud samples:  226,638\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION CHECKS\n",
      "======================================================================\n",
      "\n",
      " SMOTE Training Data:\n",
      "  X_train_smote shape:  (453204, 30)\n",
      "  y_train_smote shape:  (453204,)\n",
      "  Fraud ratio: 1.00:1\n",
      "\n",
      " ADASYN Training Data:\n",
      "  X_train_adasyn shape: (453240, 30)\n",
      "  y_train_adasyn shape: (453240,)\n",
      "  Fraud ratio: 1.00:1\n",
      "\n",
      " Test Data (UNCHANGED for both methods):\n",
      "  X_test shape: (56746, 30)\n",
      "  y_test frauds: 95\n",
      "  y_test ratio: 596:1\n",
      "\n",
      "======================================================================\n",
      "KEY DIFFERENCES\n",
      "======================================================================\n",
      "\n",
      "SMOTE:\n",
      "  ✓ Uniform synthetic generation\n",
      "  ✓ Predictable 1:1 balance\n",
      "  ✓ Industry standard approach\n",
      "  ✓ Simpler algorithm (faster)\n",
      "\n",
      "ADASYN:\n",
      "  ✓ Adaptive synthetic generation\n",
      "  ✓ Focuses on hard-to-classify cases\n",
      "  ✓ May create slightly different total counts\n",
      "  ✓ Potentially better quality synthetics\n",
      "\n",
      " NEXT STEPS:\n",
      "   We will train models on BOTH datasets and compare:\n",
      "   • Fraud recall (primary metric)\n",
      "   • Precision\n",
      "   • Training vs test performance gap\n",
      "   • Overall model quality\n",
      "\n",
      "✓ Both balanced datasets ready for model training!\n",
      "\n",
      "======================================================================\n",
      "USAGE NOTES\n",
      "======================================================================\n",
      "\n",
      " PURPOSE OF THESE BALANCED DATASETS:\n",
      "   These pre-balanced datasets (X_train_smote, X_train_adasyn)\n",
      "   will be used for FINAL model training AFTER cross-validation.\n",
      "\n",
      " CROSS-VALIDATION WORKFLOW:\n",
      "   1. Cross-validation will use ORIGINAL X_train (226,980 samples)\n",
      "      - Apply SMOTE/ADASYN fresh inside each CV fold\n",
      "      - Validate on REAL frauds (not synthetic)\n",
      "      - Get expected performance estimates\n",
      "   \n",
      "   2. Final model training will use THESE pre-balanced datasets\n",
      "      - X_train_smote (453,204 samples) for SMOTE models\n",
      "      - X_train_adasyn (453,240 samples) for ADASYN models\n",
      "   \n",
      "   3. Final testing will use X_test (56,746 samples, 95 REAL frauds)\n",
      "      - Test ONCE on best model\n",
      "      - Get honest real-world performance\n",
      "\n",
      " WHY CREATE THESE NOW?\n",
      "   - Shows SMOTE vs ADASYN comparison clearly\n",
      "   - Ready for final model training (saves time later)\n",
      "   - Educational: see exact synthetic fraud counts\n",
      "\n",
      "  IMPORTANT:\n",
      "   Test set (X_test, y_test) remains COMPLETELY UNTOUCHED\n",
      "   - NO SMOTE applied to test data\n",
      "   - Preserves real-world fraud rate (0.167%)\n",
      "   - Used ONLY for final honest evaluation\n"
     ]
    }
   ],
   "source": [
    "# PHASE 3 STEP 15 HANDLE EXTREME IMBALANCE WITH SMOTE AND ADASYN\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 15: COMPARING SMOTE VS ADASYN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n ORIGINAL IMBALANCE:\")\n",
    "print(f\"   Training frauds: {y_train.sum():,}\")\n",
    "print(f\"   Training legitimate: {(y_train==0).sum():,}\")\n",
    "print(f\"   Imbalance ratio: {(y_train==0).sum() / y_train.sum():.0f}:1\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"METHOD 1: SMOTE (Synthetic Minority Over-sampling Technique)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n SMOTE APPROACH:\")\n",
    "print(\"   • Creates equal synthetic samples near ALL fraud examples\")\n",
    "print(\"   • Uniform distribution across fraud space\")\n",
    "print(\"   • Industry-standard for extreme imbalance\")\n",
    "\n",
    "print(\"\\nBefore SMOTE:\")\n",
    "print(f\"  Legitimate (0): {(y_train == 0).sum():,}\")\n",
    "print(f\"  Fraud (1): {y_train.sum():,}\")\n",
    "print(f\"  Ratio: {(y_train==0).sum() / y_train.sum():.0f}:1\")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "print(f\"  Legitimate (0): {(y_train_smote == 0).sum():,}\")\n",
    "print(f\"  Fraud (1): {(y_train_smote == 1).sum():,}\")\n",
    "print(f\"  Ratio: 1:1 (perfectly balanced!)\")\n",
    "\n",
    "print(f\"\\n✓ SMOTE complete!\")\n",
    "print(f\"   Original training size: {X_train.shape[0]:,}\")\n",
    "print(f\"   SMOTE training size: {X_train_smote.shape[0]:,}\")\n",
    "print(f\"   Synthetic frauds created: {X_train_smote.shape[0] - X_train.shape[0]:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"METHOD 2: ADASYN (Adaptive Synthetic Sampling)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n ADASYN APPROACH:\")\n",
    "print(\"   • Creates MORE synthetic samples near hard-to-classify frauds\")\n",
    "print(\"   • Adaptive distribution based on classification difficulty\")\n",
    "print(\"   • Focuses on challenging boundary cases\")\n",
    "\n",
    "print(\"\\nBefore ADASYN:\")\n",
    "print(f\"  Legitimate (0): {(y_train == 0).sum():,}\")\n",
    "print(f\"  Fraud (1): {y_train.sum():,}\")\n",
    "print(f\"  Ratio: {(y_train==0).sum() / y_train.sum():.0f}:1\")\n",
    "\n",
    "# Apply ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nAfter ADASYN:\")\n",
    "print(f\"  Legitimate (0): {(y_train_adasyn == 0).sum():,}\")\n",
    "print(f\"  Fraud (1): {(y_train_adasyn == 1).sum():,}\")\n",
    "print(f\"  Ratio: ~1:1 (adaptively balanced!)\")\n",
    "\n",
    "print(f\"\\n✓ ADASYN complete!\")\n",
    "print(f\"   Original training size: {X_train.shape[0]:,}\")\n",
    "print(f\"   ADASYN training size: {X_train_adasyn.shape[0]:,}\")\n",
    "print(f\"   Synthetic frauds created: {X_train_adasyn.shape[0] - X_train.shape[0]:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDataset Sizes:\")\n",
    "print(f\"  Original training:  {X_train.shape[0]:>8,} samples\")\n",
    "print(f\"  SMOTE balanced:     {X_train_smote.shape[0]:>8,} samples\")\n",
    "print(f\"  ADASYN balanced:    {X_train_adasyn.shape[0]:>8,} samples\")\n",
    "\n",
    "print(\"\\nSynthetic Frauds Created:\")\n",
    "print(f\"  SMOTE:  {X_train_smote.shape[0] - X_train.shape[0]:>8,} synthetic frauds\")\n",
    "print(f\"  ADASYN: {X_train_adasyn.shape[0] - X_train.shape[0]:>8,} synthetic frauds\")\n",
    "\n",
    "print(\"\\nFraud Distribution:\")\n",
    "smote_fraud_count = (y_train_smote == 1).sum()\n",
    "adasyn_fraud_count = (y_train_adasyn == 1).sum()\n",
    "print(f\"  SMOTE fraud samples:  {smote_fraud_count:>8,}\")\n",
    "print(f\"  ADASYN fraud samples: {adasyn_fraud_count:>8,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFICATION CHECKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n SMOTE Training Data:\")\n",
    "print(f\"  X_train_smote shape:  {X_train_smote.shape}\")\n",
    "print(f\"  y_train_smote shape:  {y_train_smote.shape}\")\n",
    "print(f\"  Fraud ratio: {(y_train_smote == 0).sum() / (y_train_smote == 1).sum():.2f}:1\")\n",
    "\n",
    "print(\"\\n ADASYN Training Data:\")\n",
    "print(f\"  X_train_adasyn shape: {X_train_adasyn.shape}\")\n",
    "print(f\"  y_train_adasyn shape: {y_train_adasyn.shape}\")\n",
    "print(f\"  Fraud ratio: {(y_train_adasyn == 0).sum() / (y_train_adasyn == 1).sum():.2f}:1\")\n",
    "\n",
    "print(\"\\n Test Data (UNCHANGED for both methods):\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_test frauds: {y_test.sum()}\")\n",
    "print(f\"  y_test ratio: {(y_test == 0).sum() / y_test.sum():.0f}:1\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY DIFFERENCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nSMOTE:\")\n",
    "print(\"  ✓ Uniform synthetic generation\")\n",
    "print(\"  ✓ Predictable 1:1 balance\")\n",
    "print(\"  ✓ Industry standard approach\")\n",
    "print(\"  ✓ Simpler algorithm (faster)\")\n",
    "\n",
    "print(\"\\nADASYN:\")\n",
    "print(\"  ✓ Adaptive synthetic generation\")\n",
    "print(\"  ✓ Focuses on hard-to-classify cases\")\n",
    "print(\"  ✓ May create slightly different total counts\")\n",
    "print(\"  ✓ Potentially better quality synthetics\")\n",
    "\n",
    "print(\"\\n NEXT STEPS:\")\n",
    "print(\"   We will train models on BOTH datasets and compare:\")\n",
    "print(\"   • Fraud recall (primary metric)\")\n",
    "print(\"   • Precision\")\n",
    "print(\"   • Training vs test performance gap\")\n",
    "print(\"   • Overall model quality\")\n",
    "\n",
    "print(\"\\n✓ Both balanced datasets ready for model training!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"USAGE NOTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n PURPOSE OF THESE BALANCED DATASETS:\")\n",
    "print(\"   These pre-balanced datasets (X_train_smote, X_train_adasyn)\")\n",
    "print(\"   will be used for FINAL model training AFTER cross-validation.\")\n",
    "\n",
    "print(\"\\n CROSS-VALIDATION WORKFLOW:\")\n",
    "print(\"   1. Cross-validation will use ORIGINAL X_train (226,980 samples)\")\n",
    "print(\"      - Apply SMOTE/ADASYN fresh inside each CV fold\")\n",
    "print(\"      - Validate on REAL frauds (not synthetic)\")\n",
    "print(\"      - Get expected performance estimates\")\n",
    "print(\"   \")\n",
    "print(\"   2. Final model training will use THESE pre-balanced datasets\")\n",
    "print(\"      - X_train_smote (453,204 samples) for SMOTE models\")\n",
    "print(\"      - X_train_adasyn (453,240 samples) for ADASYN models\")\n",
    "print(\"   \")\n",
    "print(\"   3. Final testing will use X_test (56,746 samples, 95 REAL frauds)\")\n",
    "print(\"      - Test ONCE on best model\")\n",
    "print(\"      - Get honest real-world performance\")\n",
    "\n",
    "print(\"\\n WHY CREATE THESE NOW?\")\n",
    "print(\"   - Shows SMOTE vs ADASYN comparison clearly\")\n",
    "print(\"   - Ready for final model training (saves time later)\")\n",
    "print(\"   - Educational: see exact synthetic fraud counts\")\n",
    "\n",
    "print(\"\\n  IMPORTANT:\")\n",
    "print(\"   Test set (X_test, y_test) remains COMPLETELY UNTOUCHED\")\n",
    "print(\"   - NO SMOTE applied to test data\")\n",
    "print(\"   - Preserves real-world fraud rate (0.167%)\")\n",
    "print(\"   - Used ONLY for final honest evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b91aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
