{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e15c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREDIT CARD FRAUD DETECTION - MODEL BUILDING\n",
      "======================================================================\n",
      "- Build and evaluate machine learning models\n",
      "- Use proper cross-validation methodology\n",
      "- Compare SMOTE vs ADASYN resampling techniques\n",
      "- Compare model performance\n",
      "- Select best model for deployment\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 1: LOADING DATA\n",
      "======================================================================\n",
      "\n",
      " Loading raw data from CSV...\n",
      "✓ Loaded 284,807 transactions\n",
      "  Features: 30\n",
      "  Fraud cases: 492 (0.173%)\n",
      "  Normal cases: 284,315 (99.827%)\n",
      "  Imbalance ratio: 1:577.9\n",
      "\n",
      " Quick Data Quality Check:\n",
      "  Missing values: 0\n",
      "  Duplicates: 1081\n",
      "\n",
      " Removing duplicates...\n",
      "✓ Removed 1,081 duplicate transactions\n",
      "  Original: 284,807 transactions\n",
      "  After cleaning: 283,726 transactions\n",
      "  Fraud cases remaining: 473\n",
      "\n",
      " Data loaded and cleaned successfully!\n",
      "\n",
      "======================================================================\n",
      "STEP 2: TRAIN-TEST SPLIT\n",
      "======================================================================\n",
      "\n",
      " Splitting data into train (80%) and test (20%)...\n",
      "   Using stratified split to preserve fraud ratio in both sets\n",
      "\n",
      "✓ Split complete!\n",
      "\n",
      " Training Set:\n",
      "   Total samples: 226,980\n",
      "   Fraud cases: 378 (0.167%)\n",
      "   Normal cases: 226,602 (99.833%)\n",
      "\n",
      " Test Set:\n",
      "   Total samples: 56,746\n",
      "   Fraud cases: 95 (0.167%)\n",
      "   Normal cases: 56,651 (99.833%)\n",
      "\n",
      " Fraud ratio preserved: 0.001665 (train) vs 0.001674 (test)\n",
      "\n",
      "======================================================================\n",
      "STEP 3: FEATURE SCALING\n",
      "======================================================================\n",
      "\n",
      " Scaling Time and Amount features...\n",
      "   WHY: Time and Amount have different scales than V1-V28 (which are already PCA-transformed)\n",
      "   METHOD: StandardScaler (mean=0, std=1)\n",
      "   CRITICAL: Fit scaler on TRAINING data only to prevent data leakage!\n",
      "\n",
      " Fitting scalers on training data...\n",
      "✓ Time scaler fitted - mean: 94900.22, std: 47488.11\n",
      "✓ Amount scaler fitted - mean: 88.39, std: 245.77\n",
      "\n",
      " Transforming features...\n",
      "✓ Features scaled successfully!\n",
      "  Final feature count: 30\n",
      "  Feature order: V1-V28, Time_scaled, Amount_scaled\n",
      "\n",
      " Scaling Verification (Training Set):\n",
      "  Time_scaled - mean: -0.000000, std: 1.000002\n",
      "  Amount_scaled - mean: 0.000000, std: 1.000002\n",
      "\n",
      "======================================================================\n",
      "STEP 4: HANDLING CLASS IMBALANCE - SMOTE vs ADASYN\n",
      "======================================================================\n",
      "\n",
      " Creating balanced datasets with both methods...\n",
      "   WHY: Models struggle to learn from severely imbalanced data (599:1 ratio)\n",
      "   METHODS: SMOTE (uniform sampling) vs ADASYN (adaptive sampling)\n",
      "   CRITICAL: Apply to TRAINING data only - test must reflect real-world distribution!\n",
      "\n",
      " Original Training Set (Before Resampling):\n",
      "   Non-fraud: 226,602\n",
      "   Fraud: 378\n",
      "   Ratio: 1:599.5\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SMOTE (Synthetic Minority Over-sampling Technique)\n",
      "----------------------------------------------------------------------\n",
      "Creates synthetic samples uniformly between minority class neighbors\n",
      "\n",
      " After SMOTE:\n",
      "   Non-fraud: 226,602\n",
      "   Fraud: 226,602\n",
      "   Ratio: 1:1.0\n",
      "   Synthetic frauds created: 226,224\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ADASYN (Adaptive Synthetic Sampling)\n",
      "----------------------------------------------------------------------\n",
      "Creates MORE samples in harder-to-learn regions (adaptive approach)\n",
      "\n",
      " After ADASYN:\n",
      "   Non-fraud: 226,602\n",
      "   Fraud: 226,638\n",
      "   Ratio: 1:1.0\n",
      "   Synthetic frauds created: 226,260\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SMOTE vs ADASYN Comparison\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Total fraud samples (including synthetic):\n",
      "   SMOTE:  226,602\n",
      "   ADASYN: 226,638\n",
      "   Difference: 36 samples\n",
      "\n",
      " ADASYN created 36 MORE samples than SMOTE\n",
      "   This suggests ADASYN identified more difficult boundary regions\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION COMPLETE - READY FOR MODEL BUILDING!\n",
      "======================================================================\n",
      "\n",
      " Final Dataset Summary:\n",
      "\n",
      "   Original Training Set:\n",
      "      Total samples: 226,980\n",
      "      Fraud samples: 378\n",
      "      Features: 30\n",
      "      Class distribution: 0.167% fraud (imbalanced)\n",
      "\n",
      "   SMOTE-Balanced Training Set:\n",
      "      Total samples: 453,204\n",
      "      Fraud samples: 226,602\n",
      "      Features: 30\n",
      "      Class distribution: 50/50 (balanced)\n",
      "\n",
      "   ADASYN-Balanced Training Set:\n",
      "      Total samples: 453,240\n",
      "      Fraud samples: 226,638\n",
      "      Features: 30\n",
      "      Class distribution: ~50/50 (balanced)\n",
      "\n",
      "   Test Set (Original Distribution):\n",
      "      Total samples: 56,746\n",
      "      Fraud samples: 95\n",
      "      Features: 30\n",
      "      Class distribution: 0.167% fraud (real-world)\n",
      "\n",
      " KEY ACCOMPLISHMENTS:\n",
      "   ✓ Proper train-test split with stratification\n",
      "   ✓ No data leakage (scalers fitted on train only)\n",
      "   ✓ Two balanced training datasets created (SMOTE and ADASYN)\n",
      "   ✓ Test set preserves real-world distribution for valid evaluation\n",
      "   ✓ Ready to compare both resampling techniques in cross-validation!\n",
      "\n",
      " NEXT STEPS:\n",
      "   1. Build models with SMOTE-balanced data\n",
      "   2. Build models with ADASYN-balanced data\n",
      "   3. Compare which resampling technique performs better\n",
      "   4. Select best combination of model + resampling technique\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREDIT CARD FRAUD DETECTION - MODEL BUILDING NOTEBOOK\n",
    "# Phase 4: Building ML Models with Cross-Validation\n",
    "# STEP 16: Modeling Notebook\n",
    "# ============================================================================\n",
    "\n",
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, roc_auc_score, confusion_matrix, \n",
    "                             roc_curve, precision_recall_curve, auc, make_scorer)\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREDIT CARD FRAUD DETECTION - MODEL BUILDING\")\n",
    "print(\"=\"*70)\n",
    "print(\"- Build and evaluate machine learning models\")\n",
    "print(\"- Use proper cross-validation methodology\")\n",
    "print(\"- Compare SMOTE vs ADASYN resampling techniques\")\n",
    "print(\"- Compare model performance\")\n",
    "print(\"- Select best model for deployment\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load Raw Data\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Loading raw data from CSV...\")\n",
    "\n",
    "df = pd.read_csv('../data/raw/creditcard.csv')\n",
    "\n",
    "print(f\"✓ Loaded {len(df):,} transactions\")\n",
    "print(f\"  Features: {len(df.columns) - 1}\")\n",
    "print(f\"  Fraud cases: {df['Class'].sum():,} ({df['Class'].mean()*100:.3f}%)\")\n",
    "print(f\"  Normal cases: {(df['Class'] == 0).sum():,} ({(df['Class'] == 0).mean()*100:.3f}%)\")\n",
    "print(f\"  Imbalance ratio: 1:{(df['Class'] == 0).sum() / df['Class'].sum():.1f}\")\n",
    "\n",
    "# Quick data quality check\n",
    "print(f\"\\n Quick Data Quality Check:\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicates: {df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates if found\n",
    "if df.duplicated().sum() > 0:\n",
    "    print(f\"\\n Removing duplicates...\")\n",
    "    original_count = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    removed_count = original_count - len(df)\n",
    "    \n",
    "    print(f\"✓ Removed {removed_count:,} duplicate transactions\")\n",
    "    print(f\"  Original: {original_count:,} transactions\")\n",
    "    print(f\"  After cleaning: {len(df):,} transactions\")\n",
    "    print(f\"  Fraud cases remaining: {df['Class'].sum():,}\")\n",
    "else:\n",
    "    print(f\"\\n✓ No duplicates found - data is clean!\")\n",
    "\n",
    "print(f\"\\n Data loaded and cleaned successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Train-Test Split (BEFORE any preprocessing)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Splitting data into train (80%) and test (20%)...\")\n",
    "print(\"   Using stratified split to preserve fraud ratio in both sets\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Stratified split - CRITICAL for imbalanced data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Split complete!\")\n",
    "print(f\"\\n Training Set:\")\n",
    "print(f\"   Total samples: {len(X_train):,}\")\n",
    "print(f\"   Fraud cases: {y_train.sum():,} ({y_train.mean()*100:.3f}%)\")\n",
    "print(f\"   Normal cases: {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.3f}%)\")\n",
    "\n",
    "print(f\"\\n Test Set:\")\n",
    "print(f\"   Total samples: {len(X_test):,}\")\n",
    "print(f\"   Fraud cases: {y_test.sum():,} ({y_test.mean()*100:.3f}%)\")\n",
    "print(f\"   Normal cases: {(y_test == 0).sum():,} ({(y_test == 0).mean()*100:.3f}%)\")\n",
    "\n",
    "print(f\"\\n Fraud ratio preserved: {y_train.mean():.6f} (train) vs {y_test.mean():.6f} (test)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Feature Scaling (Fit on Train, Transform Both)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: FEATURE SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Scaling Time and Amount features...\")\n",
    "print(\"   WHY: Time and Amount have different scales than V1-V28 (which are already PCA-transformed)\")\n",
    "print(\"   METHOD: StandardScaler (mean=0, std=1)\")\n",
    "print(\"   CRITICAL: Fit scaler on TRAINING data only to prevent data leakage!\")\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_time = StandardScaler()\n",
    "scaler_amount = StandardScaler()\n",
    "\n",
    "# Fit scalers on TRAINING data only\n",
    "print(\"\\n Fitting scalers on training data...\")\n",
    "scaler_time.fit(X_train[['Time']])\n",
    "scaler_amount.fit(X_train[['Amount']])\n",
    "\n",
    "print(f\"✓ Time scaler fitted - mean: {scaler_time.mean_[0]:.2f}, std: {scaler_time.scale_[0]:.2f}\")\n",
    "print(f\"✓ Amount scaler fitted - mean: {scaler_amount.mean_[0]:.2f}, std: {scaler_amount.scale_[0]:.2f}\")\n",
    "\n",
    "# Transform BOTH train and test using the SAME scalers\n",
    "print(\"\\n Transforming features...\")\n",
    "\n",
    "# Create copies to avoid modifying original data\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Apply transformations\n",
    "X_train_scaled['Time_scaled'] = scaler_time.transform(X_train[['Time']])\n",
    "X_train_scaled['Amount_scaled'] = scaler_amount.transform(X_train[['Amount']])\n",
    "\n",
    "X_test_scaled['Time_scaled'] = scaler_time.transform(X_test[['Time']])\n",
    "X_test_scaled['Amount_scaled'] = scaler_amount.transform(X_test[['Amount']])\n",
    "\n",
    "# Drop original Time and Amount columns\n",
    "X_train_scaled = X_train_scaled.drop(['Time', 'Amount'], axis=1)\n",
    "X_test_scaled = X_test_scaled.drop(['Time', 'Amount'], axis=1)\n",
    "\n",
    "# Reorder columns for consistency: V1-V28, then scaled features\n",
    "feature_cols = [f'V{i}' for i in range(1, 29)] + ['Time_scaled', 'Amount_scaled']\n",
    "X_train_scaled = X_train_scaled[feature_cols]\n",
    "X_test_scaled = X_test_scaled[feature_cols]\n",
    "\n",
    "print(f\"✓ Features scaled successfully!\")\n",
    "print(f\"  Final feature count: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  Feature order: V1-V28, Time_scaled, Amount_scaled\")\n",
    "\n",
    "# Verify scaling worked\n",
    "print(f\"\\n Scaling Verification (Training Set):\")\n",
    "print(f\"  Time_scaled - mean: {X_train_scaled['Time_scaled'].mean():.6f}, std: {X_train_scaled['Time_scaled'].std():.6f}\")\n",
    "print(f\"  Amount_scaled - mean: {X_train_scaled['Amount_scaled'].mean():.6f}, std: {X_train_scaled['Amount_scaled'].std():.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Apply BOTH SMOTE and ADASYN for Comparison\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: HANDLING CLASS IMBALANCE - SMOTE vs ADASYN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Creating balanced datasets with both methods...\")\n",
    "print(\"   WHY: Models struggle to learn from severely imbalanced data (599:1 ratio)\")\n",
    "print(\"   METHODS: SMOTE (uniform sampling) vs ADASYN (adaptive sampling)\")\n",
    "print(\"   CRITICAL: Apply to TRAINING data only - test must reflect real-world distribution!\")\n",
    "\n",
    "print(f\"\\n Original Training Set (Before Resampling):\")\n",
    "print(f\"   Non-fraud: {(y_train == 0).sum():,}\")\n",
    "print(f\"   Fraud: {(y_train == 1).sum():,}\")\n",
    "print(f\"   Ratio: 1:{(y_train == 0).sum() / (y_train == 1).sum():.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Apply SMOTE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"SMOTE (Synthetic Minority Over-sampling Technique)\")\n",
    "print(\"-\"*70)\n",
    "print(\"Creates synthetic samples uniformly between minority class neighbors\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n After SMOTE:\")\n",
    "print(f\"   Non-fraud: {(y_train_smote == 0).sum():,}\")\n",
    "print(f\"   Fraud: {(y_train_smote == 1).sum():,}\")\n",
    "print(f\"   Ratio: 1:{(y_train_smote == 0).sum() / (y_train_smote == 1).sum():.1f}\")\n",
    "print(f\"   Synthetic frauds created: {(y_train_smote == 1).sum() - y_train.sum():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Apply ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ADASYN (Adaptive Synthetic Sampling)\")\n",
    "print(\"-\"*70)\n",
    "print(\"Creates MORE samples in harder-to-learn regions (adaptive approach)\")\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n After ADASYN:\")\n",
    "print(f\"   Non-fraud: {(y_train_adasyn == 0).sum():,}\")\n",
    "print(f\"   Fraud: {(y_train_adasyn == 1).sum():,}\")\n",
    "print(f\"   Ratio: 1:{(y_train_adasyn == 0).sum() / (y_train_adasyn == 1).sum():.1f}\")\n",
    "print(f\"   Synthetic frauds created: {(y_train_adasyn == 1).sum() - y_train.sum():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Compare SMOTE vs ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"SMOTE vs ADASYN Comparison\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "smote_samples = (y_train_smote == 1).sum()\n",
    "adasyn_samples = (y_train_adasyn == 1).sum()\n",
    "difference = abs(smote_samples - adasyn_samples)\n",
    "\n",
    "print(f\"\\nTotal fraud samples (including synthetic):\")\n",
    "print(f\"   SMOTE:  {smote_samples:,}\")\n",
    "print(f\"   ADASYN: {adasyn_samples:,}\")\n",
    "print(f\"   Difference: {difference:,} samples\")\n",
    "\n",
    "if adasyn_samples > smote_samples:\n",
    "    print(f\"\\n ADASYN created {difference:,} MORE samples than SMOTE\")\n",
    "    print(\"   This suggests ADASYN identified more difficult boundary regions\")\n",
    "elif smote_samples > adasyn_samples:\n",
    "    print(f\"\\n SMOTE created {difference:,} MORE samples than ADASYN\")\n",
    "    print(\"   This suggests uniform sampling covered more ground\")\n",
    "else:\n",
    "    print(\"\\n Both methods created the same number of samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPARATION COMPLETE - READY FOR MODEL BUILDING!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Final Dataset Summary:\")\n",
    "\n",
    "print(f\"\\n   Original Training Set:\")\n",
    "print(f\"      Total samples: {X_train_scaled.shape[0]:,}\")\n",
    "print(f\"      Fraud samples: {y_train.sum():,}\")\n",
    "print(f\"      Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"      Class distribution: {y_train.mean()*100:.3f}% fraud (imbalanced)\")\n",
    "\n",
    "print(f\"\\n   SMOTE-Balanced Training Set:\")\n",
    "print(f\"      Total samples: {X_train_smote.shape[0]:,}\")\n",
    "print(f\"      Fraud samples: {y_train_smote.sum():,}\")\n",
    "print(f\"      Features: {X_train_smote.shape[1]}\")\n",
    "print(f\"      Class distribution: 50/50 (balanced)\")\n",
    "\n",
    "print(f\"\\n   ADASYN-Balanced Training Set:\")\n",
    "print(f\"      Total samples: {X_train_adasyn.shape[0]:,}\")\n",
    "print(f\"      Fraud samples: {y_train_adasyn.sum():,}\")\n",
    "print(f\"      Features: {X_train_adasyn.shape[1]}\")\n",
    "print(f\"      Class distribution: ~50/50 (balanced)\")\n",
    "\n",
    "print(f\"\\n   Test Set (Original Distribution):\")\n",
    "print(f\"      Total samples: {X_test_scaled.shape[0]:,}\")\n",
    "print(f\"      Fraud samples: {y_test.sum():,}\")\n",
    "print(f\"      Features: {X_test_scaled.shape[1]}\")\n",
    "print(f\"      Class distribution: {y_test.mean()*100:.3f}% fraud (real-world)\")\n",
    "\n",
    "print(\"\\n KEY ACCOMPLISHMENTS:\")\n",
    "print(\"   ✓ Proper train-test split with stratification\")\n",
    "print(\"   ✓ No data leakage (scalers fitted on train only)\")\n",
    "print(\"   ✓ Two balanced training datasets created (SMOTE and ADASYN)\")\n",
    "print(\"   ✓ Test set preserves real-world distribution for valid evaluation\")\n",
    "print(\"   ✓ Ready to compare both resampling techniques in cross-validation!\")\n",
    "\n",
    "print(\"\\n NEXT STEPS:\")\n",
    "print(\"   1. Build models with SMOTE-balanced data\")\n",
    "print(\"   2. Build models with ADASYN-balanced data\")\n",
    "print(\"   3. Compare which resampling technique performs better\")\n",
    "print(\"   4. Select best combination of model + resampling technique\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f168939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL 1: LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "\n",
      " WHAT WE'RE DOING:\n",
      "   - Building baseline Logistic Regression model\n",
      "   - Testing TWO resampling techniques: SMOTE vs ADASYN\n",
      "   - Using 5-fold cross-validation for performance estimation\n",
      "   - Applying resampling INSIDE each CV fold (proper methodology)\n",
      "   - Validating on REAL data only (no synthetic frauds in validation)\n",
      "   - SAVING model and results to disk for later use\n",
      "\n",
      "======================================================================\n",
      "PART A: LOGISTIC REGRESSION + SMOTE\n",
      "======================================================================\n",
      "\n",
      " About SMOTE:\n",
      "   - Synthetic Minority Over-sampling Technique\n",
      "   - Creates synthetic samples by interpolating between existing frauds\n",
      "   - Places new samples along line segments between neighbors\n",
      "   - Good for general class imbalance\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Creating SMOTE Pipeline\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Building pipeline with 2 steps:\n",
      "   Step 1: SMOTE (balance training data)\n",
      "   Step 2: Logistic Regression (classification)\n",
      "\n",
      "+ SMOTE pipeline created successfully!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running 5-Fold Cross-Validation with SMOTE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Running 5-fold cross-validation...\n",
      "   Dataset: X_train_scaled (226,980 samples, 378 frauds)\n",
      "   Each fold: ~45,396 samples (~75 real frauds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "SMOTE Cross-Validation Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Cross-validation complete in 3.76 seconds\n",
      "\n",
      " Individual Fold Scores (SMOTE):\n",
      "   Fold 1: AUC = 0.9699\n",
      "   Fold 2: AUC = 0.9826\n",
      "   Fold 3: AUC = 0.9987\n",
      "   Fold 4: AUC = 0.9829\n",
      "   Fold 5: AUC = 0.9780\n",
      "\n",
      " SMOTE Summary Statistics:\n",
      "   Mean CV AUC:  0.9824\n",
      "   Std CV AUC:   0.0094\n",
      "   Min CV AUC:   0.9699\n",
      "   Max CV AUC:   0.9987\n",
      "\n",
      " 95% Confidence Interval:\n",
      "   0.9824 +/- 0.0184\n",
      "   Range: [0.9640, 1.0008]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Final SMOTE Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Training final model with SMOTE on all training data...\n",
      "+ SMOTE model training complete in 2.23 seconds\n",
      "\n",
      " SMOTE Model Summary:\n",
      "   Expected AUC: 0.9824 +/- 0.0094\n",
      "   Stability: Excellent\n",
      "\n",
      "======================================================================\n",
      "PART B: LOGISTIC REGRESSION + ADASYN\n",
      "======================================================================\n",
      "\n",
      " About ADASYN:\n",
      "   - Adaptive Synthetic Sampling\n",
      "   - Focuses on harder-to-learn minority samples\n",
      "   - Creates MORE synthetics near decision boundary (harder cases)\n",
      "   - Creates FEWER synthetics in easy regions\n",
      "   - Better for complex, varied fraud patterns\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Creating ADASYN Pipeline\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Building pipeline with 2 steps:\n",
      "   Step 1: ADASYN (adaptive balance training data)\n",
      "   Step 2: Logistic Regression (classification)\n",
      "\n",
      "+ ADASYN pipeline created successfully!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running 5-Fold Cross-Validation with ADASYN\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Running 5-fold cross-validation...\n",
      "   Dataset: X_train_scaled (226,980 samples, 378 frauds)\n",
      "   ADASYN will adaptively generate synthetics based on difficulty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "ADASYN Cross-Validation Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Cross-validation complete in 3.77 seconds\n",
      "\n",
      " Individual Fold Scores (ADASYN):\n",
      "   Fold 1: AUC = 0.9696\n",
      "   Fold 2: AUC = 0.9788\n",
      "   Fold 3: AUC = 0.9987\n",
      "   Fold 4: AUC = 0.9803\n",
      "   Fold 5: AUC = 0.9798\n",
      "\n",
      " ADASYN Summary Statistics:\n",
      "   Mean CV AUC:  0.9815\n",
      "   Std CV AUC:   0.0095\n",
      "   Min CV AUC:   0.9696\n",
      "   Max CV AUC:   0.9987\n",
      "\n",
      " 95% Confidence Interval:\n",
      "   0.9815 +/- 0.0186\n",
      "   Range: [0.9629, 1.0000]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Final ADASYN Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Training final model with ADASYN on all training data...\n",
      "+ ADASYN model training complete in 2.23 seconds\n",
      "\n",
      " ADASYN Model Summary:\n",
      "   Expected AUC: 0.9815 +/- 0.0095\n",
      "   Stability: Excellent\n",
      "\n",
      "======================================================================\n",
      "SMOTE vs ADASYN COMPARISON\n",
      "======================================================================\n",
      "\n",
      " Performance Comparison:\n",
      "   SMOTE  Mean AUC: 0.9824 (+/-0.0094)\n",
      "   ADASYN Mean AUC: 0.9815 (+/-0.0095)\n",
      "\n",
      "   Difference: 0.0009 (SMOTE better)\n",
      "\n",
      " Winner: TIE - Performance essentially identical\n",
      "\n",
      " Recommendation: Either technique is suitable - choose based on training time\n",
      "\n",
      "  Training Time Comparison:\n",
      "   SMOTE:  3.76 seconds\n",
      "   ADASYN: 3.77 seconds\n",
      "   Difference: 0.01 seconds (ADASYN slower)\n",
      "\n",
      " Stability Comparison:\n",
      "   SMOTE  Std: 0.0094 (More stable)\n",
      "   ADASYN Std: 0.0095 (Less stable)\n",
      "\n",
      " Statistical Significance:\n",
      "   Confidence intervals overlap\n",
      "   Difference may not be statistically significant\n",
      "   Both methods perform similarly on this dataset\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Visual Comparison\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Mean AUC Scores:\n",
      "   SMOTE:  █████████████████████████████████████████████████ 0.9824\n",
      "   ADASYN: █████████████████████████████████████████████████ 0.9815\n",
      "\n",
      " Consistency (Lower is Better):\n",
      "   SMOTE:  █████████ 0.0094\n",
      "   ADASYN: █████████ 0.0095\n",
      "\n",
      "======================================================================\n",
      "SELECTING BEST RESAMPLING TECHNIQUE\n",
      "======================================================================\n",
      "\n",
      " Selected Method: SMOTE\n",
      "   Reason: Similar performance, but SMOTE is more stable\n",
      "   Performance: 0.9824 +/- 0.0094\n",
      "\n",
      " Storing best model for later steps:\n",
      "   Model: Logistic Regression + SMOTE\n",
      "   Expected AUC: 0.9824\n",
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING - LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "\n",
      " WHAT WE'RE DOING:\n",
      "   - Fine-tuning the best model (SMOTE + Logistic Regression)\n",
      "   - Testing different hyperparameter combinations\n",
      "   - Using GridSearchCV with cross-validation\n",
      "   - Goal: Improve beyond baseline 0.9824 AUC\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Defining Hyperparameter Search Space\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Hyperparameters to tune:\n",
      "   1. C (Regularization strength)\n",
      "      - Controls model complexity\n",
      "      - Smaller C = stronger regularization (simpler model)\n",
      "      - Larger C = weaker regularization (more complex model)\n",
      "\n",
      "   2. penalty (Regularization type)\n",
      "      - 'l2': Ridge regularization (default)\n",
      "      - 'l1': Lasso regularization (feature selection)\n",
      "\n",
      "   3. solver (Optimization algorithm)\n",
      "      - Different algorithms for finding best coefficients\n",
      "      - Some solvers work better with certain penalties\n",
      "\n",
      " Parameter Grid:\n",
      "   C values: [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
      "   Penalties: ['l1', 'l2']\n",
      "   Solvers: ['liblinear', 'saga']\n",
      "\n",
      "   Total combinations: 24\n",
      "   With 5-fold CV: 120 model fits\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running Grid Search with Cross-Validation\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " This will take a few minutes...\n",
      "   Each combination is tested with 5-fold CV\n",
      "   Progress will be shown below\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Grid Search Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Grid search complete in 538.51 seconds (9.0 minutes)\n",
      "\n",
      " Best Parameters Found:\n",
      "   C: 0.001\n",
      "   penalty: l2\n",
      "   solver: liblinear\n",
      "\n",
      " Performance Comparison:\n",
      "   Baseline AUC:  0.9824 (+/-0.0094)\n",
      "   Tuned AUC:     0.9830 (+/-0.0086)\n",
      "\n",
      " Improvement:\n",
      "   +0.0006 AUC (+0.06%)\n",
      "   + Hyperparameter tuning improved performance!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Top 5 Hyperparameter Combinations\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Best performing combinations:\n",
      "\n",
      "   Rank 1:\n",
      "      C=0.001, penalty=l2, solver=liblinear\n",
      "      Mean AUC: 0.9830 (+/-0.0086)\n",
      "\n",
      "   Rank 2:\n",
      "      C=0.01, penalty=l2, solver=liblinear\n",
      "      Mean AUC: 0.9825 (+/-0.0092)\n",
      "\n",
      "   Rank 3:\n",
      "      C=0.1, penalty=l2, solver=liblinear\n",
      "      Mean AUC: 0.9824 (+/-0.0093)\n",
      "\n",
      "   Rank 4:\n",
      "      C=0.1, penalty=l2, solver=saga\n",
      "      Mean AUC: 0.9824 (+/-0.0094)\n",
      "\n",
      "   Rank 5:\n",
      "      C=1.0, penalty=l2, solver=liblinear\n",
      "      Mean AUC: 0.9824 (+/-0.0094)\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Storing Tuned Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Tuned model stored for final evaluation\n",
      "   Best hyperparameters saved\n",
      "   Expected AUC: 0.9830\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model Selection Decision\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Using TUNED model for final evaluation\n",
      "   Reason: Tuning improved performance by 0.0006\n",
      "\n",
      " Final Logistic Regression Model:\n",
      "   Version: Tuned\n",
      "   Resampling: SMOTE\n",
      "   Expected AUC: 0.9830 (+/-0.0086)\n",
      "\n",
      "======================================================================\n",
      "SAVING MODEL AND RESULTS\n",
      "======================================================================\n",
      "\n",
      " Saving to ../models/ directory...\n",
      "   + Model saved: ../models/logistic_regression_model.pkl\n",
      "   + Results saved: ../models/logistic_regression_results.pkl\n",
      "\n",
      "+ All files saved successfully!\n",
      "   Model can now be loaded in Phase 5 for comparison\n",
      "\n",
      "======================================================================\n",
      "STEP 17 COMPLETE - KEY ACCOMPLISHMENTS\n",
      "======================================================================\n",
      "\n",
      " What We Accomplished:\n",
      "   + Built baseline Logistic Regression with TWO resampling techniques\n",
      "   + Compared SMOTE vs ADASYN performance\n",
      "   + Used proper CV methodology (resampling inside folds)\n",
      "   + Validated on 100% real data (no synthetic contamination)\n",
      "   + Selected best resampling technique based on performance\n",
      "   + Performed hyperparameter tuning with GridSearchCV\n",
      "   + Trained final optimized model on all available training data\n",
      "   + SAVED model and results to disk\n",
      "\n",
      " Final Results:\n",
      "   Best Method: SMOTE\n",
      "   Model Version: Tuned\n",
      "   Cross-Validation AUC: 0.9830 (+/-0.0086)\n",
      "   Model Stability: Excellent\n",
      "\n",
      " Saved Files:\n",
      "   - ../models/logistic_regression_model.pkl\n",
      "   - ../models/logistic_regression_results.pkl\n",
      "\n",
      " Next Steps:\n",
      "   -> Step 18: Build Random Forest Model\n",
      "   -> Step 19: Build XGBoost Model\n",
      "   -> Step 20: Build Neural Network (Deep Learning)\n",
      "   -> Step 21: Compare all models and select winner\n",
      "\n",
      " Important Notes:\n",
      "   - Test set remains UNTOUCHED for final evaluation in Phase 5\n",
      "   - Model saved and can be loaded anytime (no need to retrain)\n",
      "   - Can restart PC without losing progress\n",
      "   - SMOTE + Tuned model ready for Phase 5\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 17: BUILD LOGISTIC REGRESSION MODEL (WITH SMOTE AND ADASYN)\n",
    "# ============================================================================\n",
    "\n",
    "# Import additional required libraries\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n WHAT WE'RE DOING:\")\n",
    "print(\"   - Building baseline Logistic Regression model\")\n",
    "print(\"   - Testing TWO resampling techniques: SMOTE vs ADASYN\")\n",
    "print(\"   - Using 5-fold cross-validation for performance estimation\")\n",
    "print(\"   - Applying resampling INSIDE each CV fold (proper methodology)\")\n",
    "print(\"   - Validating on REAL data only (no synthetic frauds in validation)\")\n",
    "print(\"   - SAVING model and results to disk for later use\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# PART A: LOGISTIC REGRESSION WITH SMOTE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART A: LOGISTIC REGRESSION + SMOTE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n About SMOTE:\")\n",
    "print(\"   - Synthetic Minority Over-sampling Technique\")\n",
    "print(\"   - Creates synthetic samples by interpolating between existing frauds\")\n",
    "print(\"   - Places new samples along line segments between neighbors\")\n",
    "print(\"   - Good for general class imbalance\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Create SMOTE Pipeline\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Creating SMOTE Pipeline\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n  Building pipeline with 2 steps:\")\n",
    "print(\"   Step 1: SMOTE (balance training data)\")\n",
    "print(\"   Step 2: Logistic Regression (classification)\")\n",
    "\n",
    "# Create SMOTE pipeline\n",
    "lr_smote_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"\\n+ SMOTE pipeline created successfully!\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Cross-Validation with SMOTE\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running 5-Fold Cross-Validation with SMOTE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Calculate dataset info dynamically (NO HARDCODING!)\n",
    "total_samples = len(X_train_scaled)\n",
    "total_frauds = int(y_train.sum())\n",
    "samples_per_fold = total_samples // 5\n",
    "frauds_per_fold = total_frauds // 5\n",
    "\n",
    "print(f\"\\n Running 5-fold cross-validation...\")\n",
    "print(f\"   Dataset: X_train_scaled ({total_samples:,} samples, {total_frauds} frauds)\")\n",
    "print(f\"   Each fold: ~{samples_per_fold:,} samples (~{frauds_per_fold} real frauds)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform 5-fold CV with SMOTE\n",
    "cv_scores_smote = cross_val_score(\n",
    "    lr_smote_pipeline,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cv_time_smote = time.time() - start_time\n",
    "\n",
    "# Display SMOTE results\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"SMOTE Cross-Validation Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Cross-validation complete in {cv_time_smote:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n Individual Fold Scores (SMOTE):\")\n",
    "for i, score in enumerate(cv_scores_smote, 1):\n",
    "    print(f\"   Fold {i}: AUC = {score:.4f}\")\n",
    "\n",
    "print(f\"\\n SMOTE Summary Statistics:\")\n",
    "print(f\"   Mean CV AUC:  {cv_scores_smote.mean():.4f}\")\n",
    "print(f\"   Std CV AUC:   {cv_scores_smote.std():.4f}\")\n",
    "print(f\"   Min CV AUC:   {cv_scores_smote.min():.4f}\")\n",
    "print(f\"   Max CV AUC:   {cv_scores_smote.max():.4f}\")\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval_smote = 1.96 * cv_scores_smote.std()\n",
    "print(f\"\\n 95% Confidence Interval:\")\n",
    "print(f\"   {cv_scores_smote.mean():.4f} +/- {confidence_interval_smote:.4f}\")\n",
    "print(f\"   Range: [{cv_scores_smote.mean() - confidence_interval_smote:.4f}, \"\n",
    "      f\"{cv_scores_smote.mean() + confidence_interval_smote:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train Final SMOTE Model\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Final SMOTE Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Training final model with SMOTE on all training data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_smote_pipeline.fit(X_train_scaled, y_train)\n",
    "train_time_smote = time.time() - start_time\n",
    "\n",
    "print(f\"+ SMOTE model training complete in {train_time_smote:.2f} seconds\")\n",
    "\n",
    "# Store SMOTE results\n",
    "lr_smote_cv_mean = cv_scores_smote.mean()\n",
    "lr_smote_cv_std = cv_scores_smote.std()\n",
    "\n",
    "print(f\"\\n SMOTE Model Summary:\")\n",
    "print(f\"   Expected AUC: {lr_smote_cv_mean:.4f} +/- {lr_smote_cv_std:.4f}\")\n",
    "print(f\"   Stability: {'Excellent' if lr_smote_cv_std < 0.01 else 'Good' if lr_smote_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART B: LOGISTIC REGRESSION WITH ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART B: LOGISTIC REGRESSION + ADASYN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n About ADASYN:\")\n",
    "print(\"   - Adaptive Synthetic Sampling\")\n",
    "print(\"   - Focuses on harder-to-learn minority samples\")\n",
    "print(\"   - Creates MORE synthetics near decision boundary (harder cases)\")\n",
    "print(\"   - Creates FEWER synthetics in easy regions\")\n",
    "print(\"   - Better for complex, varied fraud patterns\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Create ADASYN Pipeline\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Creating ADASYN Pipeline\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n  Building pipeline with 2 steps:\")\n",
    "print(\"   Step 1: ADASYN (adaptive balance training data)\")\n",
    "print(\"   Step 2: Logistic Regression (classification)\")\n",
    "\n",
    "# Create ADASYN pipeline\n",
    "lr_adasyn_pipeline = ImbPipeline([\n",
    "    ('adasyn', ADASYN(random_state=42)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"\\n+ ADASYN pipeline created successfully!\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Cross-Validation with ADASYN\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running 5-Fold Cross-Validation with ADASYN\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n Running 5-fold cross-validation...\")\n",
    "print(f\"   Dataset: X_train_scaled ({total_samples:,} samples, {total_frauds} frauds)\")\n",
    "print(f\"   ADASYN will adaptively generate synthetics based on difficulty\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform 5-fold CV with ADASYN\n",
    "cv_scores_adasyn = cross_val_score(\n",
    "    lr_adasyn_pipeline,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cv_time_adasyn = time.time() - start_time\n",
    "\n",
    "# Display ADASYN results\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ADASYN Cross-Validation Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Cross-validation complete in {cv_time_adasyn:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n Individual Fold Scores (ADASYN):\")\n",
    "for i, score in enumerate(cv_scores_adasyn, 1):\n",
    "    print(f\"   Fold {i}: AUC = {score:.4f}\")\n",
    "\n",
    "print(f\"\\n ADASYN Summary Statistics:\")\n",
    "print(f\"   Mean CV AUC:  {cv_scores_adasyn.mean():.4f}\")\n",
    "print(f\"   Std CV AUC:   {cv_scores_adasyn.std():.4f}\")\n",
    "print(f\"   Min CV AUC:   {cv_scores_adasyn.min():.4f}\")\n",
    "print(f\"   Max CV AUC:   {cv_scores_adasyn.max():.4f}\")\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval_adasyn = 1.96 * cv_scores_adasyn.std()\n",
    "print(f\"\\n 95% Confidence Interval:\")\n",
    "print(f\"   {cv_scores_adasyn.mean():.4f} +/- {confidence_interval_adasyn:.4f}\")\n",
    "print(f\"   Range: [{cv_scores_adasyn.mean() - confidence_interval_adasyn:.4f}, \"\n",
    "      f\"{cv_scores_adasyn.mean() + confidence_interval_adasyn:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train Final ADASYN Model\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Final ADASYN Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Training final model with ADASYN on all training data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "lr_adasyn_pipeline.fit(X_train_scaled, y_train)\n",
    "train_time_adasyn = time.time() - start_time\n",
    "\n",
    "print(f\"+ ADASYN model training complete in {train_time_adasyn:.2f} seconds\")\n",
    "\n",
    "# Store ADASYN results\n",
    "lr_adasyn_cv_mean = cv_scores_adasyn.mean()\n",
    "lr_adasyn_cv_std = cv_scores_adasyn.std()\n",
    "\n",
    "print(f\"\\n ADASYN Model Summary:\")\n",
    "print(f\"   Expected AUC: {lr_adasyn_cv_mean:.4f} +/- {lr_adasyn_cv_std:.4f}\")\n",
    "print(f\"   Stability: {'Excellent' if lr_adasyn_cv_std < 0.01 else 'Good' if lr_adasyn_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON: SMOTE vs ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SMOTE vs ADASYN COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "print(f\"   SMOTE  Mean AUC: {lr_smote_cv_mean:.4f} (+/-{lr_smote_cv_std:.4f})\")\n",
    "print(f\"   ADASYN Mean AUC: {lr_adasyn_cv_mean:.4f} (+/-{lr_adasyn_cv_std:.4f})\")\n",
    "\n",
    "# Calculate difference\n",
    "auc_difference = lr_adasyn_cv_mean - lr_smote_cv_mean\n",
    "print(f\"\\n   Difference: {abs(auc_difference):.4f} ({'ADASYN better' if auc_difference > 0 else 'SMOTE better'})\")\n",
    "\n",
    "# Determine winner\n",
    "if abs(auc_difference) < 0.001:\n",
    "    winner = \"TIE - Performance essentially identical\"\n",
    "    recommendation = \"Either technique is suitable - choose based on training time\"\n",
    "elif auc_difference > 0:\n",
    "    winner = \"ADASYN WINS\"\n",
    "    recommendation = \"ADASYN provides better fraud detection for this dataset\"\n",
    "else:\n",
    "    winner = \"SMOTE WINS\"\n",
    "    recommendation = \"SMOTE provides better fraud detection for this dataset\"\n",
    "\n",
    "print(f\"\\n Winner: {winner}\")\n",
    "print(f\"\\n Recommendation: {recommendation}\")\n",
    "\n",
    "print(f\"\\n  Training Time Comparison:\")\n",
    "print(f\"   SMOTE:  {cv_time_smote:.2f} seconds\")\n",
    "print(f\"   ADASYN: {cv_time_adasyn:.2f} seconds\")\n",
    "print(f\"   Difference: {abs(cv_time_smote - cv_time_adasyn):.2f} seconds ({'ADASYN slower' if cv_time_adasyn > cv_time_smote else 'SMOTE slower'})\")\n",
    "\n",
    "print(f\"\\n Stability Comparison:\")\n",
    "print(f\"   SMOTE  Std: {lr_smote_cv_std:.4f} ({'More stable' if lr_smote_cv_std < lr_adasyn_cv_std else 'Less stable'})\")\n",
    "print(f\"   ADASYN Std: {lr_adasyn_cv_std:.4f} ({'More stable' if lr_adasyn_cv_std < lr_smote_cv_std else 'Less stable'})\")\n",
    "\n",
    "# Statistical significance test (simple check)\n",
    "print(f\"\\n Statistical Significance:\")\n",
    "smote_lower = lr_smote_cv_mean - confidence_interval_smote\n",
    "smote_upper = lr_smote_cv_mean + confidence_interval_smote\n",
    "adasyn_lower = lr_adasyn_cv_mean - confidence_interval_adasyn\n",
    "adasyn_upper = lr_adasyn_cv_mean + confidence_interval_adasyn\n",
    "\n",
    "if (smote_lower <= adasyn_upper) and (adasyn_lower <= smote_upper):\n",
    "    print(f\"   Confidence intervals overlap\")\n",
    "    print(f\"   Difference may not be statistically significant\")\n",
    "    print(f\"   Both methods perform similarly on this dataset\")\n",
    "else:\n",
    "    print(f\"   + Confidence intervals don't overlap\")\n",
    "    print(f\"   Difference is likely statistically significant\")\n",
    "    print(f\"   {winner} is meaningfully better\")\n",
    "\n",
    "# Visualization of comparison\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Visual Comparison\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Mean AUC Scores:\")\n",
    "print(f\"   SMOTE:  {'█' * int(lr_smote_cv_mean * 50)} {lr_smote_cv_mean:.4f}\")\n",
    "print(f\"   ADASYN: {'█' * int(lr_adasyn_cv_mean * 50)} {lr_adasyn_cv_mean:.4f}\")\n",
    "\n",
    "print(f\"\\n Consistency (Lower is Better):\")\n",
    "print(f\"   SMOTE:  {'█' * int(lr_smote_cv_std * 1000)} {lr_smote_cv_std:.4f}\")\n",
    "print(f\"   ADASYN: {'█' * int(lr_adasyn_cv_std * 1000)} {lr_adasyn_cv_std:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT BEST MODEL FOR FINAL USE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SELECTING BEST RESAMPLING TECHNIQUE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Choose best based on mean AUC (with tie-breaker on stability)\n",
    "if abs(auc_difference) < 0.001:\n",
    "    # Performance is essentially the same, choose more stable\n",
    "    if lr_smote_cv_std < lr_adasyn_cv_std:\n",
    "        best_pipeline = lr_smote_pipeline\n",
    "        best_method = \"SMOTE\"\n",
    "        best_cv_mean = lr_smote_cv_mean\n",
    "        best_cv_std = lr_smote_cv_std\n",
    "        best_cv_scores = cv_scores_smote\n",
    "        reason = \"Similar performance, but SMOTE is more stable\"\n",
    "    else:\n",
    "        best_pipeline = lr_adasyn_pipeline\n",
    "        best_method = \"ADASYN\"\n",
    "        best_cv_mean = lr_adasyn_cv_mean\n",
    "        best_cv_std = lr_adasyn_cv_std\n",
    "        best_cv_scores = cv_scores_adasyn\n",
    "        reason = \"Similar performance, but ADASYN is more stable\"\n",
    "else:\n",
    "    # Clear winner based on AUC\n",
    "    if lr_smote_cv_mean > lr_adasyn_cv_mean:\n",
    "        best_pipeline = lr_smote_pipeline\n",
    "        best_method = \"SMOTE\"\n",
    "        best_cv_mean = lr_smote_cv_mean\n",
    "        best_cv_std = lr_smote_cv_std\n",
    "        best_cv_scores = cv_scores_smote\n",
    "        reason = f\"Higher mean AUC ({lr_smote_cv_mean:.4f} vs {lr_adasyn_cv_mean:.4f})\"\n",
    "    else:\n",
    "        best_pipeline = lr_adasyn_pipeline\n",
    "        best_method = \"ADASYN\"\n",
    "        best_cv_mean = lr_adasyn_cv_mean\n",
    "        best_cv_std = lr_adasyn_cv_std\n",
    "        best_cv_scores = cv_scores_adasyn\n",
    "        reason = f\"Higher mean AUC ({lr_adasyn_cv_mean:.4f} vs {lr_smote_cv_mean:.4f})\"\n",
    "\n",
    "print(f\"\\n Selected Method: {best_method}\")\n",
    "print(f\"   Reason: {reason}\")\n",
    "print(f\"   Performance: {best_cv_mean:.4f} +/- {best_cv_std:.4f}\")\n",
    "\n",
    "print(f\"\\n Storing best model for later steps:\")\n",
    "print(f\"   Model: Logistic Regression + {best_method}\")\n",
    "print(f\"   Expected AUC: {best_cv_mean:.4f}\")\n",
    "\n",
    "# Store the best results for comparison in Step 21\n",
    "lr_final_pipeline = best_pipeline\n",
    "lr_cv_mean = best_cv_mean\n",
    "lr_cv_std = best_cv_std\n",
    "lr_cv_scores = best_cv_scores\n",
    "lr_best_method = best_method\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETER TUNING FOR BEST MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n WHAT WE'RE DOING:\")\n",
    "print(f\"   - Fine-tuning the best model ({best_method} + Logistic Regression)\")\n",
    "print(f\"   - Testing different hyperparameter combinations\")\n",
    "print(f\"   - Using GridSearchCV with cross-validation\")\n",
    "print(f\"   - Goal: Improve beyond baseline {best_cv_mean:.4f} AUC\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Define Hyperparameter Grid\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Defining Hyperparameter Search Space\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Hyperparameters to tune:\")\n",
    "print(\"   1. C (Regularization strength)\")\n",
    "print(\"      - Controls model complexity\")\n",
    "print(\"      - Smaller C = stronger regularization (simpler model)\")\n",
    "print(\"      - Larger C = weaker regularization (more complex model)\")\n",
    "print(\"\\n   2. penalty (Regularization type)\")\n",
    "print(\"      - 'l2': Ridge regularization (default)\")\n",
    "print(\"      - 'l1': Lasso regularization (feature selection)\")\n",
    "print(\"\\n   3. solver (Optimization algorithm)\")\n",
    "print(\"      - Different algorithms for finding best coefficients\")\n",
    "print(\"      - Some solvers work better with certain penalties\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "print(\"\\n Parameter Grid:\")\n",
    "print(f\"   C values: {param_grid['classifier__C']}\")\n",
    "print(f\"   Penalties: {param_grid['classifier__penalty']}\")\n",
    "print(f\"   Solvers: {param_grid['classifier__solver']}\")\n",
    "print(f\"\\n   Total combinations: {len(param_grid['classifier__C']) * len(param_grid['classifier__penalty']) * len(param_grid['classifier__solver'])}\")\n",
    "print(f\"   With 5-fold CV: {len(param_grid['classifier__C']) * len(param_grid['classifier__penalty']) * len(param_grid['classifier__solver']) * 5} model fits\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Run Grid Search\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running Grid Search with Cross-Validation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n This will take a few minutes...\")\n",
    "print(\"   Each combination is tested with 5-fold CV\")\n",
    "print(\"   Progress will be shown below\")\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lr_final_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Run grid search\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Display Results\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Grid Search Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Grid search complete in {grid_time:.2f} seconds ({grid_time/60:.1f} minutes)\")\n",
    "\n",
    "print(f\"\\n Best Parameters Found:\")\n",
    "for param_name, param_value in grid_search.best_params_.items():\n",
    "    # Remove 'classifier__' prefix for cleaner display\n",
    "    clean_name = param_name.replace('classifier__', '')\n",
    "    print(f\"   {clean_name}: {param_value}\")\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "print(f\"   Baseline AUC:  {best_cv_mean:.4f} (+/-{best_cv_std:.4f})\")\n",
    "print(f\"   Tuned AUC:     {grid_search.best_score_:.4f} (+/-{grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.4f})\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = grid_search.best_score_ - best_cv_mean\n",
    "improvement_pct = (improvement / best_cv_mean) * 100\n",
    "\n",
    "print(f\"\\n Improvement:\")\n",
    "if improvement > 0:\n",
    "    print(f\"   +{improvement:.4f} AUC ({improvement_pct:+.2f}%)\")\n",
    "    print(f\"   + Hyperparameter tuning improved performance!\")\n",
    "elif improvement < -0.001:\n",
    "    print(f\"   {improvement:.4f} AUC ({improvement_pct:.2f}%)\")\n",
    "    print(f\"   Tuned model performed slightly worse\")\n",
    "    print(f\"   -> Baseline model was already well-optimized\")\n",
    "else:\n",
    "    print(f\"   ~{improvement:.4f} AUC (essentially no change)\")\n",
    "    print(f\"   -> Baseline hyperparameters were already optimal\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Top 5 Parameter Combinations\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Top 5 Hyperparameter Combinations\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Get results sorted by score\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df = results_df.sort_values('rank_test_score')\n",
    "\n",
    "print(\"\\n Best performing combinations:\\n\")\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"   Rank {i+1}:\")\n",
    "    print(f\"      C={row['param_classifier__C']}, \"\n",
    "          f\"penalty={row['param_classifier__penalty']}, \"\n",
    "          f\"solver={row['param_classifier__solver']}\")\n",
    "    print(f\"      Mean AUC: {row['mean_test_score']:.4f} (+/-{row['std_test_score']:.4f})\")\n",
    "    print()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Store Tuned Model\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Storing Tuned Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Store the tuned model\n",
    "lr_tuned_pipeline = grid_search.best_estimator_\n",
    "lr_tuned_cv_mean = grid_search.best_score_\n",
    "lr_tuned_cv_std = grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "lr_best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"\\n+ Tuned model stored for final evaluation\")\n",
    "print(f\"   Best hyperparameters saved\")\n",
    "print(f\"   Expected AUC: {lr_tuned_cv_mean:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Decision: Use Baseline or Tuned?\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Model Selection Decision\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Decide whether to use tuned or baseline model\n",
    "if improvement > 0.0001:  # Meaningful improvement\n",
    "    print(f\"\\n+ Using TUNED model for final evaluation\")\n",
    "    print(f\"   Reason: Tuning improved performance by {improvement:.4f}\")\n",
    "    lr_final_model = lr_tuned_pipeline\n",
    "    lr_final_cv_mean = lr_tuned_cv_mean\n",
    "    lr_final_cv_std = lr_tuned_cv_std\n",
    "    model_version = \"Tuned\"\n",
    "else:\n",
    "    print(f\"\\n+ Using BASELINE model for final evaluation\")\n",
    "    print(f\"   Reason: Tuning did not provide meaningful improvement\")\n",
    "    print(f\"   Baseline model was already well-optimized\")\n",
    "    lr_final_model = lr_final_pipeline\n",
    "    lr_final_cv_mean = best_cv_mean\n",
    "    lr_final_cv_std = best_cv_std\n",
    "    model_version = \"Baseline\"\n",
    "\n",
    "print(f\"\\n Final Logistic Regression Model:\")\n",
    "print(f\"   Version: {model_version}\")\n",
    "print(f\"   Resampling: {best_method}\")\n",
    "print(f\"   Expected AUC: {lr_final_cv_mean:.4f} (+/-{lr_final_cv_std:.4f})\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE MODEL AND RESULTS TO DISK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL AND RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Saving to ../models/ directory...\")\n",
    "\n",
    "# Save the final model\n",
    "model_path = '../models/logistic_regression_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(lr_final_model, f)\n",
    "print(f\"   + Model saved: {model_path}\")\n",
    "\n",
    "# Save all results and metadata\n",
    "results_data = {\n",
    "    'model_name': 'Logistic Regression',\n",
    "    'resampling_method': best_method,\n",
    "    'model_version': model_version,\n",
    "    'cv_mean': lr_final_cv_mean,\n",
    "    'cv_std': lr_final_cv_std,\n",
    "    'cv_scores': lr_cv_scores,\n",
    "    'best_params': lr_best_params if model_version == \"Tuned\" else None,\n",
    "    'training_samples': total_samples,\n",
    "    'training_frauds': total_frauds,\n",
    "    'timestamp': pd.Timestamp.now()\n",
    "}\n",
    "\n",
    "results_path = '../models/logistic_regression_results.pkl'\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(results_data, f)\n",
    "print(f\"   + Results saved: {results_path}\")\n",
    "\n",
    "print(f\"\\n+ All files saved successfully!\")\n",
    "print(f\"   Model can now be loaded in Phase 5 for comparison\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 17 COMPLETE - KEY ACCOMPLISHMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n What We Accomplished:\")\n",
    "print(\"   + Built baseline Logistic Regression with TWO resampling techniques\")\n",
    "print(\"   + Compared SMOTE vs ADASYN performance\")\n",
    "print(\"   + Used proper CV methodology (resampling inside folds)\")\n",
    "print(\"   + Validated on 100% real data (no synthetic contamination)\")\n",
    "print(\"   + Selected best resampling technique based on performance\")\n",
    "print(\"   + Performed hyperparameter tuning with GridSearchCV\")\n",
    "print(\"   + Trained final optimized model on all available training data\")\n",
    "print(\"   + SAVED model and results to disk\")\n",
    "\n",
    "print(f\"\\n Final Results:\")\n",
    "print(f\"   Best Method: {best_method}\")\n",
    "print(f\"   Model Version: {model_version}\")\n",
    "print(f\"   Cross-Validation AUC: {lr_final_cv_mean:.4f} (+/-{lr_final_cv_std:.4f})\")\n",
    "print(f\"   Model Stability: {'Excellent' if lr_final_cv_std < 0.01 else 'Good' if lr_final_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "print(\"\\n Saved Files:\")\n",
    "print(f\"   - {model_path}\")\n",
    "print(f\"   - {results_path}\")\n",
    "\n",
    "print(\"\\n Next Steps:\")\n",
    "print(\"   -> Step 18: Build Random Forest Model\")\n",
    "print(\"   -> Step 19: Build XGBoost Model\")\n",
    "print(\"   -> Step 20: Build Neural Network (Deep Learning)\")\n",
    "print(\"   -> Step 21: Compare all models and select winner\")\n",
    "\n",
    "print(\"\\n Important Notes:\")\n",
    "print(\"   - Test set remains UNTOUCHED for final evaluation in Phase 5\")\n",
    "print(\"   - Model saved and can be loaded anytime (no need to retrain)\")\n",
    "print(\"   - Can restart PC without losing progress\")\n",
    "print(f\"   - {best_method} + {model_version} model ready for Phase 5\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c96d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL 2: RANDOM FOREST\n",
      "======================================================================\n",
      "\n",
      " WHAT WE'RE DOING:\n",
      "   - Building Random Forest classifier (ensemble of 100 decision trees)\n",
      "   - Testing TWO resampling techniques: SMOTE vs ADASYN\n",
      "   - Using 5-fold cross-validation for performance estimation\n",
      "   - Applying resampling INSIDE each CV fold (proper methodology)\n",
      "   - Validating on REAL data only (no synthetic frauds in validation)\n",
      "   - SAVING model and results to disk for later use\n",
      "\n",
      "======================================================================\n",
      "PART A: RANDOM FOREST + SMOTE\n",
      "======================================================================\n",
      "\n",
      " About Random Forest:\n",
      "   - Ensemble of 100 decision trees voting together\n",
      "   - Each tree trained on random subset of data\n",
      "   - Each tree focuses on different feature combinations\n",
      "   - Final prediction = majority vote from all trees\n",
      "   - More powerful than Logistic Regression for complex patterns\n",
      "\n",
      " About SMOTE:\n",
      "   - Synthetic Minority Over-sampling Technique\n",
      "   - Creates synthetic samples by interpolating between existing frauds\n",
      "   - Places new samples along line segments between neighbors\n",
      "   - Good for general class imbalance\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Creating SMOTE Pipeline\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Building pipeline with 2 steps:\n",
      "   Step 1: SMOTE (balance training data)\n",
      "   Step 2: Random Forest (classification with 100 trees)\n",
      "\n",
      "+ SMOTE pipeline created successfully!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running 5-Fold Cross-Validation with SMOTE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Running 5-fold cross-validation...\n",
      "   Dataset: X_train_scaled (226,980 samples, 378 frauds)\n",
      "   Each fold: ~45,396 samples (~75 real frauds)\n",
      "   (This will take 3-5 minutes due to Random Forest complexity)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "SMOTE Cross-Validation Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Cross-validation complete in 93.79 seconds (1.6 minutes)\n",
      "\n",
      " Individual Fold Scores (SMOTE):\n",
      "   Fold 1: AUC = 0.9448\n",
      "   Fold 2: AUC = 0.9655\n",
      "   Fold 3: AUC = 0.9888\n",
      "   Fold 4: AUC = 0.9675\n",
      "   Fold 5: AUC = 0.9823\n",
      "\n",
      " SMOTE Summary Statistics:\n",
      "   Mean CV AUC:  0.9698\n",
      "   Std CV AUC:   0.0153\n",
      "   Min CV AUC:   0.9448\n",
      "   Max CV AUC:   0.9888\n",
      "\n",
      " 95% Confidence Interval:\n",
      "   0.9698 +/- 0.0299\n",
      "   Range: [0.9399, 0.9997]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Final SMOTE Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Training final model with SMOTE on all training data...\n",
      "+ SMOTE model training complete in 26.67 seconds\n",
      "\n",
      " SMOTE Model Summary:\n",
      "   Expected AUC: 0.9698 +/- 0.0153\n",
      "   Stability: Good\n",
      "\n",
      "======================================================================\n",
      "PART B: RANDOM FOREST + ADASYN\n",
      "======================================================================\n",
      "\n",
      " About ADASYN:\n",
      "   - Adaptive Synthetic Sampling\n",
      "   - Focuses on harder-to-learn minority samples\n",
      "   - Creates MORE synthetics near decision boundary (harder cases)\n",
      "   - Creates FEWER synthetics in easy regions\n",
      "   - Better for complex, varied fraud patterns\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Creating ADASYN Pipeline\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Building pipeline with 2 steps:\n",
      "   Step 1: ADASYN (adaptive balance training data)\n",
      "   Step 2: Random Forest (classification with 100 trees)\n",
      "\n",
      "+ ADASYN pipeline created successfully!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running 5-Fold Cross-Validation with ADASYN\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Running 5-fold cross-validation...\n",
      "   Dataset: X_train_scaled (226,980 samples, 378 frauds)\n",
      "   ADASYN will adaptively generate synthetics based on difficulty\n",
      "   (This will take 3-5 minutes due to Random Forest complexity)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "ADASYN Cross-Validation Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Cross-validation complete in 100.59 seconds (1.7 minutes)\n",
      "\n",
      " Individual Fold Scores (ADASYN):\n",
      "   Fold 1: AUC = 0.9330\n",
      "   Fold 2: AUC = 0.9471\n",
      "   Fold 3: AUC = 0.9890\n",
      "   Fold 4: AUC = 0.9670\n",
      "   Fold 5: AUC = 0.9770\n",
      "\n",
      " ADASYN Summary Statistics:\n",
      "   Mean CV AUC:  0.9626\n",
      "   Std CV AUC:   0.0202\n",
      "   Min CV AUC:   0.9330\n",
      "   Max CV AUC:   0.9890\n",
      "\n",
      " 95% Confidence Interval:\n",
      "   0.9626 +/- 0.0396\n",
      "   Range: [0.9230, 1.0022]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Final ADASYN Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Training final model with ADASYN on all training data...\n",
      "+ ADASYN model training complete in 28.30 seconds\n",
      "\n",
      " ADASYN Model Summary:\n",
      "   Expected AUC: 0.9626 +/- 0.0202\n",
      "   Stability: Moderate\n",
      "\n",
      "======================================================================\n",
      "SMOTE vs ADASYN COMPARISON\n",
      "======================================================================\n",
      "\n",
      " Performance Comparison:\n",
      "   SMOTE  Mean AUC: 0.9698 (+/-0.0153)\n",
      "   ADASYN Mean AUC: 0.9626 (+/-0.0202)\n",
      "\n",
      "   Difference: 0.0072 (SMOTE better)\n",
      "\n",
      " Winner: SMOTE WINS\n",
      "\n",
      " Recommendation: SMOTE provides better fraud detection for this dataset\n",
      "\n",
      " Training Time Comparison:\n",
      "   SMOTE:  93.79 seconds (1.6 minutes)\n",
      "   ADASYN: 100.59 seconds (1.7 minutes)\n",
      "   Difference: 6.80 seconds (ADASYN slower)\n",
      "\n",
      " Stability Comparison:\n",
      "   SMOTE  Std: 0.0153 (More stable)\n",
      "   ADASYN Std: 0.0202 (Less stable)\n",
      "\n",
      " Statistical Significance:\n",
      "   Confidence intervals overlap\n",
      "   Difference may not be statistically significant\n",
      "   Both methods perform similarly on this dataset\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Visual Comparison\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Mean AUC Scores:\n",
      "   SMOTE:  ████████████████████████████████████████████████ 0.9698\n",
      "   ADASYN: ████████████████████████████████████████████████ 0.9626\n",
      "\n",
      " Consistency (Lower is Better):\n",
      "   SMOTE:  ███████████████ 0.0153\n",
      "   ADASYN: ████████████████████ 0.0202\n",
      "\n",
      "======================================================================\n",
      "SELECTING BEST RESAMPLING TECHNIQUE\n",
      "======================================================================\n",
      "\n",
      " Selected Method: SMOTE\n",
      "   Reason: Higher mean AUC (0.9698 vs 0.9626)\n",
      "   Performance: 0.9698 +/- 0.0153\n",
      "\n",
      " Storing best model for later steps:\n",
      "   Model: Random Forest + SMOTE\n",
      "   Expected AUC: 0.9698\n",
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING - RANDOM FOREST\n",
      "======================================================================\n",
      "\n",
      " WHAT WE'RE DOING:\n",
      "   - Fine-tuning the best model (SMOTE + Random Forest)\n",
      "   - Testing different hyperparameter combinations\n",
      "   - Using GridSearchCV with cross-validation\n",
      "   - Goal: Improve beyond baseline 0.9698 AUC\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Defining Hyperparameter Search Space\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Hyperparameters to tune:\n",
      "   1. n_estimators (Number of trees)\n",
      "      - More trees = more voting experts\n",
      "      - But diminishing returns after certain point\n",
      "      - Trade-off: accuracy vs training time\n",
      "\n",
      "   2. max_depth (Maximum tree depth)\n",
      "      - Controls how deep each tree can grow\n",
      "      - Deeper trees = more complex patterns\n",
      "      - Trade-off: complexity vs overfitting\n",
      "\n",
      "   3. min_samples_split (Minimum samples to split node)\n",
      "      - How many samples needed before splitting\n",
      "      - Higher values = more conservative splitting\n",
      "      - Trade-off: generalization vs pattern detection\n",
      "\n",
      "   4. min_samples_leaf (Minimum samples per leaf)\n",
      "      - How many samples must be in final decision\n",
      "      - Higher values = more evidence per decision\n",
      "      - Trade-off: generalization vs specificity\n",
      "\n",
      " Parameter Grid:\n",
      "   n_estimators: [100, 200]\n",
      "   max_depth: [20, 30, None]\n",
      "   min_samples_split: [2, 5]\n",
      "   min_samples_leaf: [1, 2]\n",
      "\n",
      "   Total combinations: 24\n",
      "   With 5-fold CV: 120 model fits\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running Grid Search with Cross-Validation\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " This will take 15-25 minutes...\n",
      "   Each combination is tested with 5-fold CV\n",
      "   Random Forest is computationally intensive\n",
      "   Progress will be shown below\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Grid Search Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Grid search complete in 3283.93 seconds (54.7 minutes)\n",
      "\n",
      " Best Parameters Found:\n",
      "   max_depth: None\n",
      "   min_samples_leaf: 2\n",
      "   min_samples_split: 5\n",
      "   n_estimators: 200\n",
      "\n",
      " Performance Comparison:\n",
      "   Baseline AUC:  0.9698 (+/-0.0153)\n",
      "   Tuned AUC:     0.9793 (+/-0.0150)\n",
      "\n",
      " Improvement:\n",
      "   +0.0095 AUC (+0.98%)\n",
      "   + Hyperparameter tuning improved performance!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Top 5 Hyperparameter Combinations\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Best performing combinations:\n",
      "\n",
      "   Rank 1:\n",
      "      n_estimators=200, max_depth=None, min_samples_split=5, min_samples_leaf=2\n",
      "      Mean AUC: 0.9793 (+/-0.0150)\n",
      "\n",
      "   Rank 2:\n",
      "      n_estimators=200, max_depth=None, min_samples_split=5, min_samples_leaf=1\n",
      "      Mean AUC: 0.9792 (+/-0.0124)\n",
      "\n",
      "   Rank 3:\n",
      "      n_estimators=100, max_depth=20, min_samples_split=2, min_samples_leaf=2\n",
      "      Mean AUC: 0.9789 (+/-0.0136)\n",
      "\n",
      "   Rank 4:\n",
      "      n_estimators=200, max_depth=20, min_samples_split=5, min_samples_leaf=1\n",
      "      Mean AUC: 0.9777 (+/-0.0161)\n",
      "\n",
      "   Rank 5:\n",
      "      n_estimators=100, max_depth=None, min_samples_split=5, min_samples_leaf=2\n",
      "      Mean AUC: 0.9770 (+/-0.0121)\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Storing Tuned Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Tuned model stored for final evaluation\n",
      "   Best hyperparameters saved\n",
      "   Expected AUC: 0.9793\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model Selection Decision\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Using TUNED model for final evaluation\n",
      "   Reason: Tuning improved performance by 0.0095\n",
      "\n",
      " Final Random Forest Model:\n",
      "   Version: Tuned\n",
      "   Resampling: SMOTE\n",
      "   Expected AUC: 0.9793 (+/-0.0150)\n",
      "\n",
      "======================================================================\n",
      "SAVING MODEL AND RESULTS\n",
      "======================================================================\n",
      "\n",
      " Saving to ../models/ directory...\n",
      "   + Model saved: ../models/random_forest_model.pkl\n",
      "   + Results saved: ../models/random_forest_results.pkl\n",
      "\n",
      "+ All files saved successfully!\n",
      "   Model can now be loaded in Phase 5 for comparison\n",
      "\n",
      "======================================================================\n",
      "STEP 18 COMPLETE - KEY ACCOMPLISHMENTS\n",
      "======================================================================\n",
      "\n",
      " What We Accomplished:\n",
      "   + Built Random Forest classifier (ensemble of 100 trees)\n",
      "   + Compared SMOTE vs ADASYN performance\n",
      "   + Used proper CV methodology (resampling inside folds)\n",
      "   + Validated on 100% real data (no synthetic contamination)\n",
      "   + Selected best resampling technique based on performance\n",
      "   + Performed hyperparameter tuning with GridSearchCV\n",
      "   + Trained final optimized model on all available training data\n",
      "   + SAVED model and results to disk\n",
      "\n",
      " Final Results:\n",
      "   Best Method: SMOTE\n",
      "   Model Version: Tuned\n",
      "   Cross-Validation AUC: 0.9793 (+/-0.0150)\n",
      "   Model Stability: Good\n",
      "\n",
      " Saved Files:\n",
      "   - ../models/random_forest_model.pkl\n",
      "   - ../models/random_forest_results.pkl\n",
      "\n",
      " Next Steps:\n",
      "   -> Step 19: Build XGBoost Model\n",
      "   -> Step 20: Build Neural Network (Deep Learning)\n",
      "   -> Step 21: Compare all models and select winner\n",
      "\n",
      " Important Notes:\n",
      "   - Test set remains UNTOUCHED for final evaluation in Phase 5\n",
      "   - Model saved and can be loaded anytime (no need to retrain)\n",
      "   - Can restart PC without losing progress\n",
      "   - SMOTE + Tuned model ready for Phase 5\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 18: BUILD RANDOM FOREST MODEL (WITH SMOTE AND ADASYN)\n",
    "# ============================================================================\n",
    "\n",
    "# Import additional required libraries\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 2: RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n WHAT WE'RE DOING:\")\n",
    "print(\"   - Building Random Forest classifier (ensemble of 100 decision trees)\")\n",
    "print(\"   - Testing TWO resampling techniques: SMOTE vs ADASYN\")\n",
    "print(\"   - Using 5-fold cross-validation for performance estimation\")\n",
    "print(\"   - Applying resampling INSIDE each CV fold (proper methodology)\")\n",
    "print(\"   - Validating on REAL data only (no synthetic frauds in validation)\")\n",
    "print(\"   - SAVING model and results to disk for later use\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# PART A: RANDOM FOREST WITH SMOTE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART A: RANDOM FOREST + SMOTE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n About Random Forest:\")\n",
    "print(\"   - Ensemble of 100 decision trees voting together\")\n",
    "print(\"   - Each tree trained on random subset of data\")\n",
    "print(\"   - Each tree focuses on different feature combinations\")\n",
    "print(\"   - Final prediction = majority vote from all trees\")\n",
    "print(\"   - More powerful than Logistic Regression for complex patterns\")\n",
    "\n",
    "print(\"\\n About SMOTE:\")\n",
    "print(\"   - Synthetic Minority Over-sampling Technique\")\n",
    "print(\"   - Creates synthetic samples by interpolating between existing frauds\")\n",
    "print(\"   - Places new samples along line segments between neighbors\")\n",
    "print(\"   - Good for general class imbalance\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Create SMOTE Pipeline\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Creating SMOTE Pipeline\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Building pipeline with 2 steps:\")\n",
    "print(\"   Step 1: SMOTE (balance training data)\")\n",
    "print(\"   Step 2: Random Forest (classification with 100 trees)\")\n",
    "\n",
    "# Create SMOTE pipeline\n",
    "rf_smote_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\n+ SMOTE pipeline created successfully!\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Cross-Validation with SMOTE\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running 5-Fold Cross-Validation with SMOTE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Calculate dataset info dynamically\n",
    "total_samples = len(X_train_scaled)\n",
    "total_frauds = int(y_train.sum())\n",
    "samples_per_fold = total_samples // 5\n",
    "frauds_per_fold = total_frauds // 5\n",
    "\n",
    "print(f\"\\n Running 5-fold cross-validation...\")\n",
    "print(f\"   Dataset: X_train_scaled ({total_samples:,} samples, {total_frauds} frauds)\")\n",
    "print(f\"   Each fold: ~{samples_per_fold:,} samples (~{frauds_per_fold} real frauds)\")\n",
    "print(f\"   (This will take 3-5 minutes due to Random Forest complexity)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform 5-fold CV with SMOTE\n",
    "cv_scores_smote = cross_val_score(\n",
    "    rf_smote_pipeline,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cv_time_smote = time.time() - start_time\n",
    "\n",
    "# Display SMOTE results\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"SMOTE Cross-Validation Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Cross-validation complete in {cv_time_smote:.2f} seconds ({cv_time_smote/60:.1f} minutes)\")\n",
    "\n",
    "print(f\"\\n Individual Fold Scores (SMOTE):\")\n",
    "for i, score in enumerate(cv_scores_smote, 1):\n",
    "    print(f\"   Fold {i}: AUC = {score:.4f}\")\n",
    "\n",
    "print(f\"\\n SMOTE Summary Statistics:\")\n",
    "print(f\"   Mean CV AUC:  {cv_scores_smote.mean():.4f}\")\n",
    "print(f\"   Std CV AUC:   {cv_scores_smote.std():.4f}\")\n",
    "print(f\"   Min CV AUC:   {cv_scores_smote.min():.4f}\")\n",
    "print(f\"   Max CV AUC:   {cv_scores_smote.max():.4f}\")\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval_smote = 1.96 * cv_scores_smote.std()\n",
    "print(f\"\\n 95% Confidence Interval:\")\n",
    "print(f\"   {cv_scores_smote.mean():.4f} +/- {confidence_interval_smote:.4f}\")\n",
    "print(f\"   Range: [{cv_scores_smote.mean() - confidence_interval_smote:.4f}, \"\n",
    "      f\"{cv_scores_smote.mean() + confidence_interval_smote:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train Final SMOTE Model\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Final SMOTE Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Training final model with SMOTE on all training data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_smote_pipeline.fit(X_train_scaled, y_train)\n",
    "train_time_smote = time.time() - start_time\n",
    "\n",
    "print(f\"+ SMOTE model training complete in {train_time_smote:.2f} seconds\")\n",
    "\n",
    "# Store SMOTE results\n",
    "rf_smote_cv_mean = cv_scores_smote.mean()\n",
    "rf_smote_cv_std = cv_scores_smote.std()\n",
    "\n",
    "print(f\"\\n SMOTE Model Summary:\")\n",
    "print(f\"   Expected AUC: {rf_smote_cv_mean:.4f} +/- {rf_smote_cv_std:.4f}\")\n",
    "print(f\"   Stability: {'Excellent' if rf_smote_cv_std < 0.01 else 'Good' if rf_smote_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART B: RANDOM FOREST WITH ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART B: RANDOM FOREST + ADASYN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n About ADASYN:\")\n",
    "print(\"   - Adaptive Synthetic Sampling\")\n",
    "print(\"   - Focuses on harder-to-learn minority samples\")\n",
    "print(\"   - Creates MORE synthetics near decision boundary (harder cases)\")\n",
    "print(\"   - Creates FEWER synthetics in easy regions\")\n",
    "print(\"   - Better for complex, varied fraud patterns\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Create ADASYN Pipeline\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Creating ADASYN Pipeline\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Building pipeline with 2 steps:\")\n",
    "print(\"   Step 1: ADASYN (adaptive balance training data)\")\n",
    "print(\"   Step 2: Random Forest (classification with 100 trees)\")\n",
    "\n",
    "# Create ADASYN pipeline\n",
    "rf_adasyn_pipeline = ImbPipeline([\n",
    "    ('adasyn', ADASYN(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\n+ ADASYN pipeline created successfully!\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Cross-Validation with ADASYN\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running 5-Fold Cross-Validation with ADASYN\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n Running 5-fold cross-validation...\")\n",
    "print(f\"   Dataset: X_train_scaled ({total_samples:,} samples, {total_frauds} frauds)\")\n",
    "print(f\"   ADASYN will adaptively generate synthetics based on difficulty\")\n",
    "print(f\"   (This will take 3-5 minutes due to Random Forest complexity)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform 5-fold CV with ADASYN\n",
    "cv_scores_adasyn = cross_val_score(\n",
    "    rf_adasyn_pipeline,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cv_time_adasyn = time.time() - start_time\n",
    "\n",
    "# Display ADASYN results\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ADASYN Cross-Validation Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Cross-validation complete in {cv_time_adasyn:.2f} seconds ({cv_time_adasyn/60:.1f} minutes)\")\n",
    "\n",
    "print(f\"\\n Individual Fold Scores (ADASYN):\")\n",
    "for i, score in enumerate(cv_scores_adasyn, 1):\n",
    "    print(f\"   Fold {i}: AUC = {score:.4f}\")\n",
    "\n",
    "print(f\"\\n ADASYN Summary Statistics:\")\n",
    "print(f\"   Mean CV AUC:  {cv_scores_adasyn.mean():.4f}\")\n",
    "print(f\"   Std CV AUC:   {cv_scores_adasyn.std():.4f}\")\n",
    "print(f\"   Min CV AUC:   {cv_scores_adasyn.min():.4f}\")\n",
    "print(f\"   Max CV AUC:   {cv_scores_adasyn.max():.4f}\")\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval_adasyn = 1.96 * cv_scores_adasyn.std()\n",
    "print(f\"\\n 95% Confidence Interval:\")\n",
    "print(f\"   {cv_scores_adasyn.mean():.4f} +/- {confidence_interval_adasyn:.4f}\")\n",
    "print(f\"   Range: [{cv_scores_adasyn.mean() - confidence_interval_adasyn:.4f}, \"\n",
    "      f\"{cv_scores_adasyn.mean() + confidence_interval_adasyn:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train Final ADASYN Model\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Final ADASYN Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Training final model with ADASYN on all training data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_adasyn_pipeline.fit(X_train_scaled, y_train)\n",
    "train_time_adasyn = time.time() - start_time\n",
    "\n",
    "print(f\"+ ADASYN model training complete in {train_time_adasyn:.2f} seconds\")\n",
    "\n",
    "# Store ADASYN results\n",
    "rf_adasyn_cv_mean = cv_scores_adasyn.mean()\n",
    "rf_adasyn_cv_std = cv_scores_adasyn.std()\n",
    "\n",
    "print(f\"\\n ADASYN Model Summary:\")\n",
    "print(f\"   Expected AUC: {rf_adasyn_cv_mean:.4f} +/- {rf_adasyn_cv_std:.4f}\")\n",
    "print(f\"   Stability: {'Excellent' if rf_adasyn_cv_std < 0.01 else 'Good' if rf_adasyn_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON: SMOTE vs ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SMOTE vs ADASYN COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "print(f\"   SMOTE  Mean AUC: {rf_smote_cv_mean:.4f} (+/-{rf_smote_cv_std:.4f})\")\n",
    "print(f\"   ADASYN Mean AUC: {rf_adasyn_cv_mean:.4f} (+/-{rf_adasyn_cv_std:.4f})\")\n",
    "\n",
    "# Calculate difference\n",
    "auc_difference = rf_adasyn_cv_mean - rf_smote_cv_mean\n",
    "print(f\"\\n   Difference: {abs(auc_difference):.4f} ({'ADASYN better' if auc_difference > 0 else 'SMOTE better'})\")\n",
    "\n",
    "# Determine winner\n",
    "if abs(auc_difference) < 0.001:\n",
    "    winner = \"TIE - Performance essentially identical\"\n",
    "    recommendation = \"Either technique is suitable - choose based on training time\"\n",
    "elif auc_difference > 0:\n",
    "    winner = \"ADASYN WINS\"\n",
    "    recommendation = \"ADASYN provides better fraud detection for this dataset\"\n",
    "else:\n",
    "    winner = \"SMOTE WINS\"\n",
    "    recommendation = \"SMOTE provides better fraud detection for this dataset\"\n",
    "\n",
    "print(f\"\\n Winner: {winner}\")\n",
    "print(f\"\\n Recommendation: {recommendation}\")\n",
    "\n",
    "print(f\"\\n Training Time Comparison:\")\n",
    "print(f\"   SMOTE:  {cv_time_smote:.2f} seconds ({cv_time_smote/60:.1f} minutes)\")\n",
    "print(f\"   ADASYN: {cv_time_adasyn:.2f} seconds ({cv_time_adasyn/60:.1f} minutes)\")\n",
    "print(f\"   Difference: {abs(cv_time_smote - cv_time_adasyn):.2f} seconds ({'ADASYN slower' if cv_time_adasyn > cv_time_smote else 'SMOTE slower'})\")\n",
    "\n",
    "print(f\"\\n Stability Comparison:\")\n",
    "print(f\"   SMOTE  Std: {rf_smote_cv_std:.4f} ({'More stable' if rf_smote_cv_std < rf_adasyn_cv_std else 'Less stable'})\")\n",
    "print(f\"   ADASYN Std: {rf_adasyn_cv_std:.4f} ({'More stable' if rf_adasyn_cv_std < rf_smote_cv_std else 'Less stable'})\")\n",
    "\n",
    "# Statistical significance test\n",
    "print(f\"\\n Statistical Significance:\")\n",
    "smote_lower = rf_smote_cv_mean - confidence_interval_smote\n",
    "smote_upper = rf_smote_cv_mean + confidence_interval_smote\n",
    "adasyn_lower = rf_adasyn_cv_mean - confidence_interval_adasyn\n",
    "adasyn_upper = rf_adasyn_cv_mean + confidence_interval_adasyn\n",
    "\n",
    "if (smote_lower <= adasyn_upper) and (adasyn_lower <= smote_upper):\n",
    "    print(f\"   Confidence intervals overlap\")\n",
    "    print(f\"   Difference may not be statistically significant\")\n",
    "    print(f\"   Both methods perform similarly on this dataset\")\n",
    "else:\n",
    "    print(f\"   + Confidence intervals don't overlap\")\n",
    "    print(f\"   Difference is likely statistically significant\")\n",
    "    print(f\"   {winner} is meaningfully better\")\n",
    "\n",
    "# Visualization of comparison\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Visual Comparison\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Mean AUC Scores:\")\n",
    "print(f\"   SMOTE:  {'█' * int(rf_smote_cv_mean * 50)} {rf_smote_cv_mean:.4f}\")\n",
    "print(f\"   ADASYN: {'█' * int(rf_adasyn_cv_mean * 50)} {rf_adasyn_cv_mean:.4f}\")\n",
    "\n",
    "print(f\"\\n Consistency (Lower is Better):\")\n",
    "print(f\"   SMOTE:  {'█' * int(rf_smote_cv_std * 1000)} {rf_smote_cv_std:.4f}\")\n",
    "print(f\"   ADASYN: {'█' * int(rf_adasyn_cv_std * 1000)} {rf_adasyn_cv_std:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT BEST MODEL FOR FINAL USE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SELECTING BEST RESAMPLING TECHNIQUE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Choose best based on mean AUC (with tie-breaker on stability)\n",
    "if abs(auc_difference) < 0.001:\n",
    "    # Performance is essentially the same, choose more stable\n",
    "    if rf_smote_cv_std < rf_adasyn_cv_std:\n",
    "        best_pipeline = rf_smote_pipeline\n",
    "        best_method = \"SMOTE\"\n",
    "        best_cv_mean = rf_smote_cv_mean\n",
    "        best_cv_std = rf_smote_cv_std\n",
    "        best_cv_scores = cv_scores_smote\n",
    "        reason = \"Similar performance, but SMOTE is more stable\"\n",
    "    else:\n",
    "        best_pipeline = rf_adasyn_pipeline\n",
    "        best_method = \"ADASYN\"\n",
    "        best_cv_mean = rf_adasyn_cv_mean\n",
    "        best_cv_std = rf_adasyn_cv_std\n",
    "        best_cv_scores = cv_scores_adasyn\n",
    "        reason = \"Similar performance, but ADASYN is more stable\"\n",
    "else:\n",
    "    # Clear winner based on AUC\n",
    "    if rf_smote_cv_mean > rf_adasyn_cv_mean:\n",
    "        best_pipeline = rf_smote_pipeline\n",
    "        best_method = \"SMOTE\"\n",
    "        best_cv_mean = rf_smote_cv_mean\n",
    "        best_cv_std = rf_smote_cv_std\n",
    "        best_cv_scores = cv_scores_smote\n",
    "        reason = f\"Higher mean AUC ({rf_smote_cv_mean:.4f} vs {rf_adasyn_cv_mean:.4f})\"\n",
    "    else:\n",
    "        best_pipeline = rf_adasyn_pipeline\n",
    "        best_method = \"ADASYN\"\n",
    "        best_cv_mean = rf_adasyn_cv_mean\n",
    "        best_cv_std = rf_adasyn_cv_std\n",
    "        best_cv_scores = cv_scores_adasyn\n",
    "        reason = f\"Higher mean AUC ({rf_adasyn_cv_mean:.4f} vs {rf_smote_cv_mean:.4f})\"\n",
    "\n",
    "print(f\"\\n Selected Method: {best_method}\")\n",
    "print(f\"   Reason: {reason}\")\n",
    "print(f\"   Performance: {best_cv_mean:.4f} +/- {best_cv_std:.4f}\")\n",
    "\n",
    "print(f\"\\n Storing best model for later steps:\")\n",
    "print(f\"   Model: Random Forest + {best_method}\")\n",
    "print(f\"   Expected AUC: {best_cv_mean:.4f}\")\n",
    "\n",
    "# Store the best results for comparison in Step 21\n",
    "rf_final_pipeline = best_pipeline\n",
    "rf_cv_mean = best_cv_mean\n",
    "rf_cv_std = best_cv_std\n",
    "rf_cv_scores = best_cv_scores\n",
    "rf_best_method = best_method\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETER TUNING FOR BEST MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n WHAT WE'RE DOING:\")\n",
    "print(f\"   - Fine-tuning the best model ({best_method} + Random Forest)\")\n",
    "print(f\"   - Testing different hyperparameter combinations\")\n",
    "print(f\"   - Using GridSearchCV with cross-validation\")\n",
    "print(f\"   - Goal: Improve beyond baseline {best_cv_mean:.4f} AUC\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Define Hyperparameter Grid\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Defining Hyperparameter Search Space\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Hyperparameters to tune:\")\n",
    "print(\"   1. n_estimators (Number of trees)\")\n",
    "print(\"      - More trees = more voting experts\")\n",
    "print(\"      - But diminishing returns after certain point\")\n",
    "print(\"      - Trade-off: accuracy vs training time\")\n",
    "print(\"\\n   2. max_depth (Maximum tree depth)\")\n",
    "print(\"      - Controls how deep each tree can grow\")\n",
    "print(\"      - Deeper trees = more complex patterns\")\n",
    "print(\"      - Trade-off: complexity vs overfitting\")\n",
    "print(\"\\n   3. min_samples_split (Minimum samples to split node)\")\n",
    "print(\"      - How many samples needed before splitting\")\n",
    "print(\"      - Higher values = more conservative splitting\")\n",
    "print(\"      - Trade-off: generalization vs pattern detection\")\n",
    "print(\"\\n   4. min_samples_leaf (Minimum samples per leaf)\")\n",
    "print(\"      - How many samples must be in final decision\")\n",
    "print(\"      - Higher values = more evidence per decision\")\n",
    "print(\"      - Trade-off: generalization vs specificity\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [20, 30, None],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "print(\"\\n Parameter Grid:\")\n",
    "print(f\"   n_estimators: {param_grid['classifier__n_estimators']}\")\n",
    "print(f\"   max_depth: {param_grid['classifier__max_depth']}\")\n",
    "print(f\"   min_samples_split: {param_grid['classifier__min_samples_split']}\")\n",
    "print(f\"   min_samples_leaf: {param_grid['classifier__min_samples_leaf']}\")\n",
    "print(f\"\\n   Total combinations: {len(param_grid['classifier__n_estimators']) * len(param_grid['classifier__max_depth']) * len(param_grid['classifier__min_samples_split']) * len(param_grid['classifier__min_samples_leaf'])}\")\n",
    "print(f\"   With 5-fold CV: {len(param_grid['classifier__n_estimators']) * len(param_grid['classifier__max_depth']) * len(param_grid['classifier__min_samples_split']) * len(param_grid['classifier__min_samples_leaf']) * 5} model fits\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Run Grid Search\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running Grid Search with Cross-Validation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n This will take 15-25 minutes...\")\n",
    "print(\"   Each combination is tested with 5-fold CV\")\n",
    "print(\"   Random Forest is computationally intensive\")\n",
    "print(\"   Progress will be shown below\")\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_final_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Run grid search\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Display Results\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Grid Search Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Grid search complete in {grid_time:.2f} seconds ({grid_time/60:.1f} minutes)\")\n",
    "\n",
    "print(f\"\\n Best Parameters Found:\")\n",
    "for param_name, param_value in grid_search.best_params_.items():\n",
    "    # Remove 'classifier__' prefix for cleaner display\n",
    "    clean_name = param_name.replace('classifier__', '')\n",
    "    print(f\"   {clean_name}: {param_value}\")\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "print(f\"   Baseline AUC:  {best_cv_mean:.4f} (+/-{best_cv_std:.4f})\")\n",
    "print(f\"   Tuned AUC:     {grid_search.best_score_:.4f} (+/-{grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.4f})\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = grid_search.best_score_ - best_cv_mean\n",
    "improvement_pct = (improvement / best_cv_mean) * 100\n",
    "\n",
    "print(f\"\\n Improvement:\")\n",
    "if improvement > 0:\n",
    "    print(f\"   +{improvement:.4f} AUC ({improvement_pct:+.2f}%)\")\n",
    "    print(f\"   + Hyperparameter tuning improved performance!\")\n",
    "elif improvement < -0.001:\n",
    "    print(f\"   {improvement:.4f} AUC ({improvement_pct:.2f}%)\")\n",
    "    print(f\"   Tuned model performed slightly worse\")\n",
    "    print(f\"   -> Baseline model was already well-optimized\")\n",
    "else:\n",
    "    print(f\"   ~{improvement:.4f} AUC (essentially no change)\")\n",
    "    print(f\"   -> Baseline hyperparameters were already optimal\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Top 5 Parameter Combinations\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Top 5 Hyperparameter Combinations\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Get results sorted by score\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df = results_df.sort_values('rank_test_score')\n",
    "\n",
    "print(\"\\n Best performing combinations:\\n\")\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"   Rank {i+1}:\")\n",
    "    print(f\"      n_estimators={row['param_classifier__n_estimators']}, \"\n",
    "          f\"max_depth={row['param_classifier__max_depth']}, \"\n",
    "          f\"min_samples_split={row['param_classifier__min_samples_split']}, \"\n",
    "          f\"min_samples_leaf={row['param_classifier__min_samples_leaf']}\")\n",
    "    print(f\"      Mean AUC: {row['mean_test_score']:.4f} (+/-{row['std_test_score']:.4f})\")\n",
    "    print()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Store Tuned Model\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Storing Tuned Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Store the tuned model\n",
    "rf_tuned_pipeline = grid_search.best_estimator_\n",
    "rf_tuned_cv_mean = grid_search.best_score_\n",
    "rf_tuned_cv_std = grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "rf_best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"\\n+ Tuned model stored for final evaluation\")\n",
    "print(f\"   Best hyperparameters saved\")\n",
    "print(f\"   Expected AUC: {rf_tuned_cv_mean:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Decision: Use Baseline or Tuned?\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Model Selection Decision\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Decide whether to use tuned or baseline model\n",
    "if improvement > 0.0001:  # Meaningful improvement\n",
    "    print(f\"\\n+ Using TUNED model for final evaluation\")\n",
    "    print(f\"   Reason: Tuning improved performance by {improvement:.4f}\")\n",
    "    rf_final_model = rf_tuned_pipeline\n",
    "    rf_final_cv_mean = rf_tuned_cv_mean\n",
    "    rf_final_cv_std = rf_tuned_cv_std\n",
    "    model_version = \"Tuned\"\n",
    "else:\n",
    "    print(f\"\\n+ Using BASELINE model for final evaluation\")\n",
    "    print(f\"   Reason: Tuning did not provide meaningful improvement\")\n",
    "    print(f\"   Baseline model was already well-optimized\")\n",
    "    rf_final_model = rf_final_pipeline\n",
    "    rf_final_cv_mean = best_cv_mean\n",
    "    rf_final_cv_std = best_cv_std\n",
    "    model_version = \"Baseline\"\n",
    "\n",
    "print(f\"\\n Final Random Forest Model:\")\n",
    "print(f\"   Version: {model_version}\")\n",
    "print(f\"   Resampling: {best_method}\")\n",
    "print(f\"   Expected AUC: {rf_final_cv_mean:.4f} (+/-{rf_final_cv_std:.4f})\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE MODEL AND RESULTS TO DISK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL AND RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Saving to ../models/ directory...\")\n",
    "\n",
    "# Save the final model\n",
    "model_path = '../models/random_forest_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(rf_final_model, f)\n",
    "print(f\"   + Model saved: {model_path}\")\n",
    "\n",
    "# Save all results and metadata\n",
    "results_data = {\n",
    "    'model_name': 'Random Forest',\n",
    "    'resampling_method': best_method,\n",
    "    'model_version': model_version,\n",
    "    'cv_mean': rf_final_cv_mean,\n",
    "    'cv_std': rf_final_cv_std,\n",
    "    'cv_scores': rf_cv_scores,\n",
    "    'best_params': rf_best_params if model_version == \"Tuned\" else None,\n",
    "    'training_samples': total_samples,\n",
    "    'training_frauds': total_frauds,\n",
    "    'timestamp': pd.Timestamp.now()\n",
    "}\n",
    "\n",
    "results_path = '../models/random_forest_results.pkl'\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(results_data, f)\n",
    "print(f\"   + Results saved: {results_path}\")\n",
    "\n",
    "print(f\"\\n+ All files saved successfully!\")\n",
    "print(f\"   Model can now be loaded in Phase 5 for comparison\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 18 COMPLETE - KEY ACCOMPLISHMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n What We Accomplished:\")\n",
    "print(\"   + Built Random Forest classifier (ensemble of 100 trees)\")\n",
    "print(\"   + Compared SMOTE vs ADASYN performance\")\n",
    "print(\"   + Used proper CV methodology (resampling inside folds)\")\n",
    "print(\"   + Validated on 100% real data (no synthetic contamination)\")\n",
    "print(\"   + Selected best resampling technique based on performance\")\n",
    "print(\"   + Performed hyperparameter tuning with GridSearchCV\")\n",
    "print(\"   + Trained final optimized model on all available training data\")\n",
    "print(\"   + SAVED model and results to disk\")\n",
    "\n",
    "print(f\"\\n Final Results:\")\n",
    "print(f\"   Best Method: {best_method}\")\n",
    "print(f\"   Model Version: {model_version}\")\n",
    "print(f\"   Cross-Validation AUC: {rf_final_cv_mean:.4f} (+/-{rf_final_cv_std:.4f})\")\n",
    "print(f\"   Model Stability: {'Excellent' if rf_final_cv_std < 0.01 else 'Good' if rf_final_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "print(\"\\n Saved Files:\")\n",
    "print(f\"   - {model_path}\")\n",
    "print(f\"   - {results_path}\")\n",
    "\n",
    "print(\"\\n Next Steps:\")\n",
    "print(\"   -> Step 19: Build XGBoost Model\")\n",
    "print(\"   -> Step 20: Build Neural Network (Deep Learning)\")\n",
    "print(\"   -> Step 21: Compare all models and select winner\")\n",
    "\n",
    "print(\"\\n Important Notes:\")\n",
    "print(\"   - Test set remains UNTOUCHED for final evaluation in Phase 5\")\n",
    "print(\"   - Model saved and can be loaded anytime (no need to retrain)\")\n",
    "print(\"   - Can restart PC without losing progress\")\n",
    "print(f\"   - {best_method} + {model_version} model ready for Phase 5\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2428c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL 3: XGBOOST (EXTREME GRADIENT BOOSTING)\n",
      "======================================================================\n",
      "\n",
      " WHAT WE'RE DOING:\n",
      "   - Building XGBoost classifier (sequential ensemble of trees)\n",
      "   - Testing TWO resampling techniques: SMOTE vs ADASYN\n",
      "   - Using 5-fold cross-validation for performance estimation\n",
      "   - Applying resampling INSIDE each CV fold (proper methodology)\n",
      "   - Validating on REAL data only (no synthetic frauds in validation)\n",
      "   - SAVING model and results to disk for later use\n",
      "\n",
      "======================================================================\n",
      "ABOUT XGBOOST\n",
      "======================================================================\n",
      "\n",
      " What is XGBoost?\n",
      "   - eXtreme Gradient Boosting\n",
      "   - Builds trees SEQUENTIALLY (one after another)\n",
      "   - Each new tree learns from previous trees' mistakes\n",
      "   - Industry-standard algorithm for structured/tabular data\n",
      "   - Often wins Kaggle competitions and used in production systems\n",
      "\n",
      " How XGBoost Differs from Random Forest:\n",
      "   Random Forest:\n",
      "      - Builds all trees at SAME TIME (parallel)\n",
      "      - Trees are independent (don't talk to each other)\n",
      "      - Final prediction = VOTE from all trees\n",
      "      - Good performance, but leaves room for improvement\n",
      "\n",
      "   XGBoost:\n",
      "      - Builds trees ONE AT A TIME (sequential)\n",
      "      - Each tree learns from previous mistakes\n",
      "      - Final prediction = SUM of all tree adjustments\n",
      "      - Typically achieves better performance\n",
      "\n",
      " Why XGBoost for Fraud Detection?\n",
      "   + Handles imbalanced data well (our 0.173% fraud rate)\n",
      "   + Captures complex non-linear patterns\n",
      "   + Built-in regularization prevents overfitting\n",
      "   + Computationally efficient (uses CPU parallelization)\n",
      "   + Industry-proven for fraud detection tasks\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION\n",
      "======================================================================\n",
      "\n",
      " Training Data Overview:\n",
      "   Total samples: 226,980\n",
      "   Normal transactions: 226,602 (99.833%)\n",
      "   Fraud transactions: 378 (0.167%)\n",
      "   Class imbalance ratio: 599.5:1\n",
      "\n",
      " Resampling Strategy:\n",
      "   - Testing both SMOTE and ADASYN\n",
      "   - Resampling applied within each CV fold\n",
      "   - Validation always on 100% real data (no synthetic)\n",
      "\n",
      "======================================================================\n",
      "PART A: XGBOOST + SMOTE\n",
      "======================================================================\n",
      "\n",
      " About XGBoost:\n",
      "   - Sequential gradient boosting algorithm\n",
      "   - Each tree corrects mistakes of previous trees\n",
      "   - Combines predictions through weighted sum\n",
      "   - More sophisticated than Random Forest\n",
      "   - Often achieves best performance on tabular data\n",
      "\n",
      " About SMOTE:\n",
      "   - Synthetic Minority Over-sampling Technique\n",
      "   - Creates synthetic samples by interpolating between existing frauds\n",
      "   - Places new samples along line segments between neighbors\n",
      "   - Good for general class imbalance\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Creating SMOTE Pipeline\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Building pipeline with 2 steps:\n",
      "   Step 1: SMOTE (balance training data)\n",
      "   Step 2: XGBoost (gradient boosting classification)\n",
      "\n",
      "+ SMOTE pipeline created successfully!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running 5-Fold Cross-Validation with SMOTE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Running 5-fold cross-validation...\n",
      "   Dataset: X_train_scaled (226,980 samples, 378 frauds)\n",
      "   Each fold: ~45,396 samples (~75 real frauds)\n",
      "   (This will take 2-4 minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "SMOTE Cross-Validation Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Cross-validation complete in 6.70 seconds (0.1 minutes)\n",
      "\n",
      " Individual Fold Scores (SMOTE):\n",
      "   Fold 1: AUC = 0.9548\n",
      "   Fold 2: AUC = 0.9724\n",
      "   Fold 3: AUC = 0.9961\n",
      "   Fold 4: AUC = 0.9760\n",
      "   Fold 5: AUC = 0.9797\n",
      "\n",
      " SMOTE Summary Statistics:\n",
      "   Mean CV AUC:  0.9758\n",
      "   Std CV AUC:   0.0133\n",
      "   Min CV AUC:   0.9548\n",
      "   Max CV AUC:   0.9961\n",
      "\n",
      " 95% Confidence Interval:\n",
      "   0.9758 +/- 0.0260\n",
      "   Range: [0.9498, 1.0018]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Final SMOTE Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Training final model with SMOTE on all training data...\n",
      "+ SMOTE model training complete in 1.13 seconds\n",
      "\n",
      " SMOTE Model Summary:\n",
      "   Expected AUC: 0.9758 +/- 0.0133\n",
      "   Stability: Good\n",
      "\n",
      "======================================================================\n",
      "PART B: XGBOOST + ADASYN\n",
      "======================================================================\n",
      "\n",
      " About ADASYN:\n",
      "   - Adaptive Synthetic Sampling\n",
      "   - Focuses on harder-to-learn minority samples\n",
      "   - Creates MORE synthetics near decision boundary (harder cases)\n",
      "   - Creates FEWER synthetics in easy regions\n",
      "   - Better for complex, varied fraud patterns\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Creating ADASYN Pipeline\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Building pipeline with 2 steps:\n",
      "   Step 1: ADASYN (adaptive balance training data)\n",
      "   Step 2: XGBoost (gradient boosting classification)\n",
      "\n",
      "+ ADASYN pipeline created successfully!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running 5-Fold Cross-Validation with ADASYN\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Running 5-fold cross-validation...\n",
      "   Dataset: X_train_scaled (226,980 samples, 378 frauds)\n",
      "   ADASYN will adaptively generate synthetics based on difficulty\n",
      "   (This will take 2-4 minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "ADASYN Cross-Validation Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Cross-validation complete in 5.87 seconds (0.1 minutes)\n",
      "\n",
      " Individual Fold Scores (ADASYN):\n",
      "   Fold 1: AUC = 0.9489\n",
      "   Fold 2: AUC = 0.9688\n",
      "   Fold 3: AUC = 0.9962\n",
      "   Fold 4: AUC = 0.9749\n",
      "   Fold 5: AUC = 0.9841\n",
      "\n",
      " ADASYN Summary Statistics:\n",
      "   Mean CV AUC:  0.9746\n",
      "   Std CV AUC:   0.0158\n",
      "   Min CV AUC:   0.9489\n",
      "   Max CV AUC:   0.9962\n",
      "\n",
      " 95% Confidence Interval:\n",
      "   0.9746 +/- 0.0310\n",
      "   Range: [0.9436, 1.0056]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Final ADASYN Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Training final model with ADASYN on all training data...\n",
      "+ ADASYN model training complete in 0.98 seconds\n",
      "\n",
      " ADASYN Model Summary:\n",
      "   Expected AUC: 0.9746 +/- 0.0158\n",
      "   Stability: Good\n",
      "\n",
      "======================================================================\n",
      "SMOTE vs ADASYN COMPARISON\n",
      "======================================================================\n",
      "\n",
      " Performance Comparison:\n",
      "   SMOTE  Mean AUC: 0.9758 (+/-0.0133)\n",
      "   ADASYN Mean AUC: 0.9746 (+/-0.0158)\n",
      "\n",
      "   Difference: 0.0012 (SMOTE better)\n",
      "\n",
      " Winner: SMOTE WINS\n",
      "\n",
      " Recommendation: SMOTE provides better fraud detection for this dataset\n",
      "\n",
      " Training Time Comparison:\n",
      "   SMOTE:  6.70 seconds (0.1 minutes)\n",
      "   ADASYN: 5.87 seconds (0.1 minutes)\n",
      "   Difference: 0.83 seconds (SMOTE slower)\n",
      "\n",
      " Stability Comparison:\n",
      "   SMOTE  Std: 0.0133 (More stable)\n",
      "   ADASYN Std: 0.0158 (Less stable)\n",
      "\n",
      " Statistical Significance:\n",
      "   Confidence intervals overlap\n",
      "   Difference may not be statistically significant\n",
      "   Both methods perform similarly on this dataset\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Visual Comparison\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Mean AUC Scores:\n",
      "   SMOTE:  ████████████████████████████████████████████████ 0.9758\n",
      "   ADASYN: ████████████████████████████████████████████████ 0.9746\n",
      "\n",
      " Consistency (Lower is Better):\n",
      "   SMOTE:  █████████████ 0.0133\n",
      "   ADASYN: ███████████████ 0.0158\n",
      "\n",
      "======================================================================\n",
      "SELECTING BEST RESAMPLING TECHNIQUE\n",
      "======================================================================\n",
      "\n",
      " Selected Method: SMOTE\n",
      "   Reason: Higher mean AUC (0.9758 vs 0.9746)\n",
      "   Performance: 0.9758 +/- 0.0133\n",
      "\n",
      " Storing best model for later steps:\n",
      "   Model: XGBoost + SMOTE\n",
      "   Expected AUC: 0.9758\n",
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING - XGBOOST\n",
      "======================================================================\n",
      "\n",
      " WHAT WE'RE DOING:\n",
      "   - Fine-tuning the best model (SMOTE + XGBoost)\n",
      "   - Testing different hyperparameter combinations\n",
      "   - Using GridSearchCV with cross-validation\n",
      "   - Goal: Improve beyond baseline 0.9758 AUC\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Defining Hyperparameter Search Space\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Hyperparameters to tune:\n",
      "   1. n_estimators (Number of boosting rounds)\n",
      "      - Number of sequential trees to build\n",
      "      - More trees = more learning iterations\n",
      "      - Trade-off: accuracy vs training time\n",
      "\n",
      "   2. max_depth (Maximum tree depth)\n",
      "      - Controls how deep each tree can grow\n",
      "      - Deeper trees = more complex patterns\n",
      "      - Trade-off: complexity vs overfitting\n",
      "\n",
      "   3. learning_rate (Step size for each tree)\n",
      "      - How much each tree contributes to final prediction\n",
      "      - Smaller rate = more conservative learning\n",
      "      - Trade-off: accuracy vs training time\n",
      "\n",
      "   4. subsample (Fraction of samples per tree)\n",
      "      - Percentage of data used to train each tree\n",
      "      - Lower values = more randomness, less overfitting\n",
      "      - Trade-off: generalization vs training stability\n",
      "\n",
      " Parameter Grid:\n",
      "   n_estimators: [100, 200, 300]\n",
      "   max_depth: [3, 5, 7]\n",
      "   learning_rate: [0.01, 0.1, 0.3]\n",
      "   subsample: [0.8, 1.0]\n",
      "\n",
      "   Total combinations: 54\n",
      "   With 5-fold CV: 270 model fits\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running Grid Search with Cross-Validation\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " This will take 10-15 minutes...\n",
      "   Each combination is tested with 5-fold CV\n",
      "   XGBoost is computationally intensive\n",
      "   Progress will be shown below\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Grid Search Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Grid search complete in 234.52 seconds (3.9 minutes)\n",
      "\n",
      " Best Parameters Found:\n",
      "   learning_rate: 0.1\n",
      "   max_depth: 3\n",
      "   n_estimators: 100\n",
      "   subsample: 0.8\n",
      "\n",
      " Performance Comparison:\n",
      "   Baseline AUC:  0.9758 (+/-0.0133)\n",
      "   Tuned AUC:     0.9797 (+/-0.0122)\n",
      "\n",
      " Improvement:\n",
      "   +0.0039 AUC (+0.40%)\n",
      "   + Hyperparameter tuning improved performance!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Top 5 Hyperparameter Combinations\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Best performing combinations:\n",
      "\n",
      "   Rank 1:\n",
      "      n_estimators=100, max_depth=3, learning_rate=0.1, subsample=0.8\n",
      "      Mean AUC: 0.9797 (+/-0.0122)\n",
      "\n",
      "   Rank 2:\n",
      "      n_estimators=300, max_depth=3, learning_rate=0.01, subsample=0.8\n",
      "      Mean AUC: 0.9791 (+/-0.0126)\n",
      "\n",
      "   Rank 3:\n",
      "      n_estimators=300, max_depth=5, learning_rate=0.3, subsample=0.8\n",
      "      Mean AUC: 0.9790 (+/-0.0133)\n",
      "\n",
      "   Rank 4:\n",
      "      n_estimators=300, max_depth=3, learning_rate=0.01, subsample=1.0\n",
      "      Mean AUC: 0.9789 (+/-0.0128)\n",
      "\n",
      "   Rank 5:\n",
      "      n_estimators=300, max_depth=7, learning_rate=0.3, subsample=1.0\n",
      "      Mean AUC: 0.9786 (+/-0.0128)\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Storing Tuned Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Tuned model stored for final evaluation\n",
      "   Best hyperparameters saved\n",
      "   Expected AUC: 0.9797\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model Selection Decision\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Using TUNED model for final evaluation\n",
      "   Reason: Tuning improved performance by 0.0039\n",
      "\n",
      " Final XGBoost Model:\n",
      "   Version: Tuned\n",
      "   Resampling: SMOTE\n",
      "   Expected AUC: 0.9797 (+/-0.0122)\n",
      "\n",
      "======================================================================\n",
      "SAVING MODEL AND RESULTS\n",
      "======================================================================\n",
      "\n",
      " Saving to ../models/ directory...\n",
      "   + Model saved: ../models/xgboost_model.pkl\n",
      "   + Results saved: ../models/xgboost_results.pkl\n",
      "\n",
      "+ All files saved successfully!\n",
      "   Model can now be loaded in Phase 5 for comparison\n",
      "\n",
      "======================================================================\n",
      "STEP 19 COMPLETE - KEY ACCOMPLISHMENTS\n",
      "======================================================================\n",
      "\n",
      " What We Accomplished:\n",
      "   + Built XGBoost classifier (gradient boosting)\n",
      "   + Compared SMOTE vs ADASYN performance\n",
      "   + Used proper CV methodology (resampling inside folds)\n",
      "   + Validated on 100% real data (no synthetic contamination)\n",
      "   + Selected best resampling technique based on performance\n",
      "   + Performed hyperparameter tuning with GridSearchCV\n",
      "   + Trained final optimized model on all available training data\n",
      "   + SAVED model and results to disk\n",
      "\n",
      " Final Results:\n",
      "   Best Method: SMOTE\n",
      "   Model Version: Tuned\n",
      "   Cross-Validation AUC: 0.9797 (+/-0.0122)\n",
      "   Model Stability: Good\n",
      "\n",
      " Saved Files:\n",
      "   - ../models/xgboost_model.pkl\n",
      "   - ../models/xgboost_results.pkl\n",
      "\n",
      " Next Steps:\n",
      "   -> Step 20: Build Neural Network (Deep Learning with GPU)\n",
      "   -> Step 21: Compare all models and select winner\n",
      "\n",
      " Important Notes:\n",
      "   - Test set remains UNTOUCHED for final evaluation in Phase 5\n",
      "   - Model saved and can be loaded anytime (no need to retrain)\n",
      "   - Can restart PC without losing progress\n",
      "   - SMOTE + Tuned model ready for Phase 5\n",
      "   - All models trained with consistent methodology for fair comparison\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 19: BUILD XGBOOST MODEL (WITH SMOTE AND ADASYN)\n",
    "# ============================================================================\n",
    "\n",
    "# Import additional required libraries\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 3: XGBOOST (EXTREME GRADIENT BOOSTING)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n WHAT WE'RE DOING:\")\n",
    "print(\"   - Building XGBoost classifier (sequential ensemble of trees)\")\n",
    "print(\"   - Testing TWO resampling techniques: SMOTE vs ADASYN\")\n",
    "print(\"   - Using 5-fold cross-validation for performance estimation\")\n",
    "print(\"   - Applying resampling INSIDE each CV fold (proper methodology)\")\n",
    "print(\"   - Validating on REAL data only (no synthetic frauds in validation)\")\n",
    "print(\"   - SAVING model and results to disk for later use\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# UNDERSTANDING XGBOOST\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABOUT XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n What is XGBoost?\")\n",
    "print(\"   - eXtreme Gradient Boosting\")\n",
    "print(\"   - Builds trees SEQUENTIALLY (one after another)\")\n",
    "print(\"   - Each new tree learns from previous trees' mistakes\")\n",
    "print(\"   - Industry-standard algorithm for structured/tabular data\")\n",
    "print(\"   - Often wins Kaggle competitions and used in production systems\")\n",
    "\n",
    "print(\"\\n How XGBoost Differs from Random Forest:\")\n",
    "print(\"   Random Forest:\")\n",
    "print(\"      - Builds all trees at SAME TIME (parallel)\")\n",
    "print(\"      - Trees are independent (don't talk to each other)\")\n",
    "print(\"      - Final prediction = VOTE from all trees\")\n",
    "print(\"      - Good performance, but leaves room for improvement\")\n",
    "print(\"\\n   XGBoost:\")\n",
    "print(\"      - Builds trees ONE AT A TIME (sequential)\")\n",
    "print(\"      - Each tree learns from previous mistakes\")\n",
    "print(\"      - Final prediction = SUM of all tree adjustments\")\n",
    "print(\"      - Typically achieves better performance\")\n",
    "\n",
    "print(\"\\n Why XGBoost for Fraud Detection?\")\n",
    "print(\"   + Handles imbalanced data well (our 0.173% fraud rate)\")\n",
    "print(\"   + Captures complex non-linear patterns\")\n",
    "print(\"   + Built-in regularization prevents overfitting\")\n",
    "print(\"   + Computationally efficient (uses CPU parallelization)\")\n",
    "print(\"   + Industry-proven for fraud detection tasks\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA OVERVIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate dataset info dynamically\n",
    "total_samples = len(X_train_scaled)\n",
    "total_frauds = int(y_train.sum())\n",
    "total_normal = total_samples - total_frauds\n",
    "fraud_percentage = (total_frauds / total_samples) * 100\n",
    "\n",
    "print(f\"\\n Training Data Overview:\")\n",
    "print(f\"   Total samples: {total_samples:,}\")\n",
    "print(f\"   Normal transactions: {total_normal:,} ({100-fraud_percentage:.3f}%)\")\n",
    "print(f\"   Fraud transactions: {total_frauds} ({fraud_percentage:.3f}%)\")\n",
    "print(f\"   Class imbalance ratio: {total_normal/total_frauds:.1f}:1\")\n",
    "\n",
    "print(\"\\n Resampling Strategy:\")\n",
    "print(\"   - Testing both SMOTE and ADASYN\")\n",
    "print(\"   - Resampling applied within each CV fold\")\n",
    "print(\"   - Validation always on 100% real data (no synthetic)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART A: XGBOOST WITH SMOTE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART A: XGBOOST + SMOTE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n About XGBoost:\")\n",
    "print(\"   - Sequential gradient boosting algorithm\")\n",
    "print(\"   - Each tree corrects mistakes of previous trees\")\n",
    "print(\"   - Combines predictions through weighted sum\")\n",
    "print(\"   - More sophisticated than Random Forest\")\n",
    "print(\"   - Often achieves best performance on tabular data\")\n",
    "\n",
    "print(\"\\n About SMOTE:\")\n",
    "print(\"   - Synthetic Minority Over-sampling Technique\")\n",
    "print(\"   - Creates synthetic samples by interpolating between existing frauds\")\n",
    "print(\"   - Places new samples along line segments between neighbors\")\n",
    "print(\"   - Good for general class imbalance\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Create SMOTE Pipeline\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Creating SMOTE Pipeline\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Building pipeline with 2 steps:\")\n",
    "print(\"   Step 1: SMOTE (balance training data)\")\n",
    "print(\"   Step 2: XGBoost (gradient boosting classification)\")\n",
    "\n",
    "# Create SMOTE pipeline\n",
    "xgb_smote_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\n+ SMOTE pipeline created successfully!\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Cross-Validation with SMOTE\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running 5-Fold Cross-Validation with SMOTE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "samples_per_fold = total_samples // 5\n",
    "frauds_per_fold = total_frauds // 5\n",
    "\n",
    "print(f\"\\n Running 5-fold cross-validation...\")\n",
    "print(f\"   Dataset: X_train_scaled ({total_samples:,} samples, {total_frauds} frauds)\")\n",
    "print(f\"   Each fold: ~{samples_per_fold:,} samples (~{frauds_per_fold} real frauds)\")\n",
    "print(f\"   (This will take 2-4 minutes)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform 5-fold CV with SMOTE\n",
    "cv_scores_smote = cross_val_score(\n",
    "    xgb_smote_pipeline,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cv_time_smote = time.time() - start_time\n",
    "\n",
    "# Display SMOTE results\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"SMOTE Cross-Validation Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Cross-validation complete in {cv_time_smote:.2f} seconds ({cv_time_smote/60:.1f} minutes)\")\n",
    "\n",
    "print(f\"\\n Individual Fold Scores (SMOTE):\")\n",
    "for i, score in enumerate(cv_scores_smote, 1):\n",
    "    print(f\"   Fold {i}: AUC = {score:.4f}\")\n",
    "\n",
    "print(f\"\\n SMOTE Summary Statistics:\")\n",
    "print(f\"   Mean CV AUC:  {cv_scores_smote.mean():.4f}\")\n",
    "print(f\"   Std CV AUC:   {cv_scores_smote.std():.4f}\")\n",
    "print(f\"   Min CV AUC:   {cv_scores_smote.min():.4f}\")\n",
    "print(f\"   Max CV AUC:   {cv_scores_smote.max():.4f}\")\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval_smote = 1.96 * cv_scores_smote.std()\n",
    "print(f\"\\n 95% Confidence Interval:\")\n",
    "print(f\"   {cv_scores_smote.mean():.4f} +/- {confidence_interval_smote:.4f}\")\n",
    "print(f\"   Range: [{cv_scores_smote.mean() - confidence_interval_smote:.4f}, \"\n",
    "      f\"{cv_scores_smote.mean() + confidence_interval_smote:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train Final SMOTE Model\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Final SMOTE Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Training final model with SMOTE on all training data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_smote_pipeline.fit(X_train_scaled, y_train)\n",
    "train_time_smote = time.time() - start_time\n",
    "\n",
    "print(f\"+ SMOTE model training complete in {train_time_smote:.2f} seconds\")\n",
    "\n",
    "# Store SMOTE results\n",
    "xgb_smote_cv_mean = cv_scores_smote.mean()\n",
    "xgb_smote_cv_std = cv_scores_smote.std()\n",
    "\n",
    "print(f\"\\n SMOTE Model Summary:\")\n",
    "print(f\"   Expected AUC: {xgb_smote_cv_mean:.4f} +/- {xgb_smote_cv_std:.4f}\")\n",
    "print(f\"   Stability: {'Excellent' if xgb_smote_cv_std < 0.01 else 'Good' if xgb_smote_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART B: XGBOOST WITH ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART B: XGBOOST + ADASYN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n About ADASYN:\")\n",
    "print(\"   - Adaptive Synthetic Sampling\")\n",
    "print(\"   - Focuses on harder-to-learn minority samples\")\n",
    "print(\"   - Creates MORE synthetics near decision boundary (harder cases)\")\n",
    "print(\"   - Creates FEWER synthetics in easy regions\")\n",
    "print(\"   - Better for complex, varied fraud patterns\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Create ADASYN Pipeline\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Creating ADASYN Pipeline\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Building pipeline with 2 steps:\")\n",
    "print(\"   Step 1: ADASYN (adaptive balance training data)\")\n",
    "print(\"   Step 2: XGBoost (gradient boosting classification)\")\n",
    "\n",
    "# Create ADASYN pipeline\n",
    "xgb_adasyn_pipeline = ImbPipeline([\n",
    "    ('adasyn', ADASYN(random_state=42)),\n",
    "    ('classifier', XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\n+ ADASYN pipeline created successfully!\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Cross-Validation with ADASYN\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running 5-Fold Cross-Validation with ADASYN\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n Running 5-fold cross-validation...\")\n",
    "print(f\"   Dataset: X_train_scaled ({total_samples:,} samples, {total_frauds} frauds)\")\n",
    "print(f\"   ADASYN will adaptively generate synthetics based on difficulty\")\n",
    "print(f\"   (This will take 2-4 minutes)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform 5-fold CV with ADASYN\n",
    "cv_scores_adasyn = cross_val_score(\n",
    "    xgb_adasyn_pipeline,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cv_time_adasyn = time.time() - start_time\n",
    "\n",
    "# Display ADASYN results\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ADASYN Cross-Validation Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Cross-validation complete in {cv_time_adasyn:.2f} seconds ({cv_time_adasyn/60:.1f} minutes)\")\n",
    "\n",
    "print(f\"\\n Individual Fold Scores (ADASYN):\")\n",
    "for i, score in enumerate(cv_scores_adasyn, 1):\n",
    "    print(f\"   Fold {i}: AUC = {score:.4f}\")\n",
    "\n",
    "print(f\"\\n ADASYN Summary Statistics:\")\n",
    "print(f\"   Mean CV AUC:  {cv_scores_adasyn.mean():.4f}\")\n",
    "print(f\"   Std CV AUC:   {cv_scores_adasyn.std():.4f}\")\n",
    "print(f\"   Min CV AUC:   {cv_scores_adasyn.min():.4f}\")\n",
    "print(f\"   Max CV AUC:   {cv_scores_adasyn.max():.4f}\")\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval_adasyn = 1.96 * cv_scores_adasyn.std()\n",
    "print(f\"\\n 95% Confidence Interval:\")\n",
    "print(f\"   {cv_scores_adasyn.mean():.4f} +/- {confidence_interval_adasyn:.4f}\")\n",
    "print(f\"   Range: [{cv_scores_adasyn.mean() - confidence_interval_adasyn:.4f}, \"\n",
    "      f\"{cv_scores_adasyn.mean() + confidence_interval_adasyn:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train Final ADASYN Model\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Final ADASYN Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Training final model with ADASYN on all training data...\")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_adasyn_pipeline.fit(X_train_scaled, y_train)\n",
    "train_time_adasyn = time.time() - start_time\n",
    "\n",
    "print(f\"+ ADASYN model training complete in {train_time_adasyn:.2f} seconds\")\n",
    "\n",
    "# Store ADASYN results\n",
    "xgb_adasyn_cv_mean = cv_scores_adasyn.mean()\n",
    "xgb_adasyn_cv_std = cv_scores_adasyn.std()\n",
    "\n",
    "print(f\"\\n ADASYN Model Summary:\")\n",
    "print(f\"   Expected AUC: {xgb_adasyn_cv_mean:.4f} +/- {xgb_adasyn_cv_std:.4f}\")\n",
    "print(f\"   Stability: {'Excellent' if xgb_adasyn_cv_std < 0.01 else 'Good' if xgb_adasyn_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON: SMOTE vs ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SMOTE vs ADASYN COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "print(f\"   SMOTE  Mean AUC: {xgb_smote_cv_mean:.4f} (+/-{xgb_smote_cv_std:.4f})\")\n",
    "print(f\"   ADASYN Mean AUC: {xgb_adasyn_cv_mean:.4f} (+/-{xgb_adasyn_cv_std:.4f})\")\n",
    "\n",
    "# Calculate difference\n",
    "auc_difference = xgb_adasyn_cv_mean - xgb_smote_cv_mean\n",
    "print(f\"\\n   Difference: {abs(auc_difference):.4f} ({'ADASYN better' if auc_difference > 0 else 'SMOTE better'})\")\n",
    "\n",
    "# Determine winner\n",
    "if abs(auc_difference) < 0.001:\n",
    "    winner = \"TIE - Performance essentially identical\"\n",
    "    recommendation = \"Either technique is suitable - choose based on training time\"\n",
    "elif auc_difference > 0:\n",
    "    winner = \"ADASYN WINS\"\n",
    "    recommendation = \"ADASYN provides better fraud detection for this dataset\"\n",
    "else:\n",
    "    winner = \"SMOTE WINS\"\n",
    "    recommendation = \"SMOTE provides better fraud detection for this dataset\"\n",
    "\n",
    "print(f\"\\n Winner: {winner}\")\n",
    "print(f\"\\n Recommendation: {recommendation}\")\n",
    "\n",
    "print(f\"\\n Training Time Comparison:\")\n",
    "print(f\"   SMOTE:  {cv_time_smote:.2f} seconds ({cv_time_smote/60:.1f} minutes)\")\n",
    "print(f\"   ADASYN: {cv_time_adasyn:.2f} seconds ({cv_time_adasyn/60:.1f} minutes)\")\n",
    "print(f\"   Difference: {abs(cv_time_smote - cv_time_adasyn):.2f} seconds ({'ADASYN slower' if cv_time_adasyn > cv_time_smote else 'SMOTE slower'})\")\n",
    "\n",
    "print(f\"\\n Stability Comparison:\")\n",
    "print(f\"   SMOTE  Std: {xgb_smote_cv_std:.4f} ({'More stable' if xgb_smote_cv_std < xgb_adasyn_cv_std else 'Less stable'})\")\n",
    "print(f\"   ADASYN Std: {xgb_adasyn_cv_std:.4f} ({'More stable' if xgb_adasyn_cv_std < xgb_smote_cv_std else 'Less stable'})\")\n",
    "\n",
    "# Statistical significance test\n",
    "print(f\"\\n Statistical Significance:\")\n",
    "smote_lower = xgb_smote_cv_mean - confidence_interval_smote\n",
    "smote_upper = xgb_smote_cv_mean + confidence_interval_smote\n",
    "adasyn_lower = xgb_adasyn_cv_mean - confidence_interval_adasyn\n",
    "adasyn_upper = xgb_adasyn_cv_mean + confidence_interval_adasyn\n",
    "\n",
    "if (smote_lower <= adasyn_upper) and (adasyn_lower <= smote_upper):\n",
    "    print(f\"   Confidence intervals overlap\")\n",
    "    print(f\"   Difference may not be statistically significant\")\n",
    "    print(f\"   Both methods perform similarly on this dataset\")\n",
    "else:\n",
    "    print(f\"   + Confidence intervals don't overlap\")\n",
    "    print(f\"   Difference is likely statistically significant\")\n",
    "    print(f\"   {winner} is meaningfully better\")\n",
    "\n",
    "# Visualization of comparison\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Visual Comparison\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Mean AUC Scores:\")\n",
    "print(f\"   SMOTE:  {'█' * int(xgb_smote_cv_mean * 50)} {xgb_smote_cv_mean:.4f}\")\n",
    "print(f\"   ADASYN: {'█' * int(xgb_adasyn_cv_mean * 50)} {xgb_adasyn_cv_mean:.4f}\")\n",
    "\n",
    "print(f\"\\n Consistency (Lower is Better):\")\n",
    "print(f\"   SMOTE:  {'█' * int(xgb_smote_cv_std * 1000)} {xgb_smote_cv_std:.4f}\")\n",
    "print(f\"   ADASYN: {'█' * int(xgb_adasyn_cv_std * 1000)} {xgb_adasyn_cv_std:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT BEST MODEL FOR FINAL USE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SELECTING BEST RESAMPLING TECHNIQUE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Choose best based on mean AUC (with tie-breaker on stability)\n",
    "if abs(auc_difference) < 0.001:\n",
    "    # Performance is essentially the same, choose more stable\n",
    "    if xgb_smote_cv_std < xgb_adasyn_cv_std:\n",
    "        best_pipeline = xgb_smote_pipeline\n",
    "        best_method = \"SMOTE\"\n",
    "        best_cv_mean = xgb_smote_cv_mean\n",
    "        best_cv_std = xgb_smote_cv_std\n",
    "        best_cv_scores = cv_scores_smote\n",
    "        reason = \"Similar performance, but SMOTE is more stable\"\n",
    "    else:\n",
    "        best_pipeline = xgb_adasyn_pipeline\n",
    "        best_method = \"ADASYN\"\n",
    "        best_cv_mean = xgb_adasyn_cv_mean\n",
    "        best_cv_std = xgb_adasyn_cv_std\n",
    "        best_cv_scores = cv_scores_adasyn\n",
    "        reason = \"Similar performance, but ADASYN is more stable\"\n",
    "else:\n",
    "    # Clear winner based on AUC\n",
    "    if xgb_smote_cv_mean > xgb_adasyn_cv_mean:\n",
    "        best_pipeline = xgb_smote_pipeline\n",
    "        best_method = \"SMOTE\"\n",
    "        best_cv_mean = xgb_smote_cv_mean\n",
    "        best_cv_std = xgb_smote_cv_std\n",
    "        best_cv_scores = cv_scores_smote\n",
    "        reason = f\"Higher mean AUC ({xgb_smote_cv_mean:.4f} vs {xgb_adasyn_cv_mean:.4f})\"\n",
    "    else:\n",
    "        best_pipeline = xgb_adasyn_pipeline\n",
    "        best_method = \"ADASYN\"\n",
    "        best_cv_mean = xgb_adasyn_cv_mean\n",
    "        best_cv_std = xgb_adasyn_cv_std\n",
    "        best_cv_scores = cv_scores_adasyn\n",
    "        reason = f\"Higher mean AUC ({xgb_adasyn_cv_mean:.4f} vs {xgb_smote_cv_mean:.4f})\"\n",
    "\n",
    "print(f\"\\n Selected Method: {best_method}\")\n",
    "print(f\"   Reason: {reason}\")\n",
    "print(f\"   Performance: {best_cv_mean:.4f} +/- {best_cv_std:.4f}\")\n",
    "\n",
    "print(f\"\\n Storing best model for later steps:\")\n",
    "print(f\"   Model: XGBoost + {best_method}\")\n",
    "print(f\"   Expected AUC: {best_cv_mean:.4f}\")\n",
    "\n",
    "# Store the best results for comparison in Step 21\n",
    "xgb_final_pipeline = best_pipeline\n",
    "xgb_cv_mean = best_cv_mean\n",
    "xgb_cv_std = best_cv_std\n",
    "xgb_cv_scores = best_cv_scores\n",
    "xgb_best_method = best_method\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETER TUNING FOR BEST MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n WHAT WE'RE DOING:\")\n",
    "print(f\"   - Fine-tuning the best model ({best_method} + XGBoost)\")\n",
    "print(f\"   - Testing different hyperparameter combinations\")\n",
    "print(f\"   - Using GridSearchCV with cross-validation\")\n",
    "print(f\"   - Goal: Improve beyond baseline {best_cv_mean:.4f} AUC\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Define Hyperparameter Grid\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Defining Hyperparameter Search Space\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Hyperparameters to tune:\")\n",
    "print(\"   1. n_estimators (Number of boosting rounds)\")\n",
    "print(\"      - Number of sequential trees to build\")\n",
    "print(\"      - More trees = more learning iterations\")\n",
    "print(\"      - Trade-off: accuracy vs training time\")\n",
    "print(\"\\n   2. max_depth (Maximum tree depth)\")\n",
    "print(\"      - Controls how deep each tree can grow\")\n",
    "print(\"      - Deeper trees = more complex patterns\")\n",
    "print(\"      - Trade-off: complexity vs overfitting\")\n",
    "print(\"\\n   3. learning_rate (Step size for each tree)\")\n",
    "print(\"      - How much each tree contributes to final prediction\")\n",
    "print(\"      - Smaller rate = more conservative learning\")\n",
    "print(\"      - Trade-off: accuracy vs training time\")\n",
    "print(\"\\n   4. subsample (Fraction of samples per tree)\")\n",
    "print(\"      - Percentage of data used to train each tree\")\n",
    "print(\"      - Lower values = more randomness, less overfitting\")\n",
    "print(\"      - Trade-off: generalization vs training stability\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'classifier__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(\"\\n Parameter Grid:\")\n",
    "print(f\"   n_estimators: {param_grid['classifier__n_estimators']}\")\n",
    "print(f\"   max_depth: {param_grid['classifier__max_depth']}\")\n",
    "print(f\"   learning_rate: {param_grid['classifier__learning_rate']}\")\n",
    "print(f\"   subsample: {param_grid['classifier__subsample']}\")\n",
    "print(f\"\\n   Total combinations: {len(param_grid['classifier__n_estimators']) * len(param_grid['classifier__max_depth']) * len(param_grid['classifier__learning_rate']) * len(param_grid['classifier__subsample'])}\")\n",
    "print(f\"   With 5-fold CV: {len(param_grid['classifier__n_estimators']) * len(param_grid['classifier__max_depth']) * len(param_grid['classifier__learning_rate']) * len(param_grid['classifier__subsample']) * 5} model fits\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Run Grid Search\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running Grid Search with Cross-Validation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n This will take 10-15 minutes...\")\n",
    "print(\"   Each combination is tested with 5-fold CV\")\n",
    "print(\"   XGBoost is computationally intensive\")\n",
    "print(\"   Progress will be shown below\")\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_final_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Run grid search\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Display Results\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Grid Search Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Grid search complete in {grid_time:.2f} seconds ({grid_time/60:.1f} minutes)\")\n",
    "\n",
    "print(f\"\\n Best Parameters Found:\")\n",
    "for param_name, param_value in grid_search.best_params_.items():\n",
    "    # Remove 'classifier__' prefix for cleaner display\n",
    "    clean_name = param_name.replace('classifier__', '')\n",
    "    print(f\"   {clean_name}: {param_value}\")\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "print(f\"   Baseline AUC:  {best_cv_mean:.4f} (+/-{best_cv_std:.4f})\")\n",
    "print(f\"   Tuned AUC:     {grid_search.best_score_:.4f} (+/-{grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.4f})\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = grid_search.best_score_ - best_cv_mean\n",
    "improvement_pct = (improvement / best_cv_mean) * 100\n",
    "\n",
    "print(f\"\\n Improvement:\")\n",
    "if improvement > 0:\n",
    "    print(f\"   +{improvement:.4f} AUC ({improvement_pct:+.2f}%)\")\n",
    "    print(f\"   + Hyperparameter tuning improved performance!\")\n",
    "elif improvement < -0.001:\n",
    "    print(f\"   {improvement:.4f} AUC ({improvement_pct:.2f}%)\")\n",
    "    print(f\"   Tuned model performed slightly worse\")\n",
    "    print(f\"   -> Baseline model was already well-optimized\")\n",
    "else:\n",
    "    print(f\"   ~{improvement:.4f} AUC (essentially no change)\")\n",
    "    print(f\"   -> Baseline hyperparameters were already optimal\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Top 5 Parameter Combinations\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Top 5 Hyperparameter Combinations\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Get results sorted by score\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df = results_df.sort_values('rank_test_score')\n",
    "\n",
    "print(\"\\n Best performing combinations:\\n\")\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"   Rank {i+1}:\")\n",
    "    print(f\"      n_estimators={row['param_classifier__n_estimators']}, \"\n",
    "          f\"max_depth={row['param_classifier__max_depth']}, \"\n",
    "          f\"learning_rate={row['param_classifier__learning_rate']}, \"\n",
    "          f\"subsample={row['param_classifier__subsample']}\")\n",
    "    print(f\"      Mean AUC: {row['mean_test_score']:.4f} (+/-{row['std_test_score']:.4f})\")\n",
    "    print()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Store Tuned Model\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Storing Tuned Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Store the tuned model\n",
    "xgb_tuned_pipeline = grid_search.best_estimator_\n",
    "xgb_tuned_cv_mean = grid_search.best_score_\n",
    "xgb_tuned_cv_std = grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "xgb_best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"\\n+ Tuned model stored for final evaluation\")\n",
    "print(f\"   Best hyperparameters saved\")\n",
    "print(f\"   Expected AUC: {xgb_tuned_cv_mean:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Decision: Use Baseline or Tuned?\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Model Selection Decision\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Decide whether to use tuned or baseline model\n",
    "if improvement > 0.0001:  # Meaningful improvement\n",
    "    print(f\"\\n+ Using TUNED model for final evaluation\")\n",
    "    print(f\"   Reason: Tuning improved performance by {improvement:.4f}\")\n",
    "    xgb_final_model = xgb_tuned_pipeline\n",
    "    xgb_final_cv_mean = xgb_tuned_cv_mean\n",
    "    xgb_final_cv_std = xgb_tuned_cv_std\n",
    "    model_version = \"Tuned\"\n",
    "else:\n",
    "    print(f\"\\n+ Using BASELINE model for final evaluation\")\n",
    "    print(f\"   Reason: Tuning did not provide meaningful improvement\")\n",
    "    print(f\"   Baseline model was already well-optimized\")\n",
    "    xgb_final_model = xgb_final_pipeline\n",
    "    xgb_final_cv_mean = best_cv_mean\n",
    "    xgb_final_cv_std = best_cv_std\n",
    "    model_version = \"Baseline\"\n",
    "\n",
    "print(f\"\\n Final XGBoost Model:\")\n",
    "print(f\"   Version: {model_version}\")\n",
    "print(f\"   Resampling: {best_method}\")\n",
    "print(f\"   Expected AUC: {xgb_final_cv_mean:.4f} (+/-{xgb_final_cv_std:.4f})\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE MODEL AND RESULTS TO DISK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL AND RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Saving to ../models/ directory...\")\n",
    "\n",
    "# Save the final model\n",
    "model_path = '../models/xgboost_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(xgb_final_model, f)\n",
    "print(f\"   + Model saved: {model_path}\")\n",
    "\n",
    "# Save all results and metadata\n",
    "results_data = {\n",
    "    'model_name': 'XGBoost',\n",
    "    'resampling_method': best_method,\n",
    "    'model_version': model_version,\n",
    "    'cv_mean': xgb_final_cv_mean,\n",
    "    'cv_std': xgb_final_cv_std,\n",
    "    'cv_scores': xgb_cv_scores,\n",
    "    'best_params': xgb_best_params if model_version == \"Tuned\" else None,\n",
    "    'training_samples': total_samples,\n",
    "    'training_frauds': total_frauds,\n",
    "    'timestamp': pd.Timestamp.now()\n",
    "}\n",
    "\n",
    "results_path = '../models/xgboost_results.pkl'\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(results_data, f)\n",
    "print(f\"   + Results saved: {results_path}\")\n",
    "\n",
    "print(f\"\\n+ All files saved successfully!\")\n",
    "print(f\"   Model can now be loaded in Phase 5 for comparison\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 19 COMPLETE - KEY ACCOMPLISHMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n What We Accomplished:\")\n",
    "print(\"   + Built XGBoost classifier (gradient boosting)\")\n",
    "print(\"   + Compared SMOTE vs ADASYN performance\")\n",
    "print(\"   + Used proper CV methodology (resampling inside folds)\")\n",
    "print(\"   + Validated on 100% real data (no synthetic contamination)\")\n",
    "print(\"   + Selected best resampling technique based on performance\")\n",
    "print(\"   + Performed hyperparameter tuning with GridSearchCV\")\n",
    "print(\"   + Trained final optimized model on all available training data\")\n",
    "print(\"   + SAVED model and results to disk\")\n",
    "\n",
    "print(f\"\\n Final Results:\")\n",
    "print(f\"   Best Method: {best_method}\")\n",
    "print(f\"   Model Version: {model_version}\")\n",
    "print(f\"   Cross-Validation AUC: {xgb_final_cv_mean:.4f} (+/-{xgb_final_cv_std:.4f})\")\n",
    "print(f\"   Model Stability: {'Excellent' if xgb_final_cv_std < 0.01 else 'Good' if xgb_final_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "print(\"\\n Saved Files:\")\n",
    "print(f\"   - {model_path}\")\n",
    "print(f\"   - {results_path}\")\n",
    "\n",
    "print(\"\\n Next Steps:\")\n",
    "print(\"   -> Step 20: Build Neural Network (Deep Learning with GPU)\")\n",
    "print(\"   -> Step 21: Compare all models and select winner\")\n",
    "\n",
    "print(\"\\n Important Notes:\")\n",
    "print(\"   - Test set remains UNTOUCHED for final evaluation in Phase 5\")\n",
    "print(\"   - Model saved and can be loaded anytime (no need to retrain)\")\n",
    "print(\"   - Can restart PC without losing progress\")\n",
    "print(f\"   - {best_method} + {model_version} model ready for Phase 5\")\n",
    "print(f\"   - All models trained with consistent methodology for fair comparison\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccdd3e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL 4: NEURAL NETWORK (DEEP LEARNING)\n",
      "======================================================================\n",
      "\n",
      " WHAT WE'RE DOING:\n",
      "   - Building Neural Network classifier (deep learning)\n",
      "   - Testing TWO resampling techniques: SMOTE vs ADASYN\n",
      "   - Using manual 5-fold cross-validation (PyTorch requirement)\n",
      "   - Applying resampling INSIDE each CV fold (proper methodology)\n",
      "   - Validating on REAL data only (no synthetic frauds in validation)\n",
      "   - Leveraging GPU acceleration for faster training\n",
      "   - SAVING model and results to disk for later use\n",
      "   - TRACKING training history for visualization\n",
      "\n",
      "======================================================================\n",
      "ABOUT NEURAL NETWORKS\n",
      "======================================================================\n",
      "\n",
      " What is a Neural Network?\n",
      "   - Deep learning model inspired by human brain\n",
      "   - Multiple layers of interconnected 'neurons'\n",
      "   - Each layer learns increasingly complex patterns\n",
      "   - Can discover non-linear relationships in data\n",
      "   - State-of-the-art for many complex pattern recognition tasks\n",
      "\n",
      " How Neural Networks Differ from Other Models:\n",
      "   Logistic Regression:\n",
      "      - Linear decision boundary (straight line)\n",
      "      - Fast and interpretable\n",
      "      - Limited to simple patterns\n",
      "\n",
      "   Random Forest:\n",
      "      - Parallel decision trees voting\n",
      "      - Good for tabular data\n",
      "      - Non-linear but limited complexity\n",
      "\n",
      "   XGBoost:\n",
      "      - Sequential trees learning from mistakes\n",
      "      - Excellent for structured data\n",
      "      - Non-linear with regularization\n",
      "\n",
      "   Neural Network:\n",
      "      - Multiple layers transforming data\n",
      "      - Can learn VERY complex patterns\n",
      "      - Requires more data and computational power\n",
      "      - Most flexible but hardest to interpret\n",
      "\n",
      " Why Neural Networks for Fraud Detection?\n",
      "   + Large dataset (284K+ transactions) - NN needs lots of data\n",
      "   + Complex fraud patterns that may be non-obvious\n",
      "   + Can learn hierarchical features automatically\n",
      "   + GPU acceleration available (your RTX 5070 Ti)\n",
      "   + Industry-standard for advanced fraud detection\n",
      "   + Shows portfolio depth beyond traditional ML\n",
      "\n",
      "======================================================================\n",
      "GPU CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      " GPU Status:\n",
      "   + GPU Available: YES\n",
      "   GPU Device: NVIDIA GeForce RTX 5070 Ti\n",
      "   Training will be ACCELERATED!\n",
      "   RTX 5070 Ti will significantly speed up training\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION\n",
      "======================================================================\n",
      "\n",
      " Training Data Overview:\n",
      "   Total samples: 226,980\n",
      "   Normal transactions: 226,602 (99.833%)\n",
      "   Fraud transactions: 378 (0.167%)\n",
      "   Class imbalance ratio: 599.5:1\n",
      "\n",
      " Resampling Strategy:\n",
      "   - Testing both SMOTE and ADASYN\n",
      "   - Resampling applied within each CV fold\n",
      "   - Validation always on 100% real data (no synthetic)\n",
      "\n",
      "======================================================================\n",
      "NEURAL NETWORK ARCHITECTURE\n",
      "======================================================================\n",
      "\n",
      " Network Structure:\n",
      "   Layer 1 (Input):  128 neurons + Dropout(0.3) + BatchNorm\n",
      "   Layer 2 (Hidden): 64 neurons  + Dropout(0.3) + BatchNorm\n",
      "   Layer 3 (Hidden): 32 neurons  + Dropout(0.2)\n",
      "   Layer 4 (Output): 1 neuron    (fraud probability)\n",
      "\n",
      "   Total: 4 layers (deep network)\n",
      "   Activation: ReLU (hidden layers), Sigmoid (output)\n",
      "   Optimizer: Adam (adaptive learning rate)\n",
      "   Loss: Binary Cross Entropy (for binary classification)\n",
      "\n",
      "+ Neural network architecture defined\n",
      "\n",
      "======================================================================\n",
      "PART A: NEURAL NETWORK + SMOTE\n",
      "======================================================================\n",
      "\n",
      " About SMOTE:\n",
      "   - Synthetic Minority Over-sampling Technique\n",
      "   - Creates synthetic samples by interpolating between existing frauds\n",
      "   - Places new samples along line segments between neighbors\n",
      "   - Good for general class imbalance\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running 5-Fold Cross-Validation with SMOTE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Running manual 5-fold cross-validation...\n",
      "   Why manual? Neural networks need to be rebuilt for each fold\n",
      "   sklearn's cross_val_score doesn't work with PyTorch\n",
      "   Dataset: X_train_scaled (226,980 samples, 378 frauds)\n",
      "   Each fold: ~45,396 samples (~75 real frauds)\n",
      "   This will take 10-15 minutes (depending on GPU)\n",
      "\n",
      "    Training fold 1/5...\n",
      "      Fold 1 AUC: 0.9719\n",
      "      Stopped at epoch: 1\n",
      "\n",
      "    Training fold 2/5...\n",
      "      Fold 2 AUC: 0.9942\n",
      "      Stopped at epoch: 10\n",
      "\n",
      "    Training fold 3/5...\n",
      "      Fold 3 AUC: 0.9742\n",
      "      Stopped at epoch: 5\n",
      "\n",
      "    Training fold 4/5...\n",
      "      Fold 4 AUC: 0.9900\n",
      "      Stopped at epoch: 1\n",
      "\n",
      "    Training fold 5/5...\n",
      "      Fold 5 AUC: 0.9508\n",
      "      Stopped at epoch: 1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SMOTE Cross-Validation Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Cross-validation complete in 768.69 seconds (12.8 minutes)\n",
      "\n",
      " Individual Fold Scores (SMOTE):\n",
      "   Fold 1: AUC = 0.9719\n",
      "   Fold 2: AUC = 0.9942\n",
      "   Fold 3: AUC = 0.9742\n",
      "   Fold 4: AUC = 0.9900\n",
      "   Fold 5: AUC = 0.9508\n",
      "\n",
      " SMOTE Summary Statistics:\n",
      "   Mean CV AUC:  0.9762\n",
      "   Std CV AUC:   0.0154\n",
      "   Min CV AUC:   0.9508\n",
      "   Max CV AUC:   0.9942\n",
      "\n",
      " 95% Confidence Interval:\n",
      "   0.9762 +/- 0.0302\n",
      "   Range: [0.9460, 1.0064]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Final SMOTE Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Training final neural network with SMOTE on all training data...\n",
      "   This model will be used for final evaluation\n",
      "   Training for up to 50 epochs with early stopping\n",
      "   Tracking training history for visualization\n",
      "   This will take 3-5 minutes\n",
      "\n",
      "+ SMOTE model training complete in 376.66 seconds (6.3 minutes)\n",
      "   Stopped at epoch: 9\n",
      "   Training history captured: 19 epochs\n",
      "\n",
      " SMOTE Model Summary:\n",
      "   Expected AUC: 0.9762 +/- 0.0154\n",
      "   Stability: Good\n",
      "\n",
      "======================================================================\n",
      "PART B: NEURAL NETWORK + ADASYN\n",
      "======================================================================\n",
      "\n",
      " About ADASYN:\n",
      "   - Adaptive Synthetic Sampling\n",
      "   - Focuses on harder-to-learn minority samples\n",
      "   - Creates MORE synthetics near decision boundary (harder cases)\n",
      "   - Creates FEWER synthetics in easy regions\n",
      "   - Better for complex, varied fraud patterns\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running 5-Fold Cross-Validation with ADASYN\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Running manual 5-fold cross-validation...\n",
      "   Dataset: X_train_scaled (226,980 samples, 378 frauds)\n",
      "   ADASYN will adaptively generate synthetics based on difficulty\n",
      "   This will take 10-15 minutes (depending on GPU)\n",
      "\n",
      "    Training fold 1/5...\n",
      "      Fold 1 AUC: 0.9707\n",
      "      Stopped at epoch: 4\n",
      "\n",
      "    Training fold 2/5...\n",
      "      Fold 2 AUC: 0.9920\n",
      "      Stopped at epoch: 2\n",
      "\n",
      "    Training fold 3/5...\n",
      "      Fold 3 AUC: 0.9732\n",
      "      Stopped at epoch: 9\n",
      "\n",
      "    Training fold 4/5...\n",
      "      Fold 4 AUC: 0.9866\n",
      "      Stopped at epoch: 1\n",
      "\n",
      "    Training fold 5/5...\n",
      "      Fold 5 AUC: 0.9483\n",
      "      Stopped at epoch: 1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ADASYN Cross-Validation Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Cross-validation complete in 747.33 seconds (12.5 minutes)\n",
      "\n",
      " Individual Fold Scores (ADASYN):\n",
      "   Fold 1: AUC = 0.9707\n",
      "   Fold 2: AUC = 0.9920\n",
      "   Fold 3: AUC = 0.9732\n",
      "   Fold 4: AUC = 0.9866\n",
      "   Fold 5: AUC = 0.9483\n",
      "\n",
      " ADASYN Summary Statistics:\n",
      "   Mean CV AUC:  0.9742\n",
      "   Std CV AUC:   0.0152\n",
      "   Min CV AUC:   0.9483\n",
      "   Max CV AUC:   0.9920\n",
      "\n",
      " 95% Confidence Interval:\n",
      "   0.9742 +/- 0.0298\n",
      "   Range: [0.9444, 1.0040]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Final ADASYN Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Training final neural network with ADASYN on all training data...\n",
      "   This model will be used for final evaluation\n",
      "   Training for up to 50 epochs with early stopping\n",
      "   Tracking training history for visualization\n",
      "   This will take 3-5 minutes\n",
      "\n",
      "+ ADASYN model training complete in 873.23 seconds (14.6 minutes)\n",
      "   Stopped at epoch: 34\n",
      "   Training history captured: 44 epochs\n",
      "\n",
      " ADASYN Model Summary:\n",
      "   Expected AUC: 0.9742 +/- 0.0152\n",
      "   Stability: Good\n",
      "\n",
      "======================================================================\n",
      "SMOTE vs ADASYN COMPARISON\n",
      "======================================================================\n",
      "\n",
      " Performance Comparison:\n",
      "   SMOTE  Mean AUC: 0.9762 (+/-0.0154)\n",
      "   ADASYN Mean AUC: 0.9742 (+/-0.0152)\n",
      "\n",
      "   Difference: 0.0020 (SMOTE better)\n",
      "\n",
      " Winner: SMOTE WINS\n",
      "\n",
      " Recommendation: SMOTE provides better fraud detection for this dataset\n",
      "\n",
      " Training Time Comparison:\n",
      "   SMOTE:  768.69 seconds (12.8 minutes)\n",
      "   ADASYN: 747.33 seconds (12.5 minutes)\n",
      "   Difference: 21.37 seconds (SMOTE slower)\n",
      "\n",
      " Stability Comparison:\n",
      "   SMOTE  Std: 0.0154 (Less stable)\n",
      "   ADASYN Std: 0.0152 (More stable)\n",
      "\n",
      " Statistical Significance:\n",
      "   Confidence intervals overlap\n",
      "   Difference may not be statistically significant\n",
      "   Both methods perform similarly on this dataset\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Visual Comparison\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Mean AUC Scores:\n",
      "   SMOTE:  ████████████████████████████████████████████████ 0.9762\n",
      "   ADASYN: ████████████████████████████████████████████████ 0.9742\n",
      "\n",
      " Consistency (Lower is Better):\n",
      "   SMOTE:  ███████████████ 0.0154\n",
      "   ADASYN: ███████████████ 0.0152\n",
      "\n",
      "======================================================================\n",
      "SELECTING BEST RESAMPLING TECHNIQUE\n",
      "======================================================================\n",
      "\n",
      "+ Selected Method: SMOTE\n",
      "   Reason: Higher mean AUC (0.9762 vs 0.9742)\n",
      "   Performance: 0.9762 +/- 0.0154\n",
      "\n",
      " Storing best model for later steps:\n",
      "   Model: Neural Network + SMOTE\n",
      "   Expected AUC: 0.9762\n",
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING - NEURAL NETWORK\n",
      "======================================================================\n",
      "\n",
      " WHAT WE'RE DOING:\n",
      "   - Fine-tuning the best model (SMOTE + Neural Network)\n",
      "   - Testing different hyperparameter combinations\n",
      "   - Using manual grid search with 5-fold cross-validation\n",
      "   - Goal: Improve beyond baseline 0.9762 AUC\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Defining Hyperparameter Search Space\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Hyperparameters to tune:\n",
      "   1. learning_rate (Learning speed)\n",
      "      - How fast the model adjusts weights\n",
      "      - 0.001: Slow, careful learning (more precise)\n",
      "      - 0.01: Fast learning (quicker but might overshoot)\n",
      "\n",
      "   2. dropout (Regularization strength)\n",
      "      - Percentage of neurons randomly dropped during training\n",
      "      - 0.2: Less aggressive (20% dropout)\n",
      "      - 0.3: More aggressive (30% dropout, current baseline)\n",
      "\n",
      "   3. batch_size (Training batch size)\n",
      "      - Number of samples processed before updating weights\n",
      "      - 32: Smaller batches, noisier updates (current)\n",
      "      - 64: Larger batches, more stable updates\n",
      "\n",
      "   4. hidden_units (Network capacity)\n",
      "      - Number of neurons in first layer\n",
      "      - 64: Smaller network (faster, less overfitting)\n",
      "      - 128: Larger network (more capacity, current baseline)\n",
      "\n",
      " Parameter Grid:\n",
      "   learning_rate: [0.001, 0.01]\n",
      "   dropout: [0.2, 0.3]\n",
      "   batch_size: [32, 64]\n",
      "   hidden_units: [64, 128]\n",
      "\n",
      "   Total combinations: 16\n",
      "   With 5-fold CV: 80 model fits\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Running Grid Search with Cross-Validation\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " This will take 60-80 minutes...\n",
      "   Each combination is tested with 5-fold CV\n",
      "   Neural networks train sequentially (cannot parallelize on Windows)\n",
      "   Progress will be shown below\n",
      "\n",
      "  Testing combination 1/16:\n",
      "    learning_rate=0.001, dropout=0.2, batch_size=32, hidden_units=64\n",
      "    Mean AUC: 0.9754 (+/-0.0138)\n",
      "\n",
      "  Testing combination 2/16:\n",
      "    learning_rate=0.001, dropout=0.2, batch_size=32, hidden_units=128\n",
      "    Mean AUC: 0.9745 (+/-0.0128)\n",
      "\n",
      "  Testing combination 3/16:\n",
      "    learning_rate=0.001, dropout=0.2, batch_size=64, hidden_units=64\n",
      "    Mean AUC: 0.9759 (+/-0.0137)\n",
      "\n",
      "  Testing combination 4/16:\n",
      "    learning_rate=0.001, dropout=0.2, batch_size=64, hidden_units=128\n",
      "    Mean AUC: 0.9784 (+/-0.0110)\n",
      "\n",
      "  Testing combination 5/16:\n",
      "    learning_rate=0.001, dropout=0.3, batch_size=32, hidden_units=64\n",
      "    Mean AUC: 0.9794 (+/-0.0157)\n",
      "\n",
      "  Testing combination 6/16:\n",
      "    learning_rate=0.001, dropout=0.3, batch_size=32, hidden_units=128\n",
      "    Mean AUC: 0.9763 (+/-0.0138)\n",
      "\n",
      "  Testing combination 7/16:\n",
      "    learning_rate=0.001, dropout=0.3, batch_size=64, hidden_units=64\n",
      "    Mean AUC: 0.9793 (+/-0.0143)\n",
      "\n",
      "  Testing combination 8/16:\n",
      "    learning_rate=0.001, dropout=0.3, batch_size=64, hidden_units=128\n",
      "    Mean AUC: 0.9779 (+/-0.0135)\n",
      "\n",
      "  Testing combination 9/16:\n",
      "    learning_rate=0.01, dropout=0.2, batch_size=32, hidden_units=64\n",
      "    Mean AUC: 0.9757 (+/-0.0160)\n",
      "\n",
      "  Testing combination 10/16:\n",
      "    learning_rate=0.01, dropout=0.2, batch_size=32, hidden_units=128\n",
      "    Mean AUC: 0.9775 (+/-0.0146)\n",
      "\n",
      "  Testing combination 11/16:\n",
      "    learning_rate=0.01, dropout=0.2, batch_size=64, hidden_units=64\n",
      "    Mean AUC: 0.9792 (+/-0.0139)\n",
      "\n",
      "  Testing combination 12/16:\n",
      "    learning_rate=0.01, dropout=0.2, batch_size=64, hidden_units=128\n",
      "    Mean AUC: 0.9784 (+/-0.0153)\n",
      "\n",
      "  Testing combination 13/16:\n",
      "    learning_rate=0.01, dropout=0.3, batch_size=32, hidden_units=64\n",
      "    Mean AUC: 0.9777 (+/-0.0155)\n",
      "\n",
      "  Testing combination 14/16:\n",
      "    learning_rate=0.01, dropout=0.3, batch_size=32, hidden_units=128\n",
      "    Mean AUC: 0.9750 (+/-0.0154)\n",
      "\n",
      "  Testing combination 15/16:\n",
      "    learning_rate=0.01, dropout=0.3, batch_size=64, hidden_units=64\n",
      "    Mean AUC: 0.9803 (+/-0.0144)\n",
      "\n",
      "  Testing combination 16/16:\n",
      "    learning_rate=0.01, dropout=0.3, batch_size=64, hidden_units=128\n",
      "    Mean AUC: 0.9789 (+/-0.0161)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Grid Search Results\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Grid search complete in 7700.32 seconds (128.3 minutes)\n",
      "\n",
      " Best Parameters Found:\n",
      "   learning_rate: 0.01\n",
      "   dropout: 0.3\n",
      "   batch_size: 64\n",
      "   hidden_units: 64\n",
      "\n",
      " Performance Comparison:\n",
      "   Baseline AUC:  0.9762 (+/-0.0154)\n",
      "   Tuned AUC:     0.9803 (+/-0.0144)\n",
      "\n",
      " Improvement:\n",
      "   +0.0041 AUC (+0.42%)\n",
      "   + Hyperparameter tuning improved performance!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Top 5 Hyperparameter Combinations\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Best performing combinations:\n",
      "\n",
      "   Rank 1:\n",
      "      learning_rate=0.01, dropout=0.3, batch_size=64, hidden_units=64\n",
      "      Mean AUC: 0.9803 (+/-0.0144)\n",
      "\n",
      "   Rank 2:\n",
      "      learning_rate=0.001, dropout=0.3, batch_size=32, hidden_units=64\n",
      "      Mean AUC: 0.9794 (+/-0.0157)\n",
      "\n",
      "   Rank 3:\n",
      "      learning_rate=0.001, dropout=0.3, batch_size=64, hidden_units=64\n",
      "      Mean AUC: 0.9793 (+/-0.0143)\n",
      "\n",
      "   Rank 4:\n",
      "      learning_rate=0.01, dropout=0.2, batch_size=64, hidden_units=64\n",
      "      Mean AUC: 0.9792 (+/-0.0139)\n",
      "\n",
      "   Rank 5:\n",
      "      learning_rate=0.01, dropout=0.3, batch_size=64, hidden_units=128\n",
      "      Mean AUC: 0.9789 (+/-0.0161)\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training Final Tuned Model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Training final neural network with best hyperparameters...\n",
      "   Tracking training history for visualization\n",
      "   This will take 3-5 minutes\n",
      "\n",
      "+ Tuned model training complete in 532.05 seconds (8.9 minutes)\n",
      "   Stopped at epoch: 47\n",
      "   Training history captured: 50 epochs\n",
      "\n",
      "+ Tuned model stored for final evaluation\n",
      "   Best hyperparameters saved\n",
      "   Expected AUC: 0.9803\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model Selection Decision\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "+ Using TUNED model for final evaluation\n",
      "   Reason: Tuning improved performance by 0.0041\n",
      "\n",
      " Final Neural Network Model:\n",
      "   Version: Tuned\n",
      "   Resampling: SMOTE\n",
      "   Expected AUC: 0.9803 (+/-0.0144)\n",
      "\n",
      "======================================================================\n",
      "SAVING MODEL AND RESULTS\n",
      "======================================================================\n",
      "\n",
      " Saving to ../models/ directory...\n",
      "   + Model saved: ../models/neural_network_model.pth\n",
      "   + Results saved: ../models/neural_network_results.pkl\n",
      "\n",
      "+ All files saved successfully!\n",
      "   Model can now be loaded in Phase 5 for comparison\n",
      "   Training history available for Step 22 visualization\n",
      "\n",
      "======================================================================\n",
      "STEP 20 COMPLETE - KEY ACCOMPLISHMENTS\n",
      "======================================================================\n",
      "\n",
      " What We Accomplished:\n",
      "   + Built 4-layer Neural Network (deep learning)\n",
      "   + Compared SMOTE vs ADASYN performance\n",
      "   + Used manual 5-Fold CV (PyTorch requirement)\n",
      "   + Resampling applied inside each fold (proper methodology)\n",
      "   + Validated on 100% real data (no synthetic contamination)\n",
      "   + Leveraged GPU acceleration for faster training\n",
      "   + Selected best resampling technique based on performance\n",
      "   + Performed hyperparameter tuning with grid search\n",
      "   + Trained final optimized model on all training data\n",
      "   + SAVED model and results to disk\n",
      "   + CAPTURED training history for visualization\n",
      "\n",
      " Final Results:\n",
      "   Best Method: SMOTE\n",
      "   Model Version: Tuned\n",
      "   Cross-Validation AUC: 0.9803 (+/-0.0144)\n",
      "   Model Stability: Good\n",
      "   GPU Acceleration: + Used successfully\n",
      "\n",
      " Saved Files:\n",
      "   - ../models/neural_network_model.pth\n",
      "   - ../models/neural_network_results.pkl\n",
      "\n",
      " Next Steps:\n",
      "   -> Step 21: Compare all models and select winner\n",
      "   -> Step 22: Visualize neural network training curves\n",
      "   -> Final evaluation on test set\n",
      "\n",
      " Important Notes:\n",
      "   - Test set remains UNTOUCHED for final evaluation in Phase 5\n",
      "   - Model saved and can be loaded anytime (no need to retrain)\n",
      "   - Can restart PC without losing progress\n",
      "   - SMOTE + Tuned model ready for Phase 5\n",
      "   - All 4 models trained with consistent methodology for fair comparison\n",
      "   - Training history saved for Step 22 visualization\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 20: BUILD NEURAL NETWORK (DEEP LEARNING) WITH SMOTE AND ADASYN\n",
    "# ============================================================================\n",
    "\n",
    "# Import required libraries for Neural Network and saving\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 4: NEURAL NETWORK (DEEP LEARNING)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n WHAT WE'RE DOING:\")\n",
    "print(\"   - Building Neural Network classifier (deep learning)\")\n",
    "print(\"   - Testing TWO resampling techniques: SMOTE vs ADASYN\")\n",
    "print(\"   - Using manual 5-fold cross-validation (PyTorch requirement)\")\n",
    "print(\"   - Applying resampling INSIDE each CV fold (proper methodology)\")\n",
    "print(\"   - Validating on REAL data only (no synthetic frauds in validation)\")\n",
    "print(\"   - Leveraging GPU acceleration for faster training\")\n",
    "print(\"   - SAVING model and results to disk for later use\")\n",
    "print(\"   - TRACKING training history for visualization\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# UNDERSTANDING NEURAL NETWORKS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABOUT NEURAL NETWORKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n What is a Neural Network?\")\n",
    "print(\"   - Deep learning model inspired by human brain\")\n",
    "print(\"   - Multiple layers of interconnected 'neurons'\")\n",
    "print(\"   - Each layer learns increasingly complex patterns\")\n",
    "print(\"   - Can discover non-linear relationships in data\")\n",
    "print(\"   - State-of-the-art for many complex pattern recognition tasks\")\n",
    "\n",
    "print(\"\\n How Neural Networks Differ from Other Models:\")\n",
    "print(\"   Logistic Regression:\")\n",
    "print(\"      - Linear decision boundary (straight line)\")\n",
    "print(\"      - Fast and interpretable\")\n",
    "print(\"      - Limited to simple patterns\")\n",
    "print(\"\\n   Random Forest:\")\n",
    "print(\"      - Parallel decision trees voting\")\n",
    "print(\"      - Good for tabular data\")\n",
    "print(\"      - Non-linear but limited complexity\")\n",
    "print(\"\\n   XGBoost:\")\n",
    "print(\"      - Sequential trees learning from mistakes\")\n",
    "print(\"      - Excellent for structured data\")\n",
    "print(\"      - Non-linear with regularization\")\n",
    "print(\"\\n   Neural Network:\")\n",
    "print(\"      - Multiple layers transforming data\")\n",
    "print(\"      - Can learn VERY complex patterns\")\n",
    "print(\"      - Requires more data and computational power\")\n",
    "print(\"      - Most flexible but hardest to interpret\")\n",
    "\n",
    "print(\"\\n Why Neural Networks for Fraud Detection?\")\n",
    "print(\"   + Large dataset (284K+ transactions) - NN needs lots of data\")\n",
    "print(\"   + Complex fraud patterns that may be non-obvious\")\n",
    "print(\"   + Can learn hierarchical features automatically\")\n",
    "print(\"   + GPU acceleration available (your RTX 5070 Ti)\")\n",
    "print(\"   + Industry-standard for advanced fraud detection\")\n",
    "print(\"   + Shows portfolio depth beyond traditional ML\")\n",
    "\n",
    "# ============================================================================\n",
    "# GPU CONFIGURATION CHECK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n GPU Status:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   + GPU Available: YES\")\n",
    "    print(f\"   GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Training will be ACCELERATED!\")\n",
    "    print(f\"   RTX 5070 Ti will significantly speed up training\")\n",
    "else:\n",
    "    print(f\"   - GPU Available: NO\")\n",
    "    print(f\"   Training will run on CPU (slower but functional)\")\n",
    "    print(f\"   Consider checking GPU drivers and CUDA installation\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA OVERVIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate dataset info dynamically\n",
    "total_samples = len(X_train_scaled)\n",
    "total_frauds = int(y_train.sum())\n",
    "total_normal = total_samples - total_frauds\n",
    "fraud_percentage = (total_frauds / total_samples) * 100\n",
    "\n",
    "print(f\"\\n Training Data Overview:\")\n",
    "print(f\"   Total samples: {total_samples:,}\")\n",
    "print(f\"   Normal transactions: {total_normal:,} ({100-fraud_percentage:.3f}%)\")\n",
    "print(f\"   Fraud transactions: {total_frauds} ({fraud_percentage:.3f}%)\")\n",
    "print(f\"   Class imbalance ratio: {total_normal/total_frauds:.1f}:1\")\n",
    "\n",
    "print(\"\\n Resampling Strategy:\")\n",
    "print(\"   - Testing both SMOTE and ADASYN\")\n",
    "print(\"   - Resampling applied within each CV fold\")\n",
    "print(\"   - Validation always on 100% real data (no synthetic)\")\n",
    "\n",
    "# ============================================================================\n",
    "# NEURAL NETWORK ARCHITECTURE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEURAL NETWORK ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Network Structure:\")\n",
    "print(\"   Layer 1 (Input):  128 neurons + Dropout(0.3) + BatchNorm\")\n",
    "print(\"   Layer 2 (Hidden): 64 neurons  + Dropout(0.3) + BatchNorm\")\n",
    "print(\"   Layer 3 (Hidden): 32 neurons  + Dropout(0.2)\")\n",
    "print(\"   Layer 4 (Output): 1 neuron    (fraud probability)\")\n",
    "print(\"\\n   Total: 4 layers (deep network)\")\n",
    "print(\"   Activation: ReLU (hidden layers), Sigmoid (output)\")\n",
    "print(\"   Optimizer: Adam (adaptive learning rate)\")\n",
    "print(\"   Loss: Binary Cross Entropy (for binary classification)\")\n",
    "\n",
    "class FraudDetectionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    4-layer Neural Network for fraud detection using PyTorch\n",
    "    \n",
    "    Architecture:\n",
    "    - Input Layer: hidden_units neurons with dropout and batch normalization\n",
    "    - Hidden Layer 1: hidden_units/2 neurons with dropout and batch normalization\n",
    "    - Hidden Layer 2: hidden_units/4 neurons with dropout\n",
    "    - Output Layer: 1 neuron with sigmoid activation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Number of input features (30 in our case)\n",
    "    hidden_units : int\n",
    "        Number of neurons in first hidden layer (default 128)\n",
    "    dropout : float\n",
    "        Dropout rate for regularization (default 0.3)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_units=128, dropout=0.3):\n",
    "        super(FraudDetectionNN, self).__init__()\n",
    "        \n",
    "        # Calculate layer sizes\n",
    "        layer2_units = hidden_units // 2\n",
    "        layer3_units = hidden_units // 4\n",
    "        \n",
    "        # Layer 1: Learn basic patterns\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_units)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_units)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer 2: Combine patterns\n",
    "        self.fc2 = nn.Linear(hidden_units, layer2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(layer2_units)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer 3: Refine patterns\n",
    "        self.fc3 = nn.Linear(layer2_units, layer3_units)\n",
    "        self.dropout3 = nn.Dropout(dropout * 0.67)\n",
    "        \n",
    "        # Layer 4: Final decision\n",
    "        self.fc4 = nn.Linear(layer3_units, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.fc3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Layer 4\n",
    "        x = self.fc4(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_neural_network(model, train_loader, val_loader, epochs=20, patience=5, learning_rate=0.001, device='cpu', track_history=False):\n",
    "    \"\"\"\n",
    "    Train neural network with early stopping\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : FraudDetectionNN\n",
    "        The neural network model\n",
    "    train_loader : DataLoader\n",
    "        Training data loader\n",
    "    val_loader : DataLoader\n",
    "        Validation data loader\n",
    "    epochs : int\n",
    "        Maximum number of epochs\n",
    "    patience : int\n",
    "        Early stopping patience\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "    device : str\n",
    "        Device to train on ('cuda' or 'cpu')\n",
    "    track_history : bool\n",
    "        Whether to track training history (loss, AUC per epoch)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : FraudDetectionNN\n",
    "        Trained model\n",
    "    best_val_auc : float\n",
    "        Best validation AUC achieved\n",
    "    stopped_epoch : int\n",
    "        Epoch where training stopped\n",
    "    history : dict (if track_history=True)\n",
    "        Training history with epoch, train_loss, val_auc\n",
    "    \"\"\"\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_auc = 0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    stopped_epoch = 0\n",
    "    \n",
    "    # Training history tracking\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_auc': []\n",
    "    } if track_history else None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if track_history:\n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(y_batch.numpy())\n",
    "        \n",
    "        # Calculate validation AUC\n",
    "        val_auc = roc_auc_score(val_labels, val_predictions)\n",
    "        \n",
    "        # Track history if requested\n",
    "        if track_history:\n",
    "            history['epoch'].append(epoch + 1)\n",
    "            history['train_loss'].append(epoch_loss / batch_count)\n",
    "            history['val_auc'].append(val_auc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            stopped_epoch = epoch + 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            # Restore best weights\n",
    "            model.load_state_dict(best_model_state)\n",
    "            stopped_epoch = epoch + 1 - patience\n",
    "            break\n",
    "    \n",
    "    if track_history:\n",
    "        return model, best_val_auc, stopped_epoch, history\n",
    "    else:\n",
    "        return model, best_val_auc, stopped_epoch\n",
    "\n",
    "print(\"\\n+ Neural network architecture defined\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART A: NEURAL NETWORK WITH SMOTE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART A: NEURAL NETWORK + SMOTE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n About SMOTE:\")\n",
    "print(\"   - Synthetic Minority Over-sampling Technique\")\n",
    "print(\"   - Creates synthetic samples by interpolating between existing frauds\")\n",
    "print(\"   - Places new samples along line segments between neighbors\")\n",
    "print(\"   - Good for general class imbalance\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Manual K-Fold Cross-Validation with SMOTE\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running 5-Fold Cross-Validation with SMOTE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n Running manual 5-fold cross-validation...\")\n",
    "print(f\"   Why manual? Neural networks need to be rebuilt for each fold\")\n",
    "print(f\"   sklearn's cross_val_score doesn't work with PyTorch\")\n",
    "print(f\"   Dataset: X_train_scaled ({total_samples:,} samples, {total_frauds} frauds)\")\n",
    "print(f\"   Each fold: ~{total_samples // 5:,} samples (~{total_frauds // 5} real frauds)\")\n",
    "print(f\"   This will take 10-15 minutes (depending on GPU)\")\n",
    "\n",
    "# Initialize K-Fold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_smote = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_scaled), 1):\n",
    "    print(f\"\\n    Training fold {fold}/5...\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    X_fold_train = X_train_scaled.iloc[train_idx] if isinstance(X_train_scaled, pd.DataFrame) else X_train_scaled[train_idx]\n",
    "    X_fold_val = X_train_scaled.iloc[val_idx] if isinstance(X_train_scaled, pd.DataFrame) else X_train_scaled[val_idx]\n",
    "    y_fold_train = y_train.iloc[train_idx] if isinstance(y_train, pd.Series) else y_train[train_idx]\n",
    "    y_fold_val = y_train.iloc[val_idx] if isinstance(y_train, pd.Series) else y_train[val_idx]\n",
    "    \n",
    "    # Apply SMOTE to training fold ONLY (not validation)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_fold_train_resampled, y_fold_train_resampled = smote.fit_resample(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_fold_train_resampled.values if isinstance(X_fold_train_resampled, pd.DataFrame) else X_fold_train_resampled)\n",
    "    y_train_tensor = torch.FloatTensor(y_fold_train_resampled.values if isinstance(y_fold_train_resampled, pd.Series) else y_fold_train_resampled).reshape(-1, 1)\n",
    "    X_val_tensor = torch.FloatTensor(X_fold_val.values if isinstance(X_fold_val, pd.DataFrame) else X_fold_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_fold_val.values if isinstance(y_fold_val, pd.Series) else y_fold_val).reshape(-1, 1)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Create fresh model for this fold\n",
    "    model = FraudDetectionNN(X_train_scaled.shape[1], hidden_units=128, dropout=0.3).to(device)\n",
    "    \n",
    "    # Train model (no history tracking for CV folds)\n",
    "    model, fold_auc, stopped_epoch = train_neural_network(\n",
    "        model, train_loader, val_loader, \n",
    "        epochs=20, patience=5, learning_rate=0.001, device=device, track_history=False\n",
    "    )\n",
    "    \n",
    "    cv_scores_smote.append(fold_auc)\n",
    "    \n",
    "    print(f\"      Fold {fold} AUC: {fold_auc:.4f}\")\n",
    "    print(f\"      Stopped at epoch: {stopped_epoch}\")\n",
    "\n",
    "cv_time_smote = time.time() - start_time\n",
    "cv_scores_smote = np.array(cv_scores_smote)\n",
    "\n",
    "# Display SMOTE Cross-Validation Results\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"SMOTE Cross-Validation Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Cross-validation complete in {cv_time_smote:.2f} seconds ({cv_time_smote/60:.1f} minutes)\")\n",
    "\n",
    "print(f\"\\n Individual Fold Scores (SMOTE):\")\n",
    "for i, score in enumerate(cv_scores_smote, 1):\n",
    "    print(f\"   Fold {i}: AUC = {score:.4f}\")\n",
    "\n",
    "print(f\"\\n SMOTE Summary Statistics:\")\n",
    "print(f\"   Mean CV AUC:  {cv_scores_smote.mean():.4f}\")\n",
    "print(f\"   Std CV AUC:   {cv_scores_smote.std():.4f}\")\n",
    "print(f\"   Min CV AUC:   {cv_scores_smote.min():.4f}\")\n",
    "print(f\"   Max CV AUC:   {cv_scores_smote.max():.4f}\")\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval_smote = 1.96 * cv_scores_smote.std()\n",
    "print(f\"\\n 95% Confidence Interval:\")\n",
    "print(f\"   {cv_scores_smote.mean():.4f} +/- {confidence_interval_smote:.4f}\")\n",
    "print(f\"   Range: [{cv_scores_smote.mean() - confidence_interval_smote:.4f}, \"\n",
    "      f\"{cv_scores_smote.mean() + confidence_interval_smote:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train Final SMOTE Model WITH HISTORY TRACKING\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Final SMOTE Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Training final neural network with SMOTE on all training data...\")\n",
    "print(\"   This model will be used for final evaluation\")\n",
    "print(\"   Training for up to 50 epochs with early stopping\")\n",
    "print(\"   Tracking training history for visualization\")\n",
    "print(\"   This will take 3-5 minutes\")\n",
    "\n",
    "# Apply SMOTE to entire training set\n",
    "smote_full = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote_full.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_smote_tensor = torch.FloatTensor(X_train_smote.values if isinstance(X_train_smote, pd.DataFrame) else X_train_smote)\n",
    "y_train_smote_tensor = torch.FloatTensor(y_train_smote.values if isinstance(y_train_smote, pd.Series) else y_train_smote).reshape(-1, 1)\n",
    "\n",
    "# Create dataset and split for validation\n",
    "train_dataset_full = TensorDataset(X_train_smote_tensor, y_train_smote_tensor)\n",
    "\n",
    "# Split into train and validation (80/20)\n",
    "train_size = int(0.8 * len(train_dataset_full))\n",
    "val_size = len(train_dataset_full) - train_size\n",
    "train_dataset_split, val_dataset_split = torch.utils.data.random_split(\n",
    "    train_dataset_full, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader_full = DataLoader(train_dataset_split, batch_size=32, shuffle=True)\n",
    "val_loader_full = DataLoader(val_dataset_split, batch_size=32, shuffle=False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train final model WITH HISTORY TRACKING\n",
    "nn_smote_model = FraudDetectionNN(X_train_scaled.shape[1], hidden_units=128, dropout=0.3).to(device)\n",
    "\n",
    "nn_smote_model, final_auc, stopped_epoch, smote_training_history = train_neural_network(\n",
    "    nn_smote_model, train_loader_full, val_loader_full,\n",
    "    epochs=50, patience=10, learning_rate=0.001, device=device, track_history=True\n",
    ")\n",
    "\n",
    "train_time_smote = time.time() - start_time\n",
    "\n",
    "print(f\"\\n+ SMOTE model training complete in {train_time_smote:.2f} seconds ({train_time_smote/60:.1f} minutes)\")\n",
    "print(f\"   Stopped at epoch: {stopped_epoch}\")\n",
    "print(f\"   Training history captured: {len(smote_training_history['epoch'])} epochs\")\n",
    "\n",
    "# Store SMOTE results\n",
    "nn_smote_cv_mean = cv_scores_smote.mean()\n",
    "nn_smote_cv_std = cv_scores_smote.std()\n",
    "\n",
    "print(f\"\\n SMOTE Model Summary:\")\n",
    "print(f\"   Expected AUC: {nn_smote_cv_mean:.4f} +/- {nn_smote_cv_std:.4f}\")\n",
    "print(f\"   Stability: {'Excellent' if nn_smote_cv_std < 0.01 else 'Good' if nn_smote_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART B: NEURAL NETWORK WITH ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART B: NEURAL NETWORK + ADASYN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n About ADASYN:\")\n",
    "print(\"   - Adaptive Synthetic Sampling\")\n",
    "print(\"   - Focuses on harder-to-learn minority samples\")\n",
    "print(\"   - Creates MORE synthetics near decision boundary (harder cases)\")\n",
    "print(\"   - Creates FEWER synthetics in easy regions\")\n",
    "print(\"   - Better for complex, varied fraud patterns\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Manual K-Fold Cross-Validation with ADASYN\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running 5-Fold Cross-Validation with ADASYN\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n Running manual 5-fold cross-validation...\")\n",
    "print(f\"   Dataset: X_train_scaled ({total_samples:,} samples, {total_frauds} frauds)\")\n",
    "print(f\"   ADASYN will adaptively generate synthetics based on difficulty\")\n",
    "print(f\"   This will take 10-15 minutes (depending on GPU)\")\n",
    "\n",
    "# Initialize K-Fold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_adasyn = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_scaled), 1):\n",
    "    print(f\"\\n    Training fold {fold}/5...\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    X_fold_train = X_train_scaled.iloc[train_idx] if isinstance(X_train_scaled, pd.DataFrame) else X_train_scaled[train_idx]\n",
    "    X_fold_val = X_train_scaled.iloc[val_idx] if isinstance(X_train_scaled, pd.DataFrame) else X_train_scaled[val_idx]\n",
    "    y_fold_train = y_train.iloc[train_idx] if isinstance(y_train, pd.Series) else y_train[train_idx]\n",
    "    y_fold_val = y_train.iloc[val_idx] if isinstance(y_train, pd.Series) else y_train[val_idx]\n",
    "    \n",
    "    # Apply ADASYN to training fold ONLY (not validation)\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_fold_train_resampled, y_fold_train_resampled = adasyn.fit_resample(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_fold_train_resampled.values if isinstance(X_fold_train_resampled, pd.DataFrame) else X_fold_train_resampled)\n",
    "    y_train_tensor = torch.FloatTensor(y_fold_train_resampled.values if isinstance(y_fold_train_resampled, pd.Series) else y_fold_train_resampled).reshape(-1, 1)\n",
    "    X_val_tensor = torch.FloatTensor(X_fold_val.values if isinstance(X_fold_val, pd.DataFrame) else X_fold_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_fold_val.values if isinstance(y_fold_val, pd.Series) else y_fold_val).reshape(-1, 1)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Create fresh model for this fold\n",
    "    model = FraudDetectionNN(X_train_scaled.shape[1], hidden_units=128, dropout=0.3).to(device)\n",
    "    \n",
    "    # Train model\n",
    "    model, fold_auc, stopped_epoch = train_neural_network(\n",
    "        model, train_loader, val_loader,\n",
    "        epochs=20, patience=5, learning_rate=0.001, device=device, track_history=False\n",
    "    )\n",
    "    \n",
    "    cv_scores_adasyn.append(fold_auc)\n",
    "    \n",
    "    print(f\"      Fold {fold} AUC: {fold_auc:.4f}\")\n",
    "    print(f\"      Stopped at epoch: {stopped_epoch}\")\n",
    "\n",
    "cv_time_adasyn = time.time() - start_time\n",
    "cv_scores_adasyn = np.array(cv_scores_adasyn)\n",
    "\n",
    "# Display ADASYN Cross-Validation Results\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ADASYN Cross-Validation Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Cross-validation complete in {cv_time_adasyn:.2f} seconds ({cv_time_adasyn/60:.1f} minutes)\")\n",
    "\n",
    "print(f\"\\n Individual Fold Scores (ADASYN):\")\n",
    "for i, score in enumerate(cv_scores_adasyn, 1):\n",
    "    print(f\"   Fold {i}: AUC = {score:.4f}\")\n",
    "\n",
    "print(f\"\\n ADASYN Summary Statistics:\")\n",
    "print(f\"   Mean CV AUC:  {cv_scores_adasyn.mean():.4f}\")\n",
    "print(f\"   Std CV AUC:   {cv_scores_adasyn.std():.4f}\")\n",
    "print(f\"   Min CV AUC:   {cv_scores_adasyn.min():.4f}\")\n",
    "print(f\"   Max CV AUC:   {cv_scores_adasyn.max():.4f}\")\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval_adasyn = 1.96 * cv_scores_adasyn.std()\n",
    "print(f\"\\n 95% Confidence Interval:\")\n",
    "print(f\"   {cv_scores_adasyn.mean():.4f} +/- {confidence_interval_adasyn:.4f}\")\n",
    "print(f\"   Range: [{cv_scores_adasyn.mean() - confidence_interval_adasyn:.4f}, \"\n",
    "      f\"{cv_scores_adasyn.mean() + confidence_interval_adasyn:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train Final ADASYN Model WITH HISTORY TRACKING\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Final ADASYN Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Training final neural network with ADASYN on all training data...\")\n",
    "print(\"   This model will be used for final evaluation\")\n",
    "print(\"   Training for up to 50 epochs with early stopping\")\n",
    "print(\"   Tracking training history for visualization\")\n",
    "print(\"   This will take 3-5 minutes\")\n",
    "\n",
    "# Apply ADASYN to entire training set\n",
    "adasyn_full = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn_full.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_adasyn_tensor = torch.FloatTensor(X_train_adasyn.values if isinstance(X_train_adasyn, pd.DataFrame) else X_train_adasyn)\n",
    "y_train_adasyn_tensor = torch.FloatTensor(y_train_adasyn.values if isinstance(y_train_adasyn, pd.Series) else y_train_adasyn).reshape(-1, 1)\n",
    "\n",
    "# Create dataset and split for validation\n",
    "train_dataset_full = TensorDataset(X_train_adasyn_tensor, y_train_adasyn_tensor)\n",
    "\n",
    "# Split into train and validation (80/20)\n",
    "train_size = int(0.8 * len(train_dataset_full))\n",
    "val_size = len(train_dataset_full) - train_size\n",
    "train_dataset_split, val_dataset_split = torch.utils.data.random_split(\n",
    "    train_dataset_full, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader_full = DataLoader(train_dataset_split, batch_size=32, shuffle=True)\n",
    "val_loader_full = DataLoader(val_dataset_split, batch_size=32, shuffle=False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train final model WITH HISTORY TRACKING\n",
    "nn_adasyn_model = FraudDetectionNN(X_train_scaled.shape[1], hidden_units=128, dropout=0.3).to(device)\n",
    "\n",
    "nn_adasyn_model, final_auc, stopped_epoch, adasyn_training_history = train_neural_network(\n",
    "    nn_adasyn_model, train_loader_full, val_loader_full,\n",
    "    epochs=50, patience=10, learning_rate=0.001, device=device, track_history=True\n",
    ")\n",
    "\n",
    "train_time_adasyn = time.time() - start_time\n",
    "\n",
    "print(f\"\\n+ ADASYN model training complete in {train_time_adasyn:.2f} seconds ({train_time_adasyn/60:.1f} minutes)\")\n",
    "print(f\"   Stopped at epoch: {stopped_epoch}\")\n",
    "print(f\"   Training history captured: {len(adasyn_training_history['epoch'])} epochs\")\n",
    "\n",
    "# Store ADASYN results\n",
    "nn_adasyn_cv_mean = cv_scores_adasyn.mean()\n",
    "nn_adasyn_cv_std = cv_scores_adasyn.std()\n",
    "\n",
    "print(f\"\\n ADASYN Model Summary:\")\n",
    "print(f\"   Expected AUC: {nn_adasyn_cv_mean:.4f} +/- {nn_adasyn_cv_std:.4f}\")\n",
    "print(f\"   Stability: {'Excellent' if nn_adasyn_cv_std < 0.01 else 'Good' if nn_adasyn_cv_std < 0.02 else 'Moderate'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON: SMOTE vs ADASYN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SMOTE vs ADASYN COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "print(f\"   SMOTE  Mean AUC: {nn_smote_cv_mean:.4f} (+/-{nn_smote_cv_std:.4f})\")\n",
    "print(f\"   ADASYN Mean AUC: {nn_adasyn_cv_mean:.4f} (+/-{nn_adasyn_cv_std:.4f})\")\n",
    "\n",
    "# Calculate difference\n",
    "auc_difference = nn_adasyn_cv_mean - nn_smote_cv_mean\n",
    "print(f\"\\n   Difference: {abs(auc_difference):.4f} ({'ADASYN better' if auc_difference > 0 else 'SMOTE better'})\")\n",
    "\n",
    "# Determine winner\n",
    "if abs(auc_difference) < 0.001:\n",
    "    winner = \"TIE - Performance essentially identical\"\n",
    "    recommendation = \"Either technique is suitable - choose based on training time\"\n",
    "elif auc_difference > 0:\n",
    "    winner = \"ADASYN WINS\"\n",
    "    recommendation = \"ADASYN provides better fraud detection for this dataset\"\n",
    "else:\n",
    "    winner = \"SMOTE WINS\"\n",
    "    recommendation = \"SMOTE provides better fraud detection for this dataset\"\n",
    "\n",
    "print(f\"\\n Winner: {winner}\")\n",
    "print(f\"\\n Recommendation: {recommendation}\")\n",
    "\n",
    "print(f\"\\n Training Time Comparison:\")\n",
    "print(f\"   SMOTE:  {cv_time_smote:.2f} seconds ({cv_time_smote/60:.1f} minutes)\")\n",
    "print(f\"   ADASYN: {cv_time_adasyn:.2f} seconds ({cv_time_adasyn/60:.1f} minutes)\")\n",
    "print(f\"   Difference: {abs(cv_time_smote - cv_time_adasyn):.2f} seconds ({'ADASYN slower' if cv_time_adasyn > cv_time_smote else 'SMOTE slower'})\")\n",
    "\n",
    "print(f\"\\n Stability Comparison:\")\n",
    "print(f\"   SMOTE  Std: {nn_smote_cv_std:.4f} ({'More stable' if nn_smote_cv_std < nn_adasyn_cv_std else 'Less stable'})\")\n",
    "print(f\"   ADASYN Std: {nn_adasyn_cv_std:.4f} ({'More stable' if nn_adasyn_cv_std < nn_smote_cv_std else 'Less stable'})\")\n",
    "\n",
    "# Statistical significance test\n",
    "print(f\"\\n Statistical Significance:\")\n",
    "smote_lower = nn_smote_cv_mean - confidence_interval_smote\n",
    "smote_upper = nn_smote_cv_mean + confidence_interval_smote\n",
    "adasyn_lower = nn_adasyn_cv_mean - confidence_interval_adasyn\n",
    "adasyn_upper = nn_adasyn_cv_mean + confidence_interval_adasyn\n",
    "\n",
    "if (smote_lower <= adasyn_upper) and (adasyn_lower <= smote_upper):\n",
    "    print(f\"   Confidence intervals overlap\")\n",
    "    print(f\"   Difference may not be statistically significant\")\n",
    "    print(f\"   Both methods perform similarly on this dataset\")\n",
    "else:\n",
    "    print(f\"   + Confidence intervals don't overlap\")\n",
    "    print(f\"   Difference is likely statistically significant\")\n",
    "    print(f\"   {winner} is meaningfully better\")\n",
    "\n",
    "# Visualization of comparison\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Visual Comparison\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Mean AUC Scores:\")\n",
    "print(f\"   SMOTE:  {'█' * int(nn_smote_cv_mean * 50)} {nn_smote_cv_mean:.4f}\")\n",
    "print(f\"   ADASYN: {'█' * int(nn_adasyn_cv_mean * 50)} {nn_adasyn_cv_mean:.4f}\")\n",
    "\n",
    "print(f\"\\n Consistency (Lower is Better):\")\n",
    "print(f\"   SMOTE:  {'█' * int(nn_smote_cv_std * 1000)} {nn_smote_cv_std:.4f}\")\n",
    "print(f\"   ADASYN: {'█' * int(nn_adasyn_cv_std * 1000)} {nn_adasyn_cv_std:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT BEST MODEL FOR FINAL USE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SELECTING BEST RESAMPLING TECHNIQUE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Choose best based on mean AUC (with tie-breaker on stability)\n",
    "if abs(auc_difference) < 0.001:\n",
    "    # Performance is essentially the same, choose more stable\n",
    "    if nn_smote_cv_std < nn_adasyn_cv_std:\n",
    "        best_nn_model = nn_smote_model\n",
    "        best_method = \"SMOTE\"\n",
    "        best_cv_mean = nn_smote_cv_mean\n",
    "        best_cv_std = nn_smote_cv_std\n",
    "        best_cv_scores = cv_scores_smote\n",
    "        best_training_history = smote_training_history\n",
    "        reason = \"Similar performance, but SMOTE is more stable\"\n",
    "    else:\n",
    "        best_nn_model = nn_adasyn_model\n",
    "        best_method = \"ADASYN\"\n",
    "        best_cv_mean = nn_adasyn_cv_mean\n",
    "        best_cv_std = nn_adasyn_cv_std\n",
    "        best_cv_scores = cv_scores_adasyn\n",
    "        best_training_history = adasyn_training_history\n",
    "        reason = \"Similar performance, but ADASYN is more stable\"\n",
    "else:\n",
    "    # Clear winner based on AUC\n",
    "    if nn_smote_cv_mean > nn_adasyn_cv_mean:\n",
    "        best_nn_model = nn_smote_model\n",
    "        best_method = \"SMOTE\"\n",
    "        best_cv_mean = nn_smote_cv_mean\n",
    "        best_cv_std = nn_smote_cv_std\n",
    "        best_cv_scores = cv_scores_smote\n",
    "        best_training_history = smote_training_history\n",
    "        reason = f\"Higher mean AUC ({nn_smote_cv_mean:.4f} vs {nn_adasyn_cv_mean:.4f})\"\n",
    "    else:\n",
    "        best_nn_model = nn_adasyn_model\n",
    "        best_method = \"ADASYN\"\n",
    "        best_cv_mean = nn_adasyn_cv_mean\n",
    "        best_cv_std = nn_adasyn_cv_std\n",
    "        best_cv_scores = cv_scores_adasyn\n",
    "        best_training_history = adasyn_training_history\n",
    "        reason = f\"Higher mean AUC ({nn_adasyn_cv_mean:.4f} vs {nn_smote_cv_mean:.4f})\"\n",
    "\n",
    "print(f\"\\n+ Selected Method: {best_method}\")\n",
    "print(f\"   Reason: {reason}\")\n",
    "print(f\"   Performance: {best_cv_mean:.4f} +/- {best_cv_std:.4f}\")\n",
    "\n",
    "print(f\"\\n Storing best model for later steps:\")\n",
    "print(f\"   Model: Neural Network + {best_method}\")\n",
    "print(f\"   Expected AUC: {best_cv_mean:.4f}\")\n",
    "\n",
    "# Store the best results for comparison in Step 21\n",
    "nn_final_pipeline = best_nn_model\n",
    "nn_cv_mean = best_cv_mean\n",
    "nn_cv_std = best_cv_std\n",
    "nn_cv_scores = best_cv_scores\n",
    "nn_best_method = best_method\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETER TUNING FOR BEST MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - NEURAL NETWORK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n WHAT WE'RE DOING:\")\n",
    "print(f\"   - Fine-tuning the best model ({best_method} + Neural Network)\")\n",
    "print(f\"   - Testing different hyperparameter combinations\")\n",
    "print(f\"   - Using manual grid search with 5-fold cross-validation\")\n",
    "print(f\"   - Goal: Improve beyond baseline {best_cv_mean:.4f} AUC\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Define Hyperparameter Grid\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Defining Hyperparameter Search Space\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Hyperparameters to tune:\")\n",
    "print(\"   1. learning_rate (Learning speed)\")\n",
    "print(\"      - How fast the model adjusts weights\")\n",
    "print(\"      - 0.001: Slow, careful learning (more precise)\")\n",
    "print(\"      - 0.01: Fast learning (quicker but might overshoot)\")\n",
    "print(\"\\n   2. dropout (Regularization strength)\")\n",
    "print(\"      - Percentage of neurons randomly dropped during training\")\n",
    "print(\"      - 0.2: Less aggressive (20% dropout)\")\n",
    "print(\"      - 0.3: More aggressive (30% dropout, current baseline)\")\n",
    "print(\"\\n   3. batch_size (Training batch size)\")\n",
    "print(\"      - Number of samples processed before updating weights\")\n",
    "print(\"      - 32: Smaller batches, noisier updates (current)\")\n",
    "print(\"      - 64: Larger batches, more stable updates\")\n",
    "print(\"\\n   4. hidden_units (Network capacity)\")\n",
    "print(\"      - Number of neurons in first layer\")\n",
    "print(\"      - 64: Smaller network (faster, less overfitting)\")\n",
    "print(\"      - 128: Larger network (more capacity, current baseline)\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'batch_size': [32, 64],\n",
    "    'hidden_units': [64, 128]\n",
    "}\n",
    "\n",
    "print(\"\\n Parameter Grid:\")\n",
    "print(f\"   learning_rate: {param_grid['learning_rate']}\")\n",
    "print(f\"   dropout: {param_grid['dropout']}\")\n",
    "print(f\"   batch_size: {param_grid['batch_size']}\")\n",
    "print(f\"   hidden_units: {param_grid['hidden_units']}\")\n",
    "print(f\"\\n   Total combinations: {len(param_grid['learning_rate']) * len(param_grid['dropout']) * len(param_grid['batch_size']) * len(param_grid['hidden_units'])}\")\n",
    "print(f\"   With 5-fold CV: {len(param_grid['learning_rate']) * len(param_grid['dropout']) * len(param_grid['batch_size']) * len(param_grid['hidden_units']) * 5} model fits\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Run Manual Grid Search\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Running Grid Search with Cross-Validation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n This will take 60-80 minutes...\")\n",
    "print(\"   Each combination is tested with 5-fold CV\")\n",
    "print(\"   Neural networks train sequentially (cannot parallelize on Windows)\")\n",
    "print(\"   Progress will be shown below\")\n",
    "\n",
    "# Choose best resampling method for tuning\n",
    "if best_method == \"SMOTE\":\n",
    "    resampler_class = SMOTE\n",
    "else:\n",
    "    resampler_class = ADASYN\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Grid search loop\n",
    "combination_num = 0\n",
    "total_combinations = len(param_grid['learning_rate']) * len(param_grid['dropout']) * len(param_grid['batch_size']) * len(param_grid['hidden_units'])\n",
    "\n",
    "for lr in param_grid['learning_rate']:\n",
    "    for dropout in param_grid['dropout']:\n",
    "        for batch_size in param_grid['batch_size']:\n",
    "            for hidden_units in param_grid['hidden_units']:\n",
    "                combination_num += 1\n",
    "                \n",
    "                print(f\"\\n  Testing combination {combination_num}/{total_combinations}:\")\n",
    "                print(f\"    learning_rate={lr}, dropout={dropout}, batch_size={batch_size}, hidden_units={hidden_units}\")\n",
    "                \n",
    "                # Run 5-fold CV for this combination\n",
    "                kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                cv_scores = []\n",
    "                \n",
    "                for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_scaled), 1):\n",
    "                    # Split data\n",
    "                    X_fold_train = X_train_scaled.iloc[train_idx] if isinstance(X_train_scaled, pd.DataFrame) else X_train_scaled[train_idx]\n",
    "                    X_fold_val = X_train_scaled.iloc[val_idx] if isinstance(X_train_scaled, pd.DataFrame) else X_train_scaled[val_idx]\n",
    "                    y_fold_train = y_train.iloc[train_idx] if isinstance(y_train, pd.Series) else y_train[train_idx]\n",
    "                    y_fold_val = y_train.iloc[val_idx] if isinstance(y_train, pd.Series) else y_train[val_idx]\n",
    "                    \n",
    "                    # Apply resampling\n",
    "                    resampler = resampler_class(random_state=42)\n",
    "                    X_fold_train_resampled, y_fold_train_resampled = resampler.fit_resample(X_fold_train, y_fold_train)\n",
    "                    \n",
    "                    # Convert to tensors\n",
    "                    X_train_tensor = torch.FloatTensor(X_fold_train_resampled.values if isinstance(X_fold_train_resampled, pd.DataFrame) else X_fold_train_resampled)\n",
    "                    y_train_tensor = torch.FloatTensor(y_fold_train_resampled.values if isinstance(y_fold_train_resampled, pd.Series) else y_fold_train_resampled).reshape(-1, 1)\n",
    "                    X_val_tensor = torch.FloatTensor(X_fold_val.values if isinstance(X_fold_val, pd.DataFrame) else X_fold_val)\n",
    "                    y_val_tensor = torch.FloatTensor(y_fold_val.values if isinstance(y_fold_val, pd.Series) else y_fold_val).reshape(-1, 1)\n",
    "                    \n",
    "                    # Create data loaders with current batch_size\n",
    "                    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "                    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "                    \n",
    "                    # Create and train model\n",
    "                    model = FraudDetectionNN(X_train_scaled.shape[1], hidden_units=hidden_units, dropout=dropout).to(device)\n",
    "                    model, fold_auc, stopped_epoch = train_neural_network(\n",
    "                        model, train_loader, val_loader,\n",
    "                        epochs=20, patience=5, learning_rate=lr, device=device, track_history=False\n",
    "                    )\n",
    "                    \n",
    "                    cv_scores.append(fold_auc)\n",
    "                \n",
    "                # Calculate mean and std for this combination\n",
    "                mean_auc = np.mean(cv_scores)\n",
    "                std_auc = np.std(cv_scores)\n",
    "                \n",
    "                # Store results\n",
    "                all_results.append({\n",
    "                    'learning_rate': lr,\n",
    "                    'dropout': dropout,\n",
    "                    'batch_size': batch_size,\n",
    "                    'hidden_units': hidden_units,\n",
    "                    'mean_auc': mean_auc,\n",
    "                    'std_auc': std_auc,\n",
    "                    'cv_scores': cv_scores\n",
    "                })\n",
    "                \n",
    "                print(f\"    Mean AUC: {mean_auc:.4f} (+/-{std_auc:.4f})\")\n",
    "\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Display Results\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Grid Search Results\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n+ Grid search complete in {grid_time:.2f} seconds ({grid_time/60:.1f} minutes)\")\n",
    "\n",
    "# Find best combination\n",
    "all_results_sorted = sorted(all_results, key=lambda x: x['mean_auc'], reverse=True)\n",
    "best_params = all_results_sorted[0]\n",
    "\n",
    "print(f\"\\n Best Parameters Found:\")\n",
    "print(f\"   learning_rate: {best_params['learning_rate']}\")\n",
    "print(f\"   dropout: {best_params['dropout']}\")\n",
    "print(f\"   batch_size: {best_params['batch_size']}\")\n",
    "print(f\"   hidden_units: {best_params['hidden_units']}\")\n",
    "\n",
    "print(f\"\\n Performance Comparison:\")\n",
    "print(f\"   Baseline AUC:  {best_cv_mean:.4f} (+/-{best_cv_std:.4f})\")\n",
    "print(f\"   Tuned AUC:     {best_params['mean_auc']:.4f} (+/-{best_params['std_auc']:.4f})\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = best_params['mean_auc'] - best_cv_mean\n",
    "improvement_pct = (improvement / best_cv_mean) * 100\n",
    "\n",
    "print(f\"\\n Improvement:\")\n",
    "if improvement > 0:\n",
    "    print(f\"   +{improvement:.4f} AUC ({improvement_pct:+.2f}%)\")\n",
    "    print(f\"   + Hyperparameter tuning improved performance!\")\n",
    "elif improvement < -0.001:\n",
    "    print(f\"   {improvement:.4f} AUC ({improvement_pct:.2f}%)\")\n",
    "    print(f\"   Tuned model performed slightly worse\")\n",
    "    print(f\"   -> Baseline hyperparameters were already well-optimized\")\n",
    "else:\n",
    "    print(f\"   ~{improvement:.4f} AUC (essentially no change)\")\n",
    "    print(f\"   -> Baseline hyperparameters were already optimal\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Top 5 Parameter Combinations\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Top 5 Hyperparameter Combinations\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Best performing combinations:\\n\")\n",
    "for i, result in enumerate(all_results_sorted[:5], 1):\n",
    "    print(f\"   Rank {i}:\")\n",
    "    print(f\"      learning_rate={result['learning_rate']}, dropout={result['dropout']}, \"\n",
    "          f\"batch_size={result['batch_size']}, hidden_units={result['hidden_units']}\")\n",
    "    print(f\"      Mean AUC: {result['mean_auc']:.4f} (+/-{result['std_auc']:.4f})\")\n",
    "    print()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Train Final Tuned Model WITH HISTORY TRACKING\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Final Tuned Model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n Training final neural network with best hyperparameters...\")\n",
    "print(\"   Tracking training history for visualization\")\n",
    "print(\"   This will take 3-5 minutes\")\n",
    "\n",
    "# Apply resampling to full training set\n",
    "if best_method == \"SMOTE\":\n",
    "    resampler_full = SMOTE(random_state=42)\n",
    "else:\n",
    "    resampler_full = ADASYN(random_state=42)\n",
    "\n",
    "X_train_resampled_full, y_train_resampled_full = resampler_full.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_full_tensor = torch.FloatTensor(X_train_resampled_full.values if isinstance(X_train_resampled_full, pd.DataFrame) else X_train_resampled_full)\n",
    "y_train_full_tensor = torch.FloatTensor(y_train_resampled_full.values if isinstance(y_train_resampled_full, pd.Series) else y_train_resampled_full).reshape(-1, 1)\n",
    "\n",
    "# Create dataset and split\n",
    "train_dataset_full = TensorDataset(X_train_full_tensor, y_train_full_tensor)\n",
    "train_size = int(0.8 * len(train_dataset_full))\n",
    "val_size = len(train_dataset_full) - train_size\n",
    "train_dataset_split, val_dataset_split = torch.utils.data.random_split(\n",
    "    train_dataset_full, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader_tuned = DataLoader(train_dataset_split, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader_tuned = DataLoader(val_dataset_split, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and train final tuned model WITH HISTORY TRACKING\n",
    "nn_tuned_model = FraudDetectionNN(\n",
    "    X_train_scaled.shape[1],\n",
    "    hidden_units=best_params['hidden_units'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "nn_tuned_model, final_auc, stopped_epoch, tuned_training_history = train_neural_network(\n",
    "    nn_tuned_model, train_loader_tuned, val_loader_tuned,\n",
    "    epochs=50, patience=10, learning_rate=best_params['learning_rate'], device=device, track_history=True\n",
    ")\n",
    "\n",
    "final_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n+ Tuned model training complete in {final_train_time:.2f} seconds ({final_train_time/60:.1f} minutes)\")\n",
    "print(f\"   Stopped at epoch: {stopped_epoch}\")\n",
    "print(f\"   Training history captured: {len(tuned_training_history['epoch'])} epochs\")\n",
    "\n",
    "# Store tuned results\n",
    "nn_tuned_cv_mean = best_params['mean_auc']\n",
    "nn_tuned_cv_std = best_params['std_auc']\n",
    "nn_best_params = {\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'dropout': best_params['dropout'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'hidden_units': best_params['hidden_units']\n",
    "}\n",
    "\n",
    "print(f\"\\n+ Tuned model stored for final evaluation\")\n",
    "print(f\"   Best hyperparameters saved\")\n",
    "print(f\"   Expected AUC: {nn_tuned_cv_mean:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Decision: Use Baseline or Tuned?\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Model Selection Decision\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Decide whether to use tuned or baseline model\n",
    "if improvement > 0.0001:  # Meaningful improvement\n",
    "    print(f\"\\n+ Using TUNED model for final evaluation\")\n",
    "    print(f\"   Reason: Tuning improved performance by {improvement:.4f}\")\n",
    "    nn_final_model = nn_tuned_model\n",
    "    nn_final_cv_mean = nn_tuned_cv_mean\n",
    "    nn_final_cv_std = nn_tuned_cv_std\n",
    "    nn_final_training_history = tuned_training_history\n",
    "    model_version = \"Tuned\"\n",
    "else:\n",
    "    print(f\"\\n+ Using BASELINE model for final evaluation\")\n",
    "    print(f\"   Reason: Tuning did not provide meaningful improvement\")\n",
    "    print(f\"   Baseline hyperparameters were already well-optimized\")\n",
    "    nn_final_model = nn_final_pipeline\n",
    "    nn_final_cv_mean = best_cv_mean\n",
    "    nn_final_cv_std = best_cv_std\n",
    "    nn_final_training_history = best_training_history\n",
    "    model_version = \"Baseline\"\n",
    "\n",
    "print(f\"\\n Final Neural Network Model:\")\n",
    "print(f\"   Version: {model_version}\")\n",
    "print(f\"   Resampling: {best_method}\")\n",
    "print(f\"   Expected AUC: {nn_final_cv_mean:.4f} (+/-{nn_final_cv_std:.4f})\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE MODEL AND RESULTS TO DISK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL AND RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Saving to ../models/ directory...\")\n",
    "\n",
    "# Save the final model (PyTorch uses .pth extension)\n",
    "model_path = '../models/neural_network_model.pth'\n",
    "torch.save(nn_final_model.state_dict(), model_path)\n",
    "print(f\"   + Model saved: {model_path}\")\n",
    "\n",
    "# Save all results and metadata INCLUDING TRAINING HISTORY\n",
    "results_data = {\n",
    "    'model_name': 'Neural Network',\n",
    "    'resampling_method': best_method,\n",
    "    'model_version': model_version,\n",
    "    'cv_mean': nn_final_cv_mean,\n",
    "    'cv_std': nn_final_cv_std,\n",
    "    'cv_scores': nn_cv_scores,\n",
    "    'best_params': nn_best_params if model_version == \"Tuned\" else None,\n",
    "    'training_samples': total_samples,\n",
    "    'training_frauds': total_frauds,\n",
    "    'architecture': {\n",
    "        'input_dim': X_train_scaled.shape[1],\n",
    "        'hidden_units': best_params['hidden_units'] if model_version == \"Tuned\" else 128,\n",
    "        'dropout': best_params['dropout'] if model_version == \"Tuned\" else 0.3\n",
    "    },\n",
    "    'training_history': nn_final_training_history,  # ADDED: Training history\n",
    "    'timestamp': pd.Timestamp.now()\n",
    "}\n",
    "\n",
    "results_path = '../models/neural_network_results.pkl'\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(results_data, f)\n",
    "print(f\"   + Results saved: {results_path}\")\n",
    "\n",
    "print(f\"\\n+ All files saved successfully!\")\n",
    "print(f\"   Model can now be loaded in Phase 5 for comparison\")\n",
    "print(f\"   Training history available for Step 22 visualization\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 20 COMPLETE - KEY ACCOMPLISHMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n What We Accomplished:\")\n",
    "print(\"   + Built 4-layer Neural Network (deep learning)\")\n",
    "print(\"   + Compared SMOTE vs ADASYN performance\")\n",
    "print(\"   + Used manual 5-Fold CV (PyTorch requirement)\")\n",
    "print(\"   + Resampling applied inside each fold (proper methodology)\")\n",
    "print(\"   + Validated on 100% real data (no synthetic contamination)\")\n",
    "print(\"   + Leveraged GPU acceleration for faster training\")\n",
    "print(\"   + Selected best resampling technique based on performance\")\n",
    "print(\"   + Performed hyperparameter tuning with grid search\")\n",
    "print(\"   + Trained final optimized model on all training data\")\n",
    "print(\"   + SAVED model and results to disk\")\n",
    "print(\"   + CAPTURED training history for visualization\")\n",
    "\n",
    "print(f\"\\n Final Results:\")\n",
    "print(f\"   Best Method: {best_method}\")\n",
    "print(f\"   Model Version: {model_version}\")\n",
    "print(f\"   Cross-Validation AUC: {nn_final_cv_mean:.4f} (+/-{nn_final_cv_std:.4f})\")\n",
    "print(f\"   Model Stability: {'Excellent' if nn_final_cv_std < 0.01 else 'Good' if nn_final_cv_std < 0.02 else 'Moderate'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU Acceleration: + Used successfully\")\n",
    "else:\n",
    "    print(f\"   GPU Acceleration: - Not available (CPU training)\")\n",
    "\n",
    "print(\"\\n Saved Files:\")\n",
    "print(f\"   - {model_path}\")\n",
    "print(f\"   - {results_path}\")\n",
    "\n",
    "print(\"\\n Next Steps:\")\n",
    "print(\"   -> Step 21: Compare all models and select winner\")\n",
    "print(\"   -> Step 22: Visualize neural network training curves\")\n",
    "print(\"   -> Final evaluation on test set\")\n",
    "\n",
    "print(\"\\n Important Notes:\")\n",
    "print(\"   - Test set remains UNTOUCHED for final evaluation in Phase 5\")\n",
    "print(\"   - Model saved and can be loaded anytime (no need to retrain)\")\n",
    "print(\"   - Can restart PC without losing progress\")\n",
    "print(f\"   - {best_method} + {model_version} model ready for Phase 5\")\n",
    "print(f\"   - All 4 models trained with consistent methodology for fair comparison\")\n",
    "print(f\"   - Training history saved for Step 22 visualization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309f0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
